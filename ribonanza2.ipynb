{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ribonanza - Attempt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second approach to the [Stanford Ribonanza problem](https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/) that builds off the first approach.\n",
    "\n",
    "Major differences:\n",
    "- use of different model architecture\n",
    "- use of only filtered data (data in which SN_filter == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, this scores a 0.24796 MAE\n",
    "\n",
    "![Score](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Figure out why validation MAE differs so much from submission MAE\n",
    "- [x] Train more\n",
    "- [ ] Figure out what the reactivity_error means\n",
    "- [ ] Try a transformer, LSTM, or RNN architecture and take advantage of masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filesystem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your project directory should look like this:\n",
    "\n",
    "- `(project directory)`\n",
    "    - `ribonanza2.ipynb`\n",
    "    - `train_data.csv`\n",
    "    - `test_data.csv` (optional)\n",
    "\n",
    "`train_data.csv` is the only file necessary for training, and it can be downloaded from the kaggle competition linked in the description.\n",
    "\n",
    "`test_data.csv` is only necessary if you intend to make and submit predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 11:55:40.364087: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-06 11:55:41.009177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "import keras.optimizers as optimizers\n",
    "import pandas\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "# according to kaggle, this is the maximum # of reactivites to be used\n",
    "NUM_REACTIVITIES = 457\n",
    "\n",
    "# there are 4 different bases (AUCG)\n",
    "NUM_BASES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(out: str, key: str, value: str, file_name: str, force: bool):\n",
    "    \"\"\"\n",
    "    Filters a file to only take datapoints\n",
    "    whose values of `key` are `value`.\n",
    "\n",
    "    Parameters:\n",
    "        - out: str - the name of the file that will store the filtered datapoints\n",
    "        - key: str - the name of the key to look at\n",
    "        - value: str - the value that the key should have\n",
    "        - file_name: str - the name of the file that contains all the datapoints.\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    if os.path.exists(out) and not force:\n",
    "        print(\"File already exists, not doing any work\")\n",
    "        return\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    # count how many lines we have in total\n",
    "    with open(file_name) as file:\n",
    "        line = file.readline()  # ignore the header\n",
    "        line = (\n",
    "            file.readline()\n",
    "        )  # take the first line since we increment count in the loop\n",
    "        while line != \"\":\n",
    "            count += 1\n",
    "            line = file.readline()\n",
    "\n",
    "    # use that knowledge for a progress bar\n",
    "    with open(file_name, \"r\") as file, open(out, \"w\") as outfile:\n",
    "        # write the header\n",
    "        header = file.readline()\n",
    "        outfile.write(header)\n",
    "\n",
    "        # get what index the SN_filter is\n",
    "        SN_idx = header.split(\",\").index(key)\n",
    "\n",
    "        # only take the approved filtered lines\n",
    "        for _ in tqdm(range(count)):\n",
    "            line = file.readline()\n",
    "            temp = line.split(\",\")\n",
    "            if temp[SN_idx] == value:\n",
    "                outfile.write(line)\n",
    "\n",
    "\n",
    "def filter_train_data(force: bool = False):\n",
    "    \"\"\"\n",
    "    Filters the immense train_data.csv to only take datapoints\n",
    "    whose SN_filter (Signal to Noise filter) is 1. In other words,\n",
    "    we only take good reads. These filtered datapoints are then\n",
    "    written to the file provided\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\"train_data_filtered.csv\", \"SN_filter\", \"1\", \"train_data.csv\", force)\n",
    "\n",
    "\n",
    "def filter_2A3(force: bool = False):\n",
    "    \"\"\"\n",
    "    Only take the 2A3 points\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\n",
    "        \"train_data_2a3.csv\",\n",
    "        \"experiment_type\",\n",
    "        \"2A3_MaP\",\n",
    "        \"train_data_filtered.csv\",\n",
    "        force,\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_DMS(force: bool = False):\n",
    "    \"\"\"\n",
    "    Only take the DMS points\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\n",
    "        \"train_data_dms.csv\",\n",
    "        \"experiment_type\",\n",
    "        \"DMS_MaP\",\n",
    "        \"train_data_filtered.csv\",\n",
    "        force,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# filter our data\n",
    "filter_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# take the 2a3 points\n",
    "filter_2A3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# take the dms points\n",
    "filter_DMS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode inputs as\n",
    "# A : [1, 0, 0, 0]\n",
    "# U : [0, 1, 0, 0]\n",
    "# C : [0, 0, 1, 0]\n",
    "# G : [0, 0, 0, 1]\n",
    "base_map = {\n",
    "    \"A\": np.array([1, 0, 0, 0]),\n",
    "    \"U\": np.array([0, 1, 0, 0]),\n",
    "    \"C\": np.array([0, 0, 1, 0]),\n",
    "    \"G\": np.array([0, 0, 0, 1]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode inputs as\n",
    "# A : [1, 0, 0, 0]\n",
    "# U : [0, 1, 0, 0]\n",
    "# C : [0, 0, 1, 0]\n",
    "# G : [0, 0, 0, 1]\n",
    "base_map = {\n",
    "    \"A\": np.array([1, 0, 0, 0]),\n",
    "    \"U\": np.array([0, 1, 0, 0]),\n",
    "    \"C\": np.array([0, 0, 1, 0]),\n",
    "    \"G\": np.array([0, 0, 0, 1]),\n",
    "}\n",
    "\n",
    "\n",
    "def preprocess_csv(out: str, file_name: str, force: bool = False):\n",
    "    \"\"\"\n",
    "    Preprocess the csv and save the preprocessed data as a .npz file\n",
    "\n",
    "    Parameters:\n",
    "        - out: str - the name of the file to save the arrays to\n",
    "        - file_name: str - the name of the input csv file\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done).\n",
    "                Defaults to `False`\n",
    "    \"\"\"\n",
    "    if os.path.exists(out) and not force:\n",
    "        print(\"File already exists, not doing any work\")\n",
    "        return\n",
    "\n",
    "    df = pandas.read_csv(file_name)\n",
    "\n",
    "    inputs = np.zeros((len(df), NUM_REACTIVITIES, NUM_BASES))\n",
    "    outputs = np.zeros((len(df), NUM_REACTIVITIES))\n",
    "    output_masks = np.ones((len(df), NUM_REACTIVITIES), dtype=np.bool_)\n",
    "    errors = np.zeros((len(df), NUM_REACTIVITIES))\n",
    "\n",
    "    for index in tqdm(range(len(df))):\n",
    "        row = df.iloc[index]\n",
    "\n",
    "        # get the sequence\n",
    "        seq_len = len(row[\"sequence\"])\n",
    "\n",
    "        # map the base to its one-hot encoding\n",
    "        inputs[index, :seq_len] = np.array(\n",
    "            list(map(lambda letter: base_map[letter], row[\"sequence\"]))\n",
    "        )\n",
    "\n",
    "        # get all the reactivities and reactivity errors\n",
    "        reactivities = np.array(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda seq_idx: row[\"reactivity_\" + str(seq_idx + 1).rjust(4, \"0\")],\n",
    "                    range(seq_len),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        reactivity_errors = np.array(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda seq_idx: row[\"reactivity_error_\" + str(seq_idx + 1).rjust(4, \"0\")],\n",
    "                    range(seq_len),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # replace reactivity error nans with 0s (assume no error)\n",
    "        reactivity_errors = np.where(np.isnan(reactivity_errors), 0.0, reactivity_errors)\n",
    "\n",
    "        # get where all the reactivities are nan\n",
    "        nan_locats = np.isnan(reactivities)\n",
    "\n",
    "        # where it is nan, store True, else false\n",
    "        output_masks[index, :seq_len] = nan_locats\n",
    "\n",
    "        # where it is not nan, store the reactivity and error, else 0\n",
    "        outputs[index, :seq_len] = np.where(nan_locats == False, reactivities, 0.0)\n",
    "        errors[index, :seq_len] = np.where(nan_locats == False, reactivity_errors, 0.0)\n",
    "\n",
    "    # save the outputs\n",
    "    np.savez_compressed(out, inputs=inputs, outputs=outputs, output_masks=output_masks, errors=errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\"train_data_2a3_preprocessed.npz\", \"train_data_2a3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\"train_data_dms_preprocessed.npz\", \"train_data_dms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the desired dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:33.551791Z",
     "start_time": "2023-10-05T17:09:33.535197Z"
    }
   },
   "outputs": [],
   "source": [
    "desired_dataset = \"2a3\"  # either \"2a3\" or \"dms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.265620Z",
     "start_time": "2023-10-05T17:09:33.728394Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the npz file\n",
    "npz_file = np.load(f\"train_data_{desired_dataset}_preprocessed.npz\")\n",
    "\n",
    "# stored inputs, outputs, and output_masks\n",
    "# note: if visualizing, you may just want to only load outputs and bool_output_masks\n",
    "# since histplot takes a lot of RAM.\n",
    "inputs, outputs, bool_output_masks, errors = (\n",
    "    npz_file[\"inputs\"],\n",
    "    npz_file[\"outputs\"],\n",
    "    npz_file[\"output_masks\"],\n",
    "    npz_file[\"errors\"]\n",
    ")\n",
    "\n",
    "# close the npz file\n",
    "npz_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.843147Z",
     "start_time": "2023-10-05T17:09:42.265856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210992, 457)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to usable weights:\n",
    "# if it is meant to be masked, it should be worth 0, else it should be worth 1 - error\n",
    "output_masks = np.where(bool_output_masks, 0., 1.)\n",
    "output_masks -= errors\n",
    "output_masks[output_masks < 0.] = 0.\n",
    "output_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = np.clip(outputs, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the notebook allows for visualizing the reactivities of the\n",
    "current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.844615Z",
     "start_time": "2023-10-05T17:09:42.843236Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.851158Z",
     "start_time": "2023-10-05T17:09:42.846563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not visualizing. Set `visualize` to `True` to visualize data\n"
     ]
    }
   ],
   "source": [
    "if visualize:\n",
    "    visualized_items = []\n",
    "    for i in tqdm(range(len(outputs))):\n",
    "        for x in range(NUM_REACTIVITIES):\n",
    "            if not bool_output_masks[i, x]:\n",
    "                visualized_items.append(outputs[i, x])\n",
    "    visualized_items = np.array(visualized_items)\n",
    "    print(f\"took {len(visualized_items)}/{len(outputs)*NUM_REACTIVITIES} reactivities\")\n",
    "else:\n",
    "    print(\"Not visualizing. Set `visualize` to `True` to visualize data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.851272Z",
     "start_time": "2023-10-05T17:09:42.849526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not visualizing. Set `visualize` to `True` to visualize data\n"
     ]
    }
   ],
   "source": [
    "if visualize:\n",
    "    seaborn.histplot(visualized_items, binwidth=0.1)\n",
    "else:\n",
    "    print(\"Not visualizing. Set `visualize` to `True` to visualize data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.859550Z",
     "start_time": "2023-10-05T17:09:42.852935Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_baseline_model():\n",
    "    inputs = layers.Input((NUM_REACTIVITIES, NUM_BASES))\n",
    "\n",
    "    x = layers.Conv1D(4, 16, 1, padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = layers.Conv1D(4, 16, 1, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv1D(4, 16, 2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\", activity_regularizer=\"l2\")(x)\n",
    "\n",
    "    x = layers.Dense(NUM_REACTIVITIES, activation='relu')(x)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "def load_model() -> keras.Model:\n",
    "    return keras.models.load_model(f\"{desired_dataset}_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.930488Z",
     "start_time": "2023-10-05T17:09:42.864218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 457, 4)]          0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 457, 4)            260       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 457, 4)            260       \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 229, 4)            260       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 916)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              939008    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 457)               468425    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,408,213\n",
      "Trainable params: 1,408,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 11:55:52.658506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 11:55:52.790010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 11:55:52.790322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 11:55:52.791944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 11:55:52.792172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 11:55:52.792371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 11:55:52.871546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 11:55:52.871817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 11:55:52.872027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 11:55:52.872209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4279 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = make_baseline_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.934636Z",
     "start_time": "2023-10-05T17:09:42.924401Z"
    }
   },
   "outputs": [],
   "source": [
    "# compile the model with an optimizer and MAE loss\n",
    "model.compile(optimizer=optimizers.Adam(1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:43.992618Z",
     "start_time": "2023-10-05T17:09:43.053099Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_batch(m: keras.Model, inps: tf.Tensor, outs: tf.Tensor, masks: tf.Tensor):\n",
    "    \"\"\"\n",
    "    Get the loss on a batch and perform the corresponding weight updates.\n",
    "    Used for training purposes\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = tf.expand_dims(m(inps, training=True), axis=-1)\n",
    "        outs = tf.expand_dims(outs, axis=-1)\n",
    "        mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "        loss = 0.0\n",
    "        for b in range(inps.shape[0]):\n",
    "            loss += mae(outs[b], preds[b], sample_weight=masks[b])\n",
    "\n",
    "        # turn it into mean\n",
    "        loss /= inps.shape[0]\n",
    "    \n",
    "        # add the regularization losses\n",
    "        regularization_loss = tf.add_n(m.losses)\n",
    "        loss += regularization_loss\n",
    "        \n",
    "        # calculate gradients\n",
    "        grads = tape.gradient(loss, m.trainable_variables)\n",
    "\n",
    "    # apply grads\n",
    "    m.optimizer.apply_gradients(zip(grads, m.trainable_variables))\n",
    "\n",
    "    # return total loss, mae loss\n",
    "    return loss, loss - regularization_loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def noupdate_batch(m: keras.Model, inps: tf.Tensor, outs: tf.Tensor, masks: tf.Tensor):\n",
    "    \"\"\"\n",
    "    Get the loss on a batch without performing any updates.\n",
    "    Used for validation purposes\n",
    "    \"\"\"\n",
    "    preds = tf.expand_dims(m(inps, training=False), axis=-1)\n",
    "    outs = tf.expand_dims(outs, axis=-1)\n",
    "\n",
    "    loss = 0.0\n",
    "    mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "    for b in range(inps.shape[0]):\n",
    "        loss += mae(preds[b], outs[b], sample_weight = masks[b])\n",
    "\n",
    "    # turn it into mean\n",
    "    loss /= inps.shape[0]\n",
    "\n",
    "    # return mae loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_train(\n",
    "    m: keras.Model,\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    masks: np.ndarray,\n",
    "    batch_size: int = 32,\n",
    "    epochs: int = 1,\n",
    "    validation_split: float = 0.1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the given model.\n",
    "\n",
    "    Arguments:\n",
    "        - m: keras.Model - the model to train.\n",
    "        - x: np.ndarray - the numpy array of inputs.\n",
    "        - y: np.ndarray - the numpy array of outputs.\n",
    "        - masks: np.ndarray - the sample weights (1s and 0s).\n",
    "        - batch_size: int - how large the batches should be. Defaults to `32`.\n",
    "        - epochs: int - how many epochs to train for. Defaults to `1`.\n",
    "        - validation_split: float - how large the validation subset should be, in the range (0, 1]. Defaults to `0.1`.\n",
    "    \n",
    "    Note - The choice of np.ndarray is purely arbitrary, and this function can be modified to use tf.Tensors\n",
    "    \n",
    "    Note - shuffle code is provided in numpy, but commented out because of memory limitations that less powerful computers\n",
    "    may encounter.\n",
    "    \"\"\"\n",
    "    # shuffle\n",
    "    # shuffled_idxs = np.arange(x.shape[0])\n",
    "    # np.random.shuffle(shuffled_idxs)\n",
    "    # x = x[shuffled_idxs]\n",
    "    # y = y[shuffled_idxs]\n",
    "    # masks = masks[shuffled_idxs]\n",
    "\n",
    "    # generate validation\n",
    "    validation_size = int(x.shape[0] * validation_split)\n",
    "    x_val = x[:validation_size]\n",
    "    y_val = y[:validation_size]\n",
    "    masks_val = masks[:validation_size]\n",
    "    x = x[validation_size:]\n",
    "    y = y[validation_size:]\n",
    "    masks = masks[validation_size:]\n",
    "\n",
    "    # calculate number of batches to do\n",
    "    num_batches = x.shape[0] // batch_size\n",
    "    if x.shape[0] % batch_size != 0:\n",
    "        num_batches += 1\n",
    "\n",
    "    num_validation_batches = x_val.shape[0] // batch_size\n",
    "    if x_val.shape[0] % batch_size != 0:\n",
    "        num_validation_batches += 1\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        epoch_loss = 0.0\n",
    "        epoch_mae = 0.0\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            num_items = min(batch_size, x.shape[0] - batch * batch_size)\n",
    "\n",
    "            inps = tf.constant(x[batch * batch_size : batch * batch_size + num_items])\n",
    "            outs = tf.constant(y[batch * batch_size : batch * batch_size + num_items])\n",
    "            masks_ = tf.constant(\n",
    "                masks[batch * batch_size : batch * batch_size + num_items]\n",
    "            )\n",
    "\n",
    "            loss, mae_loss = train_batch(m, inps, outs, masks_)\n",
    "\n",
    "            epoch_loss += loss\n",
    "            epoch_mae += mae_loss\n",
    "\n",
    "            # log\n",
    "            print(\n",
    "                f\"Batch {batch+1}/{num_batches}\\t- loss: {loss.numpy():.5f}\\t- mae loss: {mae_loss.numpy():.5f}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "        epoch_loss /= num_batches\n",
    "        epoch_mae /= num_batches\n",
    "\n",
    "        # do validation\n",
    "        val_mae = 0.0\n",
    "        for batch in range(num_validation_batches):\n",
    "            num_items = min(batch_size, x_val.shape[0] - batch * batch_size)\n",
    "\n",
    "            inps = tf.constant(\n",
    "                x_val[batch * batch_size : batch * batch_size + num_items]\n",
    "            )\n",
    "            outs = tf.constant(\n",
    "                y_val[batch * batch_size : batch * batch_size + num_items]\n",
    "            )\n",
    "            masks_ = tf.constant(\n",
    "                masks_val[batch * batch_size : batch * batch_size + num_items]\n",
    "            )\n",
    "\n",
    "            mae_loss = noupdate_batch(m, inps, outs, masks_)\n",
    "\n",
    "            val_mae += mae_loss\n",
    "        val_mae /= num_validation_batches\n",
    "\n",
    "        # shuffle\n",
    "        # shuffled_idxs = np.arange(x.shape[0])\n",
    "        # np.random.shuffle(shuffled_idxs)\n",
    "        # x = x[shuffled_idxs]\n",
    "        # y = y[shuffled_idxs]\n",
    "        # masks = masks[shuffled_idxs]\n",
    "\n",
    "        print()\n",
    "        print(\n",
    "            f\"Epoch loss: {epoch_loss:.5f}\\tEpoch MAE: {epoch_mae:.5f}\\tVal MAE: {val_mae:.5f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.128074Z",
     "start_time": "2023-10-05T17:09:43.998369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 11:56:10.599639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8902\n",
      "2023-10-06 11:56:11.195355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 371/371\t- loss: 31.08087\t- mae loss: 31.06038\n",
      "Epoch loss: 21.88480\tEpoch MAE: 21.85213\tVal MAE: 24.92956\n",
      "Epoch 2\n",
      "Batch 371/371\t- loss: 26.99781\t- mae loss: 26.98098\n",
      "Epoch loss: 21.50853\tEpoch MAE: 21.48919\tVal MAE: 24.37587\n",
      "Epoch 3\n",
      "Batch 371/371\t- loss: 21.69537\t- mae loss: 21.66579\n",
      "Epoch loss: 21.06756\tEpoch MAE: 21.04674\tVal MAE: 24.07790\n",
      "Epoch 4\n",
      "Batch 371/371\t- loss: 15.15701\t- mae loss: 15.09221\n",
      "Epoch loss: 20.59314\tEpoch MAE: 20.56321\tVal MAE: 23.66617\n",
      "Epoch 5\n",
      "Batch 371/371\t- loss: 12.04827\t- mae loss: 11.95378\n",
      "Epoch loss: 20.20325\tEpoch MAE: 20.16215\tVal MAE: 23.24121\n",
      "Epoch 6\n",
      "Batch 371/371\t- loss: 10.94260\t- mae loss: 10.83698\n",
      "Epoch loss: 19.92563\tEpoch MAE: 19.87944\tVal MAE: 23.07961\n",
      "Epoch 7\n",
      "Batch 371/371\t- loss: 10.43401\t- mae loss: 10.32466\n",
      "Epoch loss: 19.69594\tEpoch MAE: 19.64114\tVal MAE: 22.88339\n",
      "Epoch 8\n",
      "Batch 371/371\t- loss: 10.12862\t- mae loss: 10.01545\n",
      "Epoch loss: 19.54281\tEpoch MAE: 19.48711\tVal MAE: 22.73209\n",
      "Epoch 9\n",
      "Batch 371/371\t- loss: 10.10537\t- mae loss: 10.00102\n",
      "Epoch loss: 19.40438\tEpoch MAE: 19.34461\tVal MAE: 22.61510\n",
      "Epoch 10\n",
      "Batch 371/371\t- loss: 9.66719\t- mae loss: 9.5505781\n",
      "Epoch loss: 19.29465\tEpoch MAE: 19.23451\tVal MAE: 22.42352\n",
      "Epoch 11\n",
      "Batch 371/371\t- loss: 9.77124\t- mae loss: 9.6658892\n",
      "Epoch loss: 19.22109\tEpoch MAE: 19.15963\tVal MAE: 22.41327\n",
      "Epoch 12\n",
      "Batch 371/371\t- loss: 9.30056\t- mae loss: 9.1937938\n",
      "Epoch loss: 19.17806\tEpoch MAE: 19.11860\tVal MAE: 22.22385\n",
      "Epoch 13\n",
      "Batch 371/371\t- loss: 9.71441\t- mae loss: 9.6102681\n",
      "Epoch loss: 19.11716\tEpoch MAE: 19.05785\tVal MAE: 22.19649\n",
      "Epoch 14\n",
      "Batch 371/371\t- loss: 9.26106\t- mae loss: 9.1564068\n",
      "Epoch loss: 19.07486\tEpoch MAE: 19.01541\tVal MAE: 22.23147\n",
      "Epoch 15\n",
      "Batch 371/371\t- loss: 10.44444\t- mae loss: 10.34795\n",
      "Epoch loss: 19.02807\tEpoch MAE: 18.96978\tVal MAE: 22.13148\n",
      "Epoch 16\n",
      "Batch 371/371\t- loss: 9.42920\t- mae loss: 9.3045588\n",
      "Epoch loss: 18.98001\tEpoch MAE: 18.91828\tVal MAE: 22.30918\n",
      "Epoch 17\n",
      "Batch 371/371\t- loss: 15.46819\t- mae loss: 15.40287\n",
      "Epoch loss: 19.03187\tEpoch MAE: 18.97750\tVal MAE: 22.00696\n",
      "Epoch 18\n",
      "Batch 371/371\t- loss: 8.64275\t- mae loss: 8.5225695\n",
      "Epoch loss: 18.85565\tEpoch MAE: 18.79560\tVal MAE: 21.96845\n",
      "Epoch 19\n",
      "Batch 371/371\t- loss: 9.05707\t- mae loss: 8.9422863\n",
      "Epoch loss: 18.80039\tEpoch MAE: 18.74182\tVal MAE: 21.92763\n",
      "Epoch 20\n",
      "Batch 371/371\t- loss: 8.77651\t- mae loss: 8.6562787\n",
      "Epoch loss: 18.75154\tEpoch MAE: 18.69162\tVal MAE: 21.91026\n"
     ]
    }
   ],
   "source": [
    "masked_train(model, inputs, outputs, output_masks, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section saves the current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.616528Z",
     "start_time": "2023-10-05T17:14:00.119617Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 2a3_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 2a3_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(f\"{desired_dataset}_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the noteboook creates a zipped csv submission file that can\n",
    "be submitted on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.624146Z",
     "start_time": "2023-10-05T17:14:00.601389Z"
    }
   },
   "outputs": [],
   "source": [
    "make_submissions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:01.233216Z",
     "start_time": "2023-10-05T17:14:00.608006Z"
    }
   },
   "outputs": [],
   "source": [
    "valid = False\n",
    "\n",
    "if os.path.exists(\"2a3_model\") and os.path.exists(\"dms_model\") and os.path.exists(\"test_sequences.csv\") and make_submissions:\n",
    "    valid = True\n",
    "    model_2a3 = keras.models.load_model(\"2a3_model\")\n",
    "    model_dms = keras.models.load_model(\"dms_model\")\n",
    "else:\n",
    "    print(\"Not going to create submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:01.247263Z",
     "start_time": "2023-10-05T17:14:01.233464Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def call_model(model_2a3, model_dms, inputs):\n",
    "    return model_2a3(inputs, training=False), model_dms(inputs, training=False)\n",
    "\n",
    "\n",
    "def pipeline(\n",
    "    model_2a3: keras.Model,\n",
    "    model_dms: keras.Model,\n",
    "    input_csv: str,\n",
    "    out: str,\n",
    "    batch_size: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Process test data and write submissions to a csv file\n",
    "\n",
    "    Arguments:\n",
    "        - model_2a3: keras.Model - the model trained on the 2a3 distribution\n",
    "        - model_dms: keras.Model - the model trained on the dms distribution\n",
    "        - input_csv: str - the name of the file that contains the test data \n",
    "        - out: str - the name of the file to write predictions to\n",
    "        - batch_size: int - how many predictions to make at a time\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "\n",
    "    # count how many lines we have in total\n",
    "    with open(input_csv) as file:\n",
    "        line = file.readline()  # ignore the header\n",
    "        # take the first line since we increment count in the loop\n",
    "        line = file.readline()\n",
    "        while line != \"\":\n",
    "            count += 1\n",
    "            line = file.readline()\n",
    "\n",
    "    # use that knowledge for a progress bar\n",
    "    with open(input_csv, \"r\") as file, open(out, \"w\") as outfile:\n",
    "        # write the header\n",
    "        outfile.write(\"id,reactivity_DMS_MaP,reactivity_2A3_MaP\\n\")\n",
    "\n",
    "        # get what index the things we need are\n",
    "        header = file.readline()\n",
    "        split_header = header.split(\",\")\n",
    "        min_idx = split_header.index(\"id_min\")\n",
    "        max_idx = split_header.index(\"id_max\")\n",
    "        sequence_idx = split_header.index(\"sequence\")\n",
    "\n",
    "        # only take the approved filtered lines\n",
    "        num_batches = count // batch_size\n",
    "        if count % batch_size != 0:\n",
    "            num_batches += 1\n",
    "        for batch in tqdm(range(num_batches)):\n",
    "            num_items = min(batch_size, count - batch * batch_size)\n",
    "\n",
    "            # initialize variables\n",
    "            inputs = np.zeros((num_items, NUM_REACTIVITIES, NUM_BASES))\n",
    "            min_seq_idxs = []\n",
    "            sequence_lengths = []\n",
    "\n",
    "            # collect the inputs\n",
    "            for i in range(num_items):\n",
    "                line = file.readline()\n",
    "                temp = line.split(\",\")\n",
    "                sequence = temp[sequence_idx]\n",
    "                max_seq_idx = int(temp[max_idx])\n",
    "                min_seq_idx = int(temp[min_idx])\n",
    "\n",
    "                # verify that everything is correct\n",
    "                assert len(sequence) + min_seq_idx - 1 == max_seq_idx\n",
    "\n",
    "                # store the data\n",
    "                inputs[i, : len(sequence)] = np.array(\n",
    "                    list(map(lambda letter: base_map[letter], sequence))\n",
    "                )\n",
    "                min_seq_idxs.append(min_seq_idx)\n",
    "                sequence_lengths.append(len(sequence))\n",
    "\n",
    "            # run inputs through the associated model\n",
    "            probs_2a3, probs_dms = call_model(model_2a3, model_dms, inputs)\n",
    "            probs_dms = probs_dms.numpy()\n",
    "            probs_2a3 = probs_2a3.numpy()\n",
    "\n",
    "            # write predictions\n",
    "            for i in range(num_items):\n",
    "                for seq_idx in range(\n",
    "                    min_seq_idxs[i], min_seq_idxs[i] + sequence_lengths[i]\n",
    "                ):\n",
    "                    outfile.write(\n",
    "                        f\"{seq_idx},{probs_dms[i, seq_idx - min_seq_idxs[i]]:.3f},{probs_2a3[i, seq_idx - min_seq_idxs[i]]:.3f}\\n\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-05T17:14:01.248308Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5250/5250 [04:42<00:00, 18.56it/s]\n"
     ]
    }
   ],
   "source": [
    "if valid:\n",
    "    pipeline(\n",
    "        model_2a3, model_dms, \"test_sequences.csv\", \"submission.csv\", batch_size=256\n",
    "    )\n",
    "else:\n",
    "    print(\"Not going to create submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipping submissions. This may take a while...\n",
      "  adding: submission.csv"
     ]
    }
   ],
   "source": [
    "if valid:\n",
    "    # zip our submission into an easily-uploadable zip file\n",
    "    print(\"zipping submissions. This may take a while...\")\n",
    "    os.system(\"zip submission.csv.zip submission.csv\")\n",
    "    print(\"Done zipping submissions!\")\n",
    "else:\n",
    "    print(\"Not going to zip submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
