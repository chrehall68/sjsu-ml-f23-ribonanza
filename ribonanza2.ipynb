{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ribonanza - Attempt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second approach to the [Stanford Ribonanza problem](https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/) that builds off the first approach.\n",
    "\n",
    "Major differences:\n",
    "- use of different model architecture\n",
    "- use of only filtered data (data in which SN_filter == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, this scores a 0.24796 MAE\n",
    "\n",
    "![Score](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Figure out why validation MAE differs so much from submission MAE\n",
    "- [x] Train more\n",
    "- [ ] Figure out what the reactivity_error means\n",
    "- [ ] Try a transformer, LSTM, or RNN architecture and take advantage of masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filesystem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your project directory should look like this:\n",
    "\n",
    "- `(project directory)`\n",
    "    - `ribonanza2.ipynb`\n",
    "    - `train_data.csv`\n",
    "    - `test_data.csv` (optional)\n",
    "\n",
    "`train_data.csv` is the only file necessary for training, and it can be downloaded from the kaggle competition linked in the description.\n",
    "\n",
    "`test_data.csv` is only necessary if you intend to make and submit predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 11:21:55.246424: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-05 11:21:56.306696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "import keras.optimizers as optimizers\n",
    "import pandas\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "# according to kaggle, this is the maximum # of reactivites to be used\n",
    "NUM_REACTIVITIES = 457\n",
    "\n",
    "# there are 4 different bases (AUCG)\n",
    "NUM_BASES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(out: str, key: str, value: str, file_name: str, force: bool):\n",
    "    \"\"\"\n",
    "    Filters a file to only take datapoints\n",
    "    whose values of `key` are `value`.\n",
    "\n",
    "    Parameters:\n",
    "        - out: str - the name of the file that will store the filtered datapoints\n",
    "        - key: str - the name of the key to look at\n",
    "        - value: str - the value that the key should have\n",
    "        - file_name: str - the name of the file that contains all the datapoints.\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    if os.path.exists(out) and not force:\n",
    "        print(\"File already exists, not doing any work\")\n",
    "        return\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    # count how many lines we have in total\n",
    "    with open(file_name) as file:\n",
    "        line = file.readline()  # ignore the header\n",
    "        line = (\n",
    "            file.readline()\n",
    "        )  # take the first line since we increment count in the loop\n",
    "        while line != \"\":\n",
    "            count += 1\n",
    "            line = file.readline()\n",
    "\n",
    "    # use that knowledge for a progress bar\n",
    "    with open(file_name, \"r\") as file, open(out, \"w\") as outfile:\n",
    "        # write the header\n",
    "        header = file.readline()\n",
    "        outfile.write(header)\n",
    "\n",
    "        # get what index the SN_filter is\n",
    "        SN_idx = header.split(\",\").index(key)\n",
    "\n",
    "        # only take the approved filtered lines\n",
    "        for _ in tqdm(range(count)):\n",
    "            line = file.readline()\n",
    "            temp = line.split(\",\")\n",
    "            if temp[SN_idx] == value:\n",
    "                outfile.write(line)\n",
    "\n",
    "\n",
    "def filter_train_data(force: bool = False):\n",
    "    \"\"\"\n",
    "    Filters the immense train_data.csv to only take datapoints\n",
    "    whose SN_filter (Signal to Noise filter) is 1. In other words,\n",
    "    we only take good reads. These filtered datapoints are then\n",
    "    written to the file provided\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\"train_data_filtered.csv\", \"SN_filter\", \"1\", \"train_data.csv\", force)\n",
    "\n",
    "\n",
    "def filter_2A3(force: bool = False):\n",
    "    \"\"\"\n",
    "    Only take the 2A3 points\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\n",
    "        \"train_data_2a3.csv\",\n",
    "        \"experiment_type\",\n",
    "        \"2A3_MaP\",\n",
    "        \"train_data_filtered.csv\",\n",
    "        force,\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_DMS(force: bool = False):\n",
    "    \"\"\"\n",
    "    Only take the DMS points\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\n",
    "        \"train_data_dms.csv\",\n",
    "        \"experiment_type\",\n",
    "        \"DMS_MaP\",\n",
    "        \"train_data_filtered.csv\",\n",
    "        force,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# filter our data\n",
    "filter_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# take the 2a3 points\n",
    "filter_2A3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# take the dms points\n",
    "filter_DMS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode inputs as\n",
    "# A : [1, 0, 0, 0]\n",
    "# U : [0, 1, 0, 0]\n",
    "# C : [0, 0, 1, 0]\n",
    "# G : [0, 0, 0, 1]\n",
    "base_map = {\n",
    "    \"A\": np.array([1, 0, 0, 0]),\n",
    "    \"U\": np.array([0, 1, 0, 0]),\n",
    "    \"C\": np.array([0, 0, 1, 0]),\n",
    "    \"G\": np.array([0, 0, 0, 1]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode inputs as\n",
    "# A : [1, 0, 0, 0]\n",
    "# U : [0, 1, 0, 0]\n",
    "# C : [0, 0, 1, 0]\n",
    "# G : [0, 0, 0, 1]\n",
    "base_map = {\n",
    "    \"A\": np.array([1, 0, 0, 0]),\n",
    "    \"U\": np.array([0, 1, 0, 0]),\n",
    "    \"C\": np.array([0, 0, 1, 0]),\n",
    "    \"G\": np.array([0, 0, 0, 1]),\n",
    "}\n",
    "\n",
    "\n",
    "def preprocess_csv(out: str, file_name: str, force: bool = False):\n",
    "    \"\"\"\n",
    "    Preprocess the csv and save the preprocessed data as a .npz file\n",
    "\n",
    "    Parameters:\n",
    "        - out: str - the name of the file to save the arrays to\n",
    "        - file_name: str - the name of the input csv file\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done).\n",
    "                Defaults to `False`\n",
    "    \"\"\"\n",
    "    if os.path.exists(out) and not force:\n",
    "        print(\"File already exists, not doing any work\")\n",
    "        return\n",
    "\n",
    "    df = pandas.read_csv(file_name)\n",
    "\n",
    "    inputs = np.zeros((len(df), NUM_REACTIVITIES, NUM_BASES))\n",
    "    outputs = np.zeros((len(df), NUM_REACTIVITIES))\n",
    "    output_masks = np.ones((len(df), NUM_REACTIVITIES), dtype=np.bool_)\n",
    "    errors = np.zeros((len(df), NUM_REACTIVITIES))\n",
    "\n",
    "    for index in tqdm(range(len(df))):\n",
    "        row = df.iloc[index]\n",
    "\n",
    "        # get the sequence\n",
    "        seq_len = len(row[\"sequence\"])\n",
    "\n",
    "        # map the base to its one-hot encoding\n",
    "        inputs[index, :seq_len] = np.array(\n",
    "            list(map(lambda letter: base_map[letter], row[\"sequence\"]))\n",
    "        )\n",
    "\n",
    "        # get all the reactivities and whether or not they are nan\n",
    "        reactivities = np.array(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda seq_idx: row[\"reactivity_\" + str(seq_idx + 1).rjust(4, \"0\")],\n",
    "                    range(seq_len),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        reactivity_errors = np.array(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda seq_idx: row[\"reactivity_error_\" + str(seq_idx + 1).rjust(4, \"0\")],\n",
    "                    range(seq_len),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        nan_locats = np.isnan(reactivities)\n",
    "\n",
    "        # where it is nan, store True, else false\n",
    "        output_masks[index, :seq_len] = nan_locats\n",
    "\n",
    "        # where it is not nan, store the reactivity and error, else 0\n",
    "        outputs[index, :seq_len] = np.where(nan_locats == False, reactivities, 0.0)\n",
    "        errors[index, :seq_len] = np.where(nan_locats == False, reactivity_errors, 0.0)\n",
    "\n",
    "    # save the outputs\n",
    "    np.savez_compressed(out, inputs=inputs, outputs=outputs, output_masks=output_masks, errors=errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\"train_data_2a3_preprocessed.npz\", \"train_data_2a3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\"train_data_dms_preprocessed.npz\", \"train_data_dms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the desired dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:33.551791Z",
     "start_time": "2023-10-05T17:09:33.535197Z"
    }
   },
   "outputs": [],
   "source": [
    "desired_dataset = \"dms\"  # either \"2a3\" or \"dms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.265620Z",
     "start_time": "2023-10-05T17:09:33.728394Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the npz file\n",
    "npz_file = np.load(f\"train_data_{desired_dataset}_preprocessed.npz\")\n",
    "\n",
    "# stored inputs, outputs, and output_masks\n",
    "# note: if visualizing, you may just want to only load outputs and bool_output_masks\n",
    "# since histplot takes a lot of RAM.\n",
    "inputs, outputs, bool_output_masks, errors = (\n",
    "    npz_file[\"inputs\"],\n",
    "    npz_file[\"outputs\"],\n",
    "    npz_file[\"output_masks\"],\n",
    "    npz_file[\"errors\"]\n",
    ")\n",
    "\n",
    "# close the npz file\n",
    "npz_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.843147Z",
     "start_time": "2023-10-05T17:09:42.265856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226925, 457)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to usable weights:\n",
    "# if it is meant to be masked, it should be worth 0, else it should be worth 1 - error\n",
    "output_masks = np.where(bool_output_masks, 0., 1.)\n",
    "output_masks -= errors\n",
    "output_masks[output_masks < 0.] = 0.\n",
    "output_masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the notebook allows for visualizing the reactivities of the\n",
    "current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.844615Z",
     "start_time": "2023-10-05T17:09:42.843236Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.851158Z",
     "start_time": "2023-10-05T17:09:42.846563Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226925/226925 [00:11<00:00, 20179.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 22491057/103704725 reactivities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if visualize:\n",
    "    visualized_items = []\n",
    "    for i in tqdm(range(len(outputs))):\n",
    "        for x in range(NUM_REACTIVITIES):\n",
    "            if not bool_output_masks[i, x]:\n",
    "                visualized_items.append(outputs[i, x])\n",
    "    visualized_items = np.array(visualized_items)\n",
    "    print(f\"took {len(visualized_items)}/{len(outputs)*NUM_REACTIVITIES} reactivities\")\n",
    "else:\n",
    "    print(\"Not visualizing. Set `visualize` to `True` to visualize data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.851272Z",
     "start_time": "2023-10-05T17:09:42.849526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if visualize:\n",
    "    seaborn.histplot(visualized_items, binwidth=0.1)\n",
    "else:\n",
    "    print(\"Not visualizing. Set `visualize` to `True` to visualize data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.859550Z",
     "start_time": "2023-10-05T17:09:42.852935Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_baseline_model():\n",
    "    inputs = layers.Input((NUM_REACTIVITIES, NUM_BASES))\n",
    "\n",
    "    x = layers.Conv1D(4, 16, 1, padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = layers.Conv1D(4, 16, 1, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv1D(4, 16, 2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\", activity_regularizer=\"l2\")(x)\n",
    "\n",
    "    x = layers.Dense(NUM_REACTIVITIES)(x)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "def load_model() -> keras.Model:\n",
    "    return keras.models.load_model(f\"{desired_dataset}_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.930488Z",
     "start_time": "2023-10-05T17:09:42.864218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 457, 4)]          0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 457, 4)            260       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 457, 4)            260       \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 229, 4)            260       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 916)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              939008    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 457)               468425    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,408,213\n",
      "Trainable params: 1,408,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 11:01:02.019888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:01:02.148214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:01:02.148531: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:01:02.150040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:01:02.150251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:01:02.150430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:01:02.218409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:01:02.218641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:01:02.218830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:01:02.218982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4279 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = make_baseline_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.934636Z",
     "start_time": "2023-10-05T17:09:42.924401Z"
    }
   },
   "outputs": [],
   "source": [
    "# compile the model with an optimizer and MAE loss\n",
    "model.compile(optimizer=optimizers.Adam(1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:43.992618Z",
     "start_time": "2023-10-05T17:09:43.053099Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_batch(m: keras.Model, inps: tf.Tensor, outs: tf.Tensor, masks: tf.Tensor):\n",
    "    \"\"\"\n",
    "    Get the loss on a batch and perform the corresponding weight updates.\n",
    "    Used for training purposes\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = tf.expand_dims(m(inps, training=True), axis=-1)\n",
    "        outs = tf.expand_dims(outs, axis=-1)\n",
    "        mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "        loss = 0.0\n",
    "        for b in range(inps.shape[0]):\n",
    "            loss += mae(outs[b], preds[b], sample_weight=masks[b])\n",
    "\n",
    "        # turn it into mean\n",
    "        loss /= inps.shape[0]\n",
    "    \n",
    "        # add the regularization losses\n",
    "        regularization_loss = tf.add_n(m.losses)\n",
    "        loss += regularization_loss\n",
    "        \n",
    "        # calculate gradients\n",
    "        grads = tape.gradient(loss, m.trainable_variables)\n",
    "\n",
    "    # apply grads\n",
    "    m.optimizer.apply_gradients(zip(grads, m.trainable_variables))\n",
    "\n",
    "    # return total loss, mae loss\n",
    "    return loss, loss - regularization_loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def noupdate_batch(m: keras.Model, inps: tf.Tensor, outs: tf.Tensor, masks: tf.Tensor):\n",
    "    \"\"\"\n",
    "    Get the loss on a batch without performing any updates.\n",
    "    Used for validation purposes\n",
    "    \"\"\"\n",
    "    preds = tf.expand_dims(m(inps, training=False), axis=-1)\n",
    "    outs = tf.expand_dims(outs, axis=-1)\n",
    "\n",
    "    loss = 0.0\n",
    "    mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "    for b in range(inps.shape[0]):\n",
    "        loss += mae(preds[b], outs[b], sample_weight = masks[b])\n",
    "\n",
    "    # turn it into mean\n",
    "    loss /= inps.shape[0]\n",
    "\n",
    "    # return mae loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_train(\n",
    "    m: keras.Model,\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    masks: np.ndarray,\n",
    "    batch_size: int = 32,\n",
    "    epochs: int = 1,\n",
    "    validation_split: float = 0.1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the given model.\n",
    "\n",
    "    Arguments:\n",
    "        - m: keras.Model - the model to train.\n",
    "        - x: np.ndarray - the numpy array of inputs.\n",
    "        - y: np.ndarray - the numpy array of outputs.\n",
    "        - masks: np.ndarray - the sample weights (1s and 0s).\n",
    "        - batch_size: int - how large the batches should be. Defaults to `32`.\n",
    "        - epochs: int - how many epochs to train for. Defaults to `1`.\n",
    "        - validation_split: float - how large the validation subset should be, in the range (0, 1]. Defaults to `0.1`.\n",
    "    \n",
    "    Note - The choice of np.ndarray is purely arbitrary, and this function can be modified to use tf.Tensors\n",
    "    \n",
    "    Note - shuffle code is provided in numpy, but commented out because of memory limitations that less powerful computers\n",
    "    may encounter.\n",
    "    \"\"\"\n",
    "    # shuffle\n",
    "    # shuffled_idxs = np.arange(x.shape[0])\n",
    "    # np.random.shuffle(shuffled_idxs)\n",
    "    # x = x[shuffled_idxs]\n",
    "    # y = y[shuffled_idxs]\n",
    "    # masks = masks[shuffled_idxs]\n",
    "\n",
    "    # generate validation\n",
    "    validation_size = int(x.shape[0] * validation_split)\n",
    "    x_val = x[:validation_size]\n",
    "    y_val = y[:validation_size]\n",
    "    masks_val = masks[:validation_size]\n",
    "    x = x[validation_size:]\n",
    "    y = y[validation_size:]\n",
    "    masks = masks[validation_size:]\n",
    "\n",
    "    # calculate number of batches to do\n",
    "    num_batches = x.shape[0] // batch_size\n",
    "    if x.shape[0] % batch_size != 0:\n",
    "        num_batches += 1\n",
    "\n",
    "    num_validation_batches = x_val.shape[0] // batch_size\n",
    "    if x_val.shape[0] % batch_size != 0:\n",
    "        num_validation_batches += 1\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        epoch_loss = 0.0\n",
    "        epoch_mae = 0.0\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            num_items = min(batch_size, x.shape[0] - batch * batch_size)\n",
    "\n",
    "            inps = tf.constant(x[batch * batch_size : batch * batch_size + num_items])\n",
    "            outs = tf.constant(y[batch * batch_size : batch * batch_size + num_items])\n",
    "            masks_ = tf.constant(\n",
    "                masks[batch * batch_size : batch * batch_size + num_items]\n",
    "            )\n",
    "\n",
    "            loss, mae_loss = train_batch(m, inps, outs, masks_)\n",
    "\n",
    "            epoch_loss += loss\n",
    "            epoch_mae += mae_loss\n",
    "\n",
    "            # log\n",
    "            print(\n",
    "                f\"Batch {batch+1}/{num_batches}\\t- loss: {loss.numpy():.5f}\\t- mae loss: {mae_loss.numpy():.5f}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "        epoch_loss /= num_batches\n",
    "        epoch_mae /= num_batches\n",
    "\n",
    "        # do validation\n",
    "        val_mae = 0.0\n",
    "        for batch in range(num_validation_batches):\n",
    "            num_items = min(batch_size, x.shape[0] - batch * batch_size)\n",
    "\n",
    "            inps = tf.constant(\n",
    "                x_val[batch * batch_size : batch * batch_size + num_items]\n",
    "            )\n",
    "            outs = tf.constant(\n",
    "                y_val[batch * batch_size : batch * batch_size + num_items]\n",
    "            )\n",
    "            masks_ = tf.constant(\n",
    "                masks_val[batch * batch_size : batch * batch_size + num_items]\n",
    "            )\n",
    "\n",
    "            mae_loss = noupdate_batch(m, inps, outs, masks_)\n",
    "\n",
    "            val_mae += mae_loss\n",
    "        val_mae /= num_validation_batches\n",
    "\n",
    "        # shuffle\n",
    "        # shuffled_idxs = np.arange(x.shape[0])\n",
    "        # np.random.shuffle(shuffled_idxs)\n",
    "        # x = x[shuffled_idxs]\n",
    "        # y = y[shuffled_idxs]\n",
    "        # masks = masks[shuffled_idxs]\n",
    "\n",
    "        print()\n",
    "        print(\n",
    "            f\"Epoch loss: {epoch_loss:.5f}\\tEpoch MAE: {epoch_mae:.5f}\\tVal MAE: {val_mae:.5f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.128074Z",
     "start_time": "2023-10-05T17:09:43.998369Z"
    }
   },
   "outputs": [],
   "source": [
    "masked_train(model, inputs, outputs, output_masks, epochs=5, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section saves the current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.616528Z",
     "start_time": "2023-10-05T17:14:00.119617Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dms_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dms_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(f\"{desired_dataset}_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the noteboook creates a zipped csv submission file that can\n",
    "be submitted on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.624146Z",
     "start_time": "2023-10-05T17:14:00.601389Z"
    }
   },
   "outputs": [],
   "source": [
    "make_submissions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:01.233216Z",
     "start_time": "2023-10-05T17:14:00.608006Z"
    }
   },
   "outputs": [],
   "source": [
    "valid = False\n",
    "\n",
    "if os.path.exists(\"2a3_model\") and os.path.exists(\"dms_model\") and os.path.exists(\"test_data.csv\") and make_submissions:\n",
    "    valid = True\n",
    "    model_2a3 = keras.models.load_model(\"2a3_model\")\n",
    "    model_dms = keras.models.load_model(\"dms_model\")\n",
    "else:\n",
    "    print(\"Not going to create submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:01.247263Z",
     "start_time": "2023-10-05T17:14:01.233464Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def call_model(model_2a3, model_dms, inputs):\n",
    "    return model_2a3(inputs, training=False), model_dms(inputs, training=False)\n",
    "\n",
    "\n",
    "def pipeline(\n",
    "    model_2a3: keras.Model,\n",
    "    model_dms: keras.Model,\n",
    "    input_csv: str,\n",
    "    out: str,\n",
    "    batch_size: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Process test data and write submissions to a csv file\n",
    "\n",
    "    Arguments:\n",
    "        - model_2a3: keras.Model - the model trained on the 2a3 distribution\n",
    "        - model_dms: keras.Model - the model trained on the dms distribution\n",
    "        - input_csv: str - the name of the file that contains the test data \n",
    "        - out: str - the name of the file to write predictions to\n",
    "        - batch_size: int - how many predictions to make at a time\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "\n",
    "    # count how many lines we have in total\n",
    "    with open(input_csv) as file:\n",
    "        line = file.readline()  # ignore the header\n",
    "        # take the first line since we increment count in the loop\n",
    "        line = file.readline()\n",
    "        while line != \"\":\n",
    "            count += 1\n",
    "            line = file.readline()\n",
    "\n",
    "    # use that knowledge for a progress bar\n",
    "    with open(input_csv, \"r\") as file, open(out, \"w\") as outfile:\n",
    "        # write the header\n",
    "        outfile.write(\"id,reactivity_DMS_MaP,reactivity_2A3_MaP\\n\")\n",
    "\n",
    "        # get what index the things we need are\n",
    "        header = file.readline()\n",
    "        split_header = header.split(\",\")\n",
    "        min_idx = split_header.index(\"id_min\")\n",
    "        max_idx = split_header.index(\"id_max\")\n",
    "        sequence_idx = split_header.index(\"sequence\")\n",
    "\n",
    "        # only take the approved filtered lines\n",
    "        num_batches = count // batch_size\n",
    "        if count % batch_size != 0:\n",
    "            num_batches += 1\n",
    "        for batch in tqdm(range(num_batches)):\n",
    "            num_items = min(batch_size, count - batch * batch_size)\n",
    "\n",
    "            # initialize variables\n",
    "            inputs = np.zeros((num_items, NUM_REACTIVITIES, NUM_BASES))\n",
    "            min_seq_idxs = []\n",
    "            sequence_lengths = []\n",
    "\n",
    "            # collect the inputs\n",
    "            for i in range(num_items):\n",
    "                line = file.readline()\n",
    "                temp = line.split(\",\")\n",
    "                sequence = temp[sequence_idx]\n",
    "                max_seq_idx = int(temp[max_idx])\n",
    "                min_seq_idx = int(temp[min_idx])\n",
    "\n",
    "                # verify that everything is correct\n",
    "                assert len(sequence) + min_seq_idx - 1 == max_seq_idx\n",
    "\n",
    "                # store the data\n",
    "                inputs[i, : len(sequence)] = np.array(\n",
    "                    list(map(lambda letter: base_map[letter], sequence))\n",
    "                )\n",
    "                min_seq_idxs.append(min_seq_idx)\n",
    "                sequence_lengths.append(len(sequence))\n",
    "\n",
    "            # run inputs through the associated model\n",
    "            probs_2a3, probs_dms = call_model(model_2a3, model_dms, inputs)\n",
    "            probs_dms = probs_dms.numpy()\n",
    "            probs_2a3 = probs_2a3.numpy()\n",
    "\n",
    "            # write predictions\n",
    "            for i in range(num_items):\n",
    "                for seq_idx in range(\n",
    "                    min_seq_idxs[i], min_seq_idxs[i] + sequence_lengths[i]\n",
    "                ):\n",
    "                    outfile.write(\n",
    "                        f\"{seq_idx},{probs_dms[i, seq_idx - min_seq_idxs[i]]:.3f},{probs_2a3[i, seq_idx - min_seq_idxs[i]]:.3f}\\n\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-05T17:14:01.248308Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 5179/5250 [04:40<00:04, 17.17it/s]"
     ]
    }
   ],
   "source": [
    "if valid:\n",
    "    pipeline(\n",
    "        model_2a3, model_dms, \"test_data.csv\", \"submission.csv\", batch_size=256\n",
    "    )\n",
    "else:\n",
    "    print(\"Not going to create submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "if valid:\n",
    "    # zip our submission into an easily-uploadable zip file\n",
    "    print(\"zipping submissions. This may take a while...\")\n",
    "    os.system(\"zip submission.csv.zip submission.csv\")\n",
    "    print(\"Done zipping submissions!\")\n",
    "else:\n",
    "    print(\"Not going to zip submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
