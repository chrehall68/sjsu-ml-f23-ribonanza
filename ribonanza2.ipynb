{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ribonanza 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second approach to the [Stanford Ribonanza problem](https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/) that builds off the first approach.\n",
    "\n",
    "Major differences:\n",
    "- use of different model architecture\n",
    "- use of only filtered data (data in which SN_filter == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, this scores a 0.26166 MAE\n",
    "\n",
    " ![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filesystem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your project directory should look like this:\n",
    "\n",
    "- `(project directory)`\n",
    "    - `ribonanza2.ipynb`\n",
    "    - `train_data.csv`\n",
    "\n",
    "`train_data.csv` is the only file necessary, and it can be downloaded from the kaggle competition linked in the description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 23:16:19.262193: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-02 23:16:20.000560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "import keras.optimizers as optimizers\n",
    "import pandas\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "# according to kaggle, this is the maximum # of reactivites to be used\n",
    "NUM_REACTIVITIES = 457\n",
    "\n",
    "# there are 4 different bases (AUCG)\n",
    "NUM_BASES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(out: str, key: str, value: str, file_name: str, force: bool):\n",
    "    \"\"\"\n",
    "    Filters a file to only take datapoints\n",
    "    whose values of `key` are `value`.\n",
    "\n",
    "    Parameters:\n",
    "        - out: str - the name of the file that will store the filtered datapoints\n",
    "        - key: str - the name of the key to look at\n",
    "        - value: str - the value that the key should have\n",
    "        - file_name: str - the name of the file that contains all the datapoints.\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    if os.path.exists(out) and not force:\n",
    "        print(\"File already exists, not doing any work\")\n",
    "        return\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    # count how many lines we have in total\n",
    "    with open(file_name) as file:\n",
    "        line = file.readline()  # ignore the header\n",
    "        line = (\n",
    "            file.readline()\n",
    "        )  # take the first line since we increment count in the loop\n",
    "        while line != \"\":\n",
    "            count += 1\n",
    "            line = file.readline()\n",
    "\n",
    "    # use that knowledge for a progress bar\n",
    "    with open(file_name, \"r\") as file, open(out, \"w\") as outfile:\n",
    "        # write the header\n",
    "        header = file.readline()\n",
    "        outfile.write(header)\n",
    "\n",
    "        # get what index the SN_filter is\n",
    "        SN_idx = header.split(\",\").index(key)\n",
    "\n",
    "        # only take the approved filtered lines\n",
    "        for _ in tqdm(range(count)):\n",
    "            line = file.readline()\n",
    "            temp = line.split(\",\")\n",
    "            if temp[SN_idx] == value:\n",
    "                outfile.write(line)\n",
    "\n",
    "\n",
    "def filter_train_data(force: bool = False):\n",
    "    \"\"\"\n",
    "    Filters the immense train_data.csv to only take datapoints\n",
    "    whose SN_filter (Signal to Noise filter) is 1. In other words,\n",
    "    we only take good reads. These filtered datapoints are then\n",
    "    written to the file provided\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\"train_data_filtered.csv\", \"SN_filter\", \"1\", \"train_data.csv\", force)\n",
    "\n",
    "\n",
    "def filter_2A3(force: bool = False):\n",
    "    \"\"\"\n",
    "    Only take the 2A3 points\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\n",
    "        \"train_data_2a3.csv\",\n",
    "        \"experiment_type\",\n",
    "        \"2A3_MaP\",\n",
    "        \"train_data_filtered.csv\",\n",
    "        force,\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_DMS(force: bool = False):\n",
    "    \"\"\"\n",
    "    Only take the DMS points\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\n",
    "        \"train_data_dms.csv\",\n",
    "        \"experiment_type\",\n",
    "        \"DMS_MaP\",\n",
    "        \"train_data_filtered.csv\",\n",
    "        force,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# filter our data\n",
    "filter_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# take the 2a3 points\n",
    "filter_2A3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# take the dms points\n",
    "filter_DMS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode inputs as\n",
    "# A : [1, 0, 0, 0]\n",
    "# U : [0, 1, 0, 0]\n",
    "# C : [0, 0, 1, 0]\n",
    "# G : [0, 0, 0, 1]\n",
    "base_map = {\n",
    "    \"A\": np.array([1, 0, 0, 0]),\n",
    "    \"U\": np.array([0, 1, 0, 0]),\n",
    "    \"C\": np.array([0, 0, 1, 0]),\n",
    "    \"G\": np.array([0, 0, 0, 1]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_csv(out: str, file_name: str, force: bool = False):\n",
    "    \"\"\"\n",
    "    Preprocess the csv and save the preprocessed data as a .npz file\n",
    "\n",
    "    Parameters:\n",
    "        - out: str - the name of the file to save the arrays to\n",
    "        - file_name: str - the name of the input csv file\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done).\n",
    "                Defaults to `False`\n",
    "    \"\"\"\n",
    "    if os.path.exists(out) and not force:\n",
    "        print(\"File already exists, not doing any work\")\n",
    "        return\n",
    "\n",
    "    df = pandas.read_csv(file_name)\n",
    "\n",
    "    inputs = np.zeros((len(df), NUM_REACTIVITIES, NUM_BASES))\n",
    "    outputs = np.zeros((len(df), NUM_REACTIVITIES))\n",
    "    output_masks = np.ones((len(df), NUM_REACTIVITIES), dtype=np.bool_)\n",
    "\n",
    "    for index in tqdm(range(len(df))):\n",
    "        row = df.iloc[index]\n",
    "\n",
    "        # get the sequence\n",
    "        seq_len = len(row[\"sequence\"])\n",
    "\n",
    "        # map the base to its one-hot encoding\n",
    "        inputs[index, :seq_len] = np.array(\n",
    "            list(map(lambda letter: base_map[letter], row[\"sequence\"]))\n",
    "        )\n",
    "\n",
    "        # get all the reactivities and whether or not they are nan\n",
    "        reactivities = np.array(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda seq_idx: row[\"reactivity_\" + str(seq_idx + 1).rjust(4, \"0\")],\n",
    "                    range(seq_len),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        nan_locats = np.isnan(reactivities)\n",
    "\n",
    "        # where it is nan, store True, else false\n",
    "        output_masks[index, :seq_len] = nan_locats\n",
    "\n",
    "        # where it is not nan, store the reactiviy, else 0\n",
    "        outputs[index, :seq_len] = np.where(nan_locats == False, reactivities, 0.0)\n",
    "\n",
    "    # save the outputs\n",
    "    np.savez_compressed(out, inputs=inputs, outputs=outputs, output_masks=output_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\"train_data_2a3_preprocessed.npz\", \"train_data_2a3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\"train_data_dms_preprocessed.npz\", \"train_data_dms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the desired dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_dataset = \"2a3\"  # either \"2a3\" or \"dms\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the npz file\n",
    "npz_file = np.load(f\"train_data_{desired_dataset}_preprocessed.npz\")\n",
    "\n",
    "# stored inputs, outputs, and output_masks\n",
    "inputs, outputs, output_masks = (\n",
    "    npz_file[\"inputs\"],\n",
    "    npz_file[\"outputs\"],\n",
    "    npz_file[\"output_masks\"],\n",
    ")\n",
    "\n",
    "# close the npz file\n",
    "npz_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210992, 457)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to usable weights:\n",
    "# if it is meant to be masked, it should be worth 0, else it should be worth 1\n",
    "output_masks = np.where(output_masks, 0, 1)\n",
    "output_masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not visualizing. Set `visualize` to `True` to visualize data\n"
     ]
    }
   ],
   "source": [
    "if visualize:\n",
    "    visualized_items = []\n",
    "    for i in tqdm(range(len(outputs))):\n",
    "        for x in range(NUM_REACTIVITIES):\n",
    "            if not output_masks[i, x]:\n",
    "                visualized_items.append(outputs[i, x])\n",
    "    visualized_items = np.array(visualized_items)\n",
    "else:\n",
    "    print(\"Not visualizing. Set `visualize` to `True` to visualize data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not visualizing. Set `visualize` to `True` to visualize data\n"
     ]
    }
   ],
   "source": [
    "if visualize:\n",
    "    seaborn.histplot(visualized_items, stat=\"count\", binwidth=0.1)\n",
    "else:\n",
    "    print(\"Not visualizing. Set `visualize` to `True` to visualize data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_baseline_model():\n",
    "    inputs = layers.Input((NUM_REACTIVITIES, NUM_BASES))\n",
    "    \n",
    "    x = layers.Conv1D(4, 16, 1, padding='same', activation='relu')(inputs)\n",
    "    x = layers.Conv1D(4, 16, 1, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv1D(4, 16, 2, padding='same', activation='relu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\", activity_regularizer=\"l2\")(x)\n",
    "\n",
    "    x = layers.Dense(NUM_REACTIVITIES)(x)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 23:16:31.358617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 23:16:31.387324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 23:16:31.387564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 23:16:31.389145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 23:16:31.389367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 23:16:31.389516: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 23:16:31.471665: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 23:16:31.471867: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 23:16:31.472023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 23:16:31.472137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10035 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:06:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 457, 4)]          0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 457, 4)            260       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 457, 4)            260       \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 229, 4)            260       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 916)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              939008    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 457)               468425    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,408,213\n",
      "Trainable params: 1,408,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_baseline_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizers.Adam(1e-4), loss=tf.keras.losses.MeanAbsoluteError())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_batch(m:keras.Model, inps:tf.Tensor, outs:tf.Tensor, masks:tf.Tensor):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = tf.expand_dims(m(inps, training=True), axis=-1)\n",
    "        outs = tf.expand_dims(outs, axis=-1)\n",
    "\n",
    "        loss = 0.0\n",
    "        for b in range(inps.shape[0]):\n",
    "            loss += m.loss(preds[b], outs[b], masks[b])\n",
    "        \n",
    "        # turn it into mean\n",
    "        loss /= inps.shape[0]\n",
    "        \n",
    "        # add the regularization losses\n",
    "        loss += model.losses\n",
    "        # calculate gradients\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "    # apply grads\n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # return total loss, mae loss\n",
    "    return loss, loss - model.losses\n",
    "\n",
    "@tf.function\n",
    "def noupdate_batch(m:keras.Model, inps:tf.Tensor, outs:tf.Tensor, masks:tf.Tensor):\n",
    "    preds = tf.expand_dims(m(inps, training=False), axis=-1)\n",
    "    outs = tf.expand_dims(outs, axis=-1)\n",
    "    \n",
    "    loss = 0.0\n",
    "    for b in range(inps.shape[0]):\n",
    "        loss += m.loss(preds[b], outs[b], masks[b])\n",
    "    \n",
    "    # turn it into mean\n",
    "    loss /= inps.shape[0]\n",
    "    \n",
    "    # add the regularization losses\n",
    "    loss += model.losses\n",
    "        \n",
    "    # return total loss, mae loss\n",
    "    return loss, loss - model.losses\n",
    "\n",
    "def masked_train(m: keras.Model, x: np.ndarray, y: np.ndarray, masks: np.ndarray, batch_size: int = 32, epochs: int = 1, validation_split: float = 0.1):\n",
    "    # shuffle\n",
    "    # shuffled_idxs = np.arange(x.shape[0])\n",
    "    # np.random.shuffle(shuffled_idxs)\n",
    "    # x = x[shuffled_idxs]\n",
    "    # y = y[shuffled_idxs]\n",
    "    # masks = masks[shuffled_idxs]\n",
    "    \n",
    "    # generate validation\n",
    "    validation_size = int(x.shape[0] * validation_split)\n",
    "    x_val = x[:validation_size]\n",
    "    y_val = y[:validation_size]\n",
    "    masks_val= masks[:validation_size]\n",
    "    x = x[validation_size:]\n",
    "    y = y[validation_size:]\n",
    "    masks = masks[validation_size:]\n",
    "\n",
    "    # calculate number of batches to do\n",
    "    num_batches = x.shape[0] // batch_size\n",
    "    if x.shape[0] % batch_size != 0:\n",
    "        num_batches += 1\n",
    "\n",
    "    num_validation_batches = x_val.shape[0] // batch_size\n",
    "    if x_val.shape[0] % batch_size != 0:\n",
    "        num_validation_batches += 1\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        epoch_loss = 0.0\n",
    "        epoch_mae = 0.0\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            num_items = min(batch_size, x.shape[0] - batch*batch_size)\n",
    "\n",
    "            inps = tf.constant(x[batch*batch_size:batch*batch_size+num_items])\n",
    "            outs = tf.constant(y[batch*batch_size:batch*batch_size+num_items])\n",
    "            masks_ = tf.constant(masks[batch*batch_size:batch*batch_size+num_items])\n",
    "\n",
    "            loss, mae_loss = train_batch(m, inps, outs, masks_)\n",
    "\n",
    "            epoch_loss += loss\n",
    "            epoch_mae += mae_loss\n",
    "\n",
    "            # log\n",
    "            print(f\"Batch {batch+1}/{num_batches}\\t- loss: {loss[0].numpy():.5f}\\t- mae loss: {mae_loss[0].numpy():.5f}\", end=\"\\r\")\n",
    "        epoch_loss /= num_batches\n",
    "        epoch_mae /= num_batches\n",
    "\n",
    "        # do validation\n",
    "        val_loss = 0.0\n",
    "        val_mae = 0.0\n",
    "        for batch in range(num_validation_batches):\n",
    "            num_items = min(batch_size, x.shape[0] - batch*batch_size)\n",
    "\n",
    "            inps = tf.constant(x_val[batch*batch_size:batch*batch_size+num_items])\n",
    "            outs = tf.constant(y_val[batch*batch_size:batch*batch_size+num_items])\n",
    "            masks_ = tf.constant(masks_val[batch*batch_size:batch*batch_size+num_items])\n",
    "\n",
    "            loss, mae_loss = noupdate_batch(m, inps, outs, masks_)\n",
    "\n",
    "            val_loss += loss\n",
    "            val_mae += mae_loss\n",
    "        val_loss /= num_validation_batches\n",
    "        val_mae /= num_validation_batches\n",
    "\n",
    "        # shuffle\n",
    "        # shuffled_idxs = np.arange(x.shape[0])\n",
    "        # np.random.shuffle(shuffled_idxs)\n",
    "        # x = x[shuffled_idxs]\n",
    "        # y = y[shuffled_idxs]\n",
    "        # masks = masks[shuffled_idxs]\n",
    "        \n",
    "        print()\n",
    "        print(f\"Epoch loss: {epoch_loss[0]:.5f}\\tEpoch MAE: {epoch_mae[0]:.5f}\\tVal loss: {val_loss[0]:.5f}\\tVal MAE: {val_mae[0]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 23:16:41.533805: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-10-02 23:16:42.377644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1484/1484\t- loss: 0.08924\t- mae loss: 0.08902\n",
      "Epoch loss: 0.08359\tEpoch MAE: 0.08221\tVal loss: 0.08229\tVal MAE: 0.08194\n",
      "Epoch 2\n",
      "Batch 1484/1484\t- loss: 0.08722\t- mae loss: 0.08690\n",
      "Epoch loss: 0.08191\tEpoch MAE: 0.08142\tVal loss: 0.08149\tVal MAE: 0.08107\n",
      "Epoch 3\n",
      "Batch 1484/1484\t- loss: 0.08243\t- mae loss: 0.08147\n",
      "Epoch loss: 0.08162\tEpoch MAE: 0.08122\tVal loss: 0.08138\tVal MAE: 0.08092\n",
      "Epoch 4\n",
      "Batch 1484/1484\t- loss: 0.07300\t- mae loss: 0.06994\n",
      "Epoch loss: 0.08144\tEpoch MAE: 0.08109\tVal loss: 0.08126\tVal MAE: 0.08064\n",
      "Epoch 5\n",
      "Batch 1484/1484\t- loss: 0.06245\t- mae loss: 0.05555\n",
      "Epoch loss: 0.08120\tEpoch MAE: 0.08081\tVal loss: 0.08105\tVal MAE: 0.08044\n"
     ]
    }
   ],
   "source": [
    "masked_train(model, inputs, outputs, output_masks, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 2a3_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 2a3_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(f\"{desired_dataset}_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = False\n",
    "if os.path.exists(\"2a3_model\") and os.path.exists(\"dms_model\"):\n",
    "    valid = True\n",
    "    model_2a3 = keras.models.load_model(\"2a3_model\")\n",
    "    model_dms = keras.models.load_model(\"dms_model\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def call_model(model_2a3, model_dms, inputs):\n",
    "    return model_2a3(inputs, training=False), model_dms(inputs, training=False)\n",
    "\n",
    "def pipeline(model_2a3:keras.Model, model_dms:keras.Model, input_csv:str, out:str, batch_size:int):\n",
    "    count = 0\n",
    "\n",
    "    # count how many lines we have in total\n",
    "    with open(input_csv) as file:\n",
    "        line = file.readline()  # ignore the header\n",
    "        # take the first line since we increment count in the loop\n",
    "        line = file.readline()\n",
    "        while line != \"\":\n",
    "            count += 1\n",
    "            line = file.readline()\n",
    "\n",
    "    # use that knowledge for a progress bar\n",
    "    with open(input_csv, \"r\") as file, open(out, \"w\") as outfile:\n",
    "        # write the header\n",
    "        outfile.write(\"id,reactivity_DMS_MaP,reactivity_2A3_MaP\\n\")\n",
    "\n",
    "        # get what index the things we need are\n",
    "        header = file.readline()\n",
    "        split_header = header.split(\",\")\n",
    "        min_idx = split_header.index(\"id_min\")\n",
    "        max_idx = split_header.index(\"id_max\")\n",
    "        sequence_idx = split_header.index(\"sequence\")\n",
    "\n",
    "        # only take the approved filtered lines\n",
    "        num_batches = count // batch_size\n",
    "        if count % batch_size != 0:\n",
    "            num_batches += 1\n",
    "        for batch in tqdm(range(num_batches)):\n",
    "            num_items = min(batch_size, count - batch*batch_size)\n",
    "\n",
    "            inputs = np.zeros((num_items, NUM_REACTIVITIES, NUM_BASES))\n",
    "            min_seq_idxs = []\n",
    "            sequence_lengths = []\n",
    "            \n",
    "            for i in range(num_items):\n",
    "                line = file.readline()\n",
    "                temp = line.split(\",\")\n",
    "                sequence = temp[sequence_idx]\n",
    "                max_seq_idx = int(temp[max_idx])\n",
    "                min_seq_idx = int(temp[min_idx])\n",
    "                assert len(sequence) + min_seq_idx -1 == max_seq_idx\n",
    "\n",
    "                inputs[i, :len(sequence)] = np.array(list(map(lambda letter: base_map[letter], sequence)))\n",
    "                min_seq_idxs.append(min_seq_idx)\n",
    "                sequence_lengths.append(len(sequence))\n",
    "\n",
    "            # run it through the associated model\n",
    "            probs_2a3, probs_dms = call_model(model_2a3, model_dms, inputs)\n",
    "            probs_dms = probs_dms.numpy()\n",
    "            probs_2a3 = probs_2a3.numpy()\n",
    "\n",
    "            for i in range(num_items):\n",
    "                for seq_idx in range(min_seq_idxs[i], min_seq_idxs[i] + sequence_lengths[i]):\n",
    "                    outfile.write(f\"{seq_idx},{probs_dms[i, seq_idx - min_seq_idxs[i]]:.3f},{probs_2a3[i, seq_idx - min_seq_idxs[i]]:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5250/5250 [07:22<00:00, 11.86it/s]\n"
     ]
    }
   ],
   "source": [
    "if valid:\n",
    "    pipeline(model_2a3, model_dms, \"test_sequences.csv\", \"submission.csv\", batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip our submission into an easily-uploadable zip file\n",
    "os.system(\"zip submission.csv.zip submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
