{"cells":[{"cell_type":"markdown","id":"658eee87e2f641d0","metadata":{"collapsed":false,"id":"658eee87e2f641d0"},"source":["# ML@SJSU Ribonanza Project\n","### Active Members / Roles\n","Haydon Behl / Project Lead\n","Neal Chandra\n","Eliot Hall\n","<div style=\"text-align: right\">If your name isn't here yet, add it to the Project Members google sheet (or speak to me abt it)  -Haydon&nbsp;&nbsp;&nbsp;</div>"]},{"cell_type":"markdown","id":"e0703a07b1bddafd","metadata":{"collapsed":false,"id":"e0703a07b1bddafd"},"source":["## Todo list\n","- [x] Finish baseline model\n","- [ ] Convert data preprocess pipeline\n","| Current data format: 000100010001...[1 rows, 1644 columns, batchsize 32]\n","| New data format: 0001,0001,0001...[4 rows, 411 columns, batchsize 32]\n","| Assigned Members: Neal, Haydon\n","- [ ] Create 2-label Classification model (Reactive / Non-Reactive)\n","- [ ] Produce a submission with current model (and design a better way to process test_sequences.csv)\n","| Assigned Members: Haydon\n"]},{"cell_type":"markdown","id":"a3c568dbf9b263e5","metadata":{"collapsed":false,"id":"a3c568dbf9b263e5"},"source":["### Getting Started Instructions\n","1. Create a new project with your IDE (recommended: pycharm)\n","2. Keep this .ipynb notebook in the root directory of your project.\n","3. Download all files from the Kaggle page.\n","4. Extract to ./input (notebook must be able to reach './input/train_data.csv')\n","5. Using a conda venv (recommended) install the required modules below."]},{"cell_type":"markdown","id":"12705f1265be8a48","metadata":{"collapsed":false,"id":"12705f1265be8a48"},"source":["### Modules"]},{"cell_type":"code","execution_count":null,"id":"744f5b758414313b","metadata":{"id":"744f5b758414313b","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["import os\n","import time\n","import warnings\n","\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","#import prince # multiple correspondence analysis\n","#import altair as alt # data transformer, pre-evaluates data\n","#from sklearn.tree import DecisionTreeRegressor\n","#from sklearn.ensemble import RandomForestRegressor\n","#from xgboost import XGBRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error\n","#import seaborn as sns\n","#import matplotlib.pyplot as plt # data visuals with matplotlib my beloved"]},{"cell_type":"markdown","id":"39d6e3980282d287","metadata":{"collapsed":false,"id":"39d6e3980282d287"},"source":["### Load Data / Inspect\n","Note: Only run if you don't already have preprocessed data local on your machine (ex. train_2A3_1000.bin)"]},{"cell_type":"code","execution_count":null,"id":"99a40b8e5256dac4","metadata":{"id":"99a40b8e5256dac4","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# Load data\n","data = pd.read_csv('./input/train_data.csv')\n","train_data, test_data = train_test_split(data)\n","train_2A3_MaP, train_DMS_MaP, test_2A3_MaP, test_DMS_MaP = 0, 0, 0, 0\n","for i, g in test_data.groupby('experiment_type'):\n","    globals()['test_' + str(i)] = g\n","for i, g in train_data.groupby('experiment_type'):\n","    globals()['train_' + str(i)] = g"]},{"cell_type":"code","execution_count":null,"id":"3a98e30fd487b0b","metadata":{"id":"3a98e30fd487b0b","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# Analysis of tests/sample submission\n","test_set = pd.read_csv('./input/test_sequences.csv')\n","samplesub = pd.read_csv('./input/sample_submission.csv')\n","for col in test_set:\n","    print(col, end=\" \")\n","print(\"\\n\")\n","for col in samplesub:\n","    print(col, end=\" \")"]},{"cell_type":"markdown","id":"275ba80f562bc358","metadata":{"collapsed":false,"id":"275ba80f562bc358"},"source":["### Preprocessing Functions\n","Takes the input data and preprocesses into usable binary data format"]},{"cell_type":"code","execution_count":null,"id":"9a581f1e432f3bed","metadata":{"id":"9a581f1e432f3bed","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["#todo; Convert data preprocess pipeline\n","base_dict = {   'A': \"1,0,0,0,\",\n","                'C': \"0,1,0,0,\",\n","                'G': \"0,0,1,0,\",\n","                'U': \"0,0,0,1,\"}\n","\n","# progression based csv preprocessing (defunct; csv files are too chonky)\n","def preprocess_csv(df, csv, limit=100):\n","    start = 0\n","    line = \"\"\n","    features = []\n","    for n in range(-205,206):\n","        if n < 0:\n","            sign = 'n'\n","        else:\n","            sign = ''\n","        end = sign + str(abs(n))\n","        f = ['b_A_' + end, 'b_C_' + end, 'b_G_' + end, 'b_U_' + end]\n","        for i in f:\n","            features.append(i)\n","            line += i + \",\"\n","    line += 'reactivities'\n","    sub_file = open(csv, \"w\")\n","    sub_file.write(line)\n","    sub_file.close()\n","\n","    b_id=-1\n","    for index, row in df.iterrows():\n","        start+=1\n","        seq = row['sequence']\n","        sub_file = open(csv, \"a\")\n","        for i in range(len(seq)):\n","            b_id+=1\n","            if i<9:\n","                r = row['reactivity_000'+str(i+1)]\n","            elif i<99:\n","                r = row['reactivity_00'+str(i+1)]\n","            else:\n","                r = row['reactivity_0'+str(i+1)]\n","            if not np.isnan(r):\n","                line = \"\\n\"\n","                for n in range(-205,206):\n","                    if 0 <= (n + i) < len(seq):\n","                        line+= base_dict[seq[i+n]]\n","                    else:\n","                        line+= \"0,0,0,0,\"\n","                if r >= 0:\n","                    line+= str(r)\n","                else:\n","                    line+= '0'\n","                sub_file.write(line)\n","        sub_file.close()\n","        if start >= limit:\n","            break\n","\n","features = []\n","for n in range(-205,206):\n","    if n < 0:\n","        sign = 'n'\n","    else:\n","        sign = ''\n","    end = sign + str(abs(n))\n","    f = ['b_A_' + end, 'b_C_' + end, 'b_G_' + end, 'b_U_' + end]\n","    for i in f:\n","        features.append(i)\n","\n","print(len(features))"]},{"cell_type":"code","execution_count":null,"id":"63dbd27b7dcab204","metadata":{"id":"63dbd27b7dcab204"},"outputs":[],"source":["base_dict = {   'A': \"1000\",\n","                'C': \"0100\",\n","                'G': \"0010\",\n","                'U': \"0001\"}\n","\n","# binary preprocessing (active)\n","def preprocess_bin(df, bin, limit=100):\n","    # limit for insanity\n","    start = 0\n","    b_id=-1\n","    for index, row in df.iterrows():\n","        start+=1\n","        seq = row['sequence']\n","        sub_file = open(bin, \"a\")\n","        for i in range(len(seq)):\n","            b_id+=1\n","            if i<9:\n","                r = row['reactivity_000'+str(i+1)]\n","            elif i<99:\n","                r = row['reactivity_00'+str(i+1)]\n","            else:\n","                r = row['reactivity_0'+str(i+1)]\n","            if not np.isnan(r):\n","                line = \"\\n\"\n","                for n in range(-205,206):\n","                    if 0 <= (n + i) < len(seq):\n","                        line+= base_dict[seq[i+n]]\n","                    else:\n","                        line+= \"0000\"\n","                if r > 0:\n","                    line+= \",\"+str(r)\n","                else:\n","                    line+= ',0'\n","                sub_file.write(line)\n","        sub_file.close()\n","        if start >= limit:\n","            break\n","\n","features = []\n","for n in range(-205,206):\n","    if n < 0:\n","        sign = 'n'\n","    else:\n","        sign = ''\n","    end = sign + str(abs(n))\n","    f = ['b_A_' + end, 'b_C_' + end, 'b_G_' + end, 'b_U_' + end]\n","    for i in f:\n","        features.append(i)\n","\n","print(len(features))"]},{"cell_type":"code","execution_count":null,"id":"0014b1c5","metadata":{"id":"0014b1c5"},"outputs":[],"source":["# mass preprocess csv\n","for samples in [100, 1000, 3000]:      #Note: csv is a terrible file format. Currently only in use for test splits.\n","    preprocess_csv(train_2A3_MaP, './train_2A3_'+str(samples)+'.csv', limit=samples)\n","    preprocess_csv(test_2A3_MaP, './test_2A3_'+str(samples)+'.csv', limit=samples)\n","    preprocess_csv(train_DMS_MaP, './train_DMS_'+str(samples)+'.csv', limit=samples)\n","    preprocess_csv(test_DMS_MaP, './test_DMS_'+str(samples)+'.csv', limit=samples)"]},{"cell_type":"code","execution_count":null,"id":"d77f10f3bdfc0133","metadata":{"id":"d77f10f3bdfc0133"},"outputs":[],"source":["# mass preprocess bin\n","for samples in [100, 1000, 3000]:   #Note: mass preprocess only needs to be run once; preprocessed data is stored in root directory.\n","    preprocess_bin(train_2A3_MaP, './train_2A3_'+str(samples)+'.bin', limit=samples)\n","    preprocess_bin(test_2A3_MaP, './test_2A3_'+str(samples)+'.bin', limit=samples)\n","    preprocess_bin(train_DMS_MaP, './train_DMS_'+str(samples)+'.bin', limit=samples)\n","    preprocess_bin(test_DMS_MaP, './test_DMS_'+str(samples)+'.bin', limit=samples)"]},{"cell_type":"markdown","id":"7db3e37a94c23395","metadata":{"collapsed":false,"id":"7db3e37a94c23395"},"source":["### Model Preparation\n","Self-explanatory functions. Only thing to note is that we're using a dataset generator for model training, as loading all the data at once will crash."]},{"cell_type":"code","execution_count":null,"id":"4398528238a3fece","metadata":{"id":"4398528238a3fece"},"outputs":[],"source":["fp_train_2A3 = './train_2A3_1000.bin'           # determines which files are used for training/testing\n","fp_test_2A3 = './test_2A3_100.csv'              # 1000 seqs seem to be enough for good training (3000 makes a small difference)\n","fp_train_DMS = './train_DMS_1000.bin'           # 100 for testing\n","fp_test_DMS = './test_DMS_100.csv'"]},{"cell_type":"code","execution_count":null,"id":"93d0436890341643","metadata":{"id":"93d0436890341643","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# loading the CSV will break everything, so a generator function must be used in its place\n","input_shape = (1644, 1)\n","# generator function for csv\n","def read_csv(filename):\n","    first_line = True\n","    with open(filename, 'r') as f:\n","        for line in f.readlines():\n","            if first_line:\n","                first_line = False\n","            else:\n","                record = line.rstrip().split(',')\n","                feature = [int(n) for n in record[:-1]]\n","                label = float(record[-1])\n","                yield feature, [label]\n","# generator function for bin\n","def read_bin(filename):\n","    first_line = True\n","    with open(filename, 'r') as f:\n","        for line in f.readlines():\n","            if first_line:\n","                first_line = False\n","            else:\n","                record = line.rstrip().split(',')\n","                #print(record)\n","                label = float(record[1])\n","                record = record[0]\n","                feature = [int(n) for n in record]\n","                yield feature, [label]\n","\n","# dynamic dataset generator (works for .csv and for .bin)\n","def get_dataset(filename):\n","    if filename[-1] == \"v\":  # csv\n","        generator = lambda: read_csv(filename)\n","    elif filename[-1] == \"n\":  # bin\n","        generator = lambda: read_bin(filename)\n","    return tf.data.Dataset.from_generator(\n","        generator,\n","        output_signature=(\n","        tf.TensorSpec(shape=(1644), dtype=tf.int32),\n","        tf.TensorSpec(shape=(1), dtype=tf.float32,)\n","        )\n","    )\n","# define features ! will not be necessary after proprocess pipeline conversion\n","feat_dict = dict()\n","features = []\n","for n in range(-205,206):\n","    if n < 0:\n","        sign = 'n'\n","    else:\n","        sign = ''\n","    end = sign + str(abs(n))\n","    f = ['b_A_' + end, 'b_C_' + end, 'b_G_' + end, 'b_U_' + end]\n","    for i in f:\n","        features.append(i)\n","for i in features:\n","    feat_dict[i] = []\n","print(features)"]},{"cell_type":"code","execution_count":null,"id":"422c06d2ebe71aeb","metadata":{"id":"422c06d2ebe71aeb","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["batch_size = 32\n","# Prepare training set\n","train_dataset_2A3 = get_dataset(fp_train_2A3)\n","train_dataset_2A3 = train_dataset_2A3.batch(batch_size)\n","# Prepare the validation set\n","test_dataset_2A3 = get_dataset(fp_test_2A3)\n","test_dataset_2A3 = train_dataset_2A3.batch(batch_size)"]},{"cell_type":"code","execution_count":null,"id":"e5689ba24bde044e","metadata":{"id":"e5689ba24bde044e"},"outputs":[],"source":["batch_size = 32\n","# Prepare training set\n","train_dataset_DMS = get_dataset(fp_train_DMS)\n","train_dataset_DMS = train_dataset_DMS.batch(batch_size)\n","# Prepare the validation set\n","test_dataset_DMS = get_dataset(fp_test_DMS)\n","test_dataset_DMS = train_dataset_DMS.batch(batch_size)"]},{"cell_type":"code","execution_count":null,"id":"8946308feb179998","metadata":{"id":"8946308feb179998","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# Defining loss function\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n","accuracy_metric = tf.keras.metrics.Accuracy()\n","\n","# Calculate loss\n","def loss_fn(y_true, y_pred):\n","    mae = tf.keras.losses.MeanAbsoluteError()\n","    #print (mae(y_true, y_pred))\n","    return mae(y_true, y_pred)   # Kaggle submission will be graded on MAE\n","\n","#def accuracy_fn(gt_label, output):\n","    #todo; make accuracy function to assess model training"]},{"cell_type":"markdown","id":"f55a8b4d3c5deb7b","metadata":{"collapsed":false,"id":"f55a8b4d3c5deb7b"},"source":["### Model Training\n","Right now we only have 2 regression models (one for DMS and one for 2A3)\n","They're both pretty good in terms of the competition's loss function MAE...\n","but they cant seem to tell the difference between a reactive base and a non-reactive base (which, is the whole point of the project...)\n","The next step on the ML side of things is to develop a classification model for Reactive and Non-Reactive.\n","Both labels (R & NR) will train on their own regression model, and for both probes DMS and 2A3.\n","This means we're looking at about 5 models total in our ML pipeline. (as of 9/29/23)"]},{"cell_type":"code","execution_count":null,"id":"ade4bf5b18c18510","metadata":{"id":"ade4bf5b18c18510"},"outputs":[],"source":["# Model building\n","model_2A3 = tf.keras.Sequential([\n","    tf.keras.layers.Conv1D(8, 1, activation='linear', batch_input_shape = (32, 1644, 1),padding='same'),\n","    tf.keras.layers.Conv1D(16, 1, activation='linear',padding='same'),\n","    tf.keras.layers.MaxPooling1D(padding='same'),\n","    #tf.keras.layers.Conv1D(32, 1, activation='linear',padding='same'),\n","    #tf.keras.layers.Conv1D(64, 1, activation='linear',padding='same'),\n","    #tf.keras.layers.MaxPooling1D(padding='same'),\n","    tf.keras.layers.Flatten(),\n","    #tf.keras.layers.Dense(512),\n","    tf.keras.layers.Dense(128),\n","    tf.keras.layers.Dense(32),\n","    tf.keras.layers.Dense(1)]\n",")\n","# Define the optimizer\n","optimizer_2A3 = tf.keras.optimizers.Adam(learning_rate=0.01)"]},{"cell_type":"code","execution_count":null,"id":"aabb9ab30836322e","metadata":{"id":"aabb9ab30836322e","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["NUM_EPOCHS = 2\n","EPOCH_PER_DISPLAY = 1\n","#running_accuracy = []\n","running_loss = []\n","for epoch in range(NUM_EPOCHS):\n","    for feat, target in train_dataset_2A3:\n","        with tf.GradientTape() as tape:\n","            # Calculate model output and loss\n","            output = model_2A3(feat, training=True)\n","            loss_ = loss_fn(target, output)\n","            #accuracy_ = accuracy_fn(target, output)\n","            #print(accuracy_)\n","\n","            # Tape gradients\n","            grads = tape.gradient(loss_, model_2A3.trainable_variables)\n","\n","        # Track batch loss and accuracy\n","        running_loss.append(loss_)\n","        #running_accuracy.append(accuracy_)\n","\n","        # Optimize model based on the gradients\n","        optimizer_2A3.apply_gradients(zip(grads, model_2A3.trainable_variables))\n","    # Epoch calculations\n","    epoch_loss = np.mean(running_loss)\n","    #epoch_accuracy = np.mean(running_accuracy)\n","    if (epoch + 1) % EPOCH_PER_DISPLAY == 0:\n","        print(\"Epoch {}: Loss: {:.4f}\".format(epoch+1, epoch_loss))"]},{"cell_type":"code","execution_count":null,"id":"30262cdba9eb11ab","metadata":{"id":"30262cdba9eb11ab","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["test_2A3 = pd.read_csv(fp_test_2A3)\n","X_test = test_2A3[features]\n","y_test = test_2A3['reactivities']"]},{"cell_type":"code","execution_count":null,"id":"72b1b80a65f5066b","metadata":{"id":"72b1b80a65f5066b","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["predictions = model_2A3.predict(X_test)\n","mean_absolute_error(predictions, y_test)\n","# 69 Sample Best : 0.4064400595049948 3 epochs\n","# 100 Sample Best : 0.40317076430271853  3 epochs\n","# 1000 Sample Best : 0.4044880844526148  2 epochs\n","# 3000 Sample Best : 0.3622463577769028\n","# 10000 Sample Best : 0.41434083974280383 3 epochs"]},{"cell_type":"code","execution_count":null,"id":"e3f2c2730c798b97","metadata":{"id":"e3f2c2730c798b97"},"outputs":[],"source":["# Model building\n","model_DMS = tf.keras.Sequential([\n","    tf.keras.layers.Conv1D(8, 1, activation='linear', batch_input_shape = (32, 1644, 1),padding='same'),\n","    tf.keras.layers.Conv1D(16, 1, activation='linear',padding='same'),\n","    tf.keras.layers.Conv1D(32, 1, activation='linear',padding='same'),\n","    tf.keras.layers.MaxPooling1D(padding='same'),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(96),\n","    tf.keras.layers.Dense(32),\n","    tf.keras.layers.Dense(8),\n","    tf.keras.layers.Dense(1)]\n",")\n","# Define the optimizer\n","optimizer_DMS = tf.keras.optimizers.Adam(learning_rate=0.01)"]},{"cell_type":"code","execution_count":null,"id":"9cc4a8af8c434e23","metadata":{"id":"9cc4a8af8c434e23"},"outputs":[],"source":["running_accuracy_DMS = []\n","running_loss_DMS = []\n","for feat, target in train_dataset_DMS:\n","    with tf.GradientTape() as tape:\n","        # Calculate model output and loss\n","        output = model_DMS(feat, training=True)\n","        loss_ = loss_fn(target, output)\n","        #accuracy_ = accuracy_fn(target, output)\n","        #print(accuracy_)\n","\n","        # Tape gradients\n","        grads = tape.gradient(loss_, model_DMS.trainable_variables)\n","\n","    # Track batch loss and accuracy\n","    running_loss_DMS.append(loss_)\n","    #running_accuracy_DMS.append(accuracy_)\n","\n","    # Optimize model based on the gradients\n","    optimizer_DMS.apply_gradients(zip(grads, model_DMS.trainable_variables))"]},{"cell_type":"code","execution_count":null,"id":"9ffebb416b0f47c7","metadata":{"id":"9ffebb416b0f47c7"},"outputs":[],"source":["test_DMS = pd.read_csv(fp_test_DMS)\n","X_test = test_DMS[features]\n","y_test = test_DMS['reactivities']"]},{"cell_type":"code","execution_count":null,"id":"1b90a7ee94b70d14","metadata":{"id":"1b90a7ee94b70d14"},"outputs":[],"source":["predictions = model_DMS.predict(X_test)\n","mean_absolute_error(predictions, y_test)\n","# 100 Sample Best : 0.3598897815134145\n","# 1000 Sample Best : 0.5850446463735238\n","# 3000 Sample Best : 0.3551389458161534"]},{"cell_type":"code","execution_count":null,"id":"49b94a92","metadata":{"id":"49b94a92"},"outputs":[],"source":["'''# Model building                                         Current best solo regression model for DMS\n","model_2A3 = tf.keras.Sequential([\n","    tf.keras.layers.Conv1D(8, 1, activation='linear', batch_input_shape = (32, 1644, 1),padding='same'),\n","    tf.keras.layers.Conv1D(16, 1, activation='linear',padding='same'),\n","    tf.keras.layers.Conv1D(32, 1, activation='linear',padding='same'),\n","    tf.keras.layers.MaxPooling1D(padding='same'),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(96),\n","    tf.keras.layers.Dense(32),\n","    tf.keras.layers.Dense(8),\n","    tf.keras.layers.Dense(1)]\n",")'''"]},{"cell_type":"code","execution_count":null,"id":"d46cd94914a81107","metadata":{"id":"d46cd94914a81107"},"outputs":[],"source":["'''# Model building                                    Current best solo regression model for 2A3\n","model_2A3 = tf.keras.Sequential([\n","    tf.keras.layers.Conv1D(8, 1, activation='linear', batch_input_shape = (32, 1644, 1),padding='same'),\n","    tf.keras.layers.Conv1D(16, 1, activation='linear',padding='same'),\n","    tf.keras.layers.MaxPooling1D(padding='same'),\n","    #tf.keras.layers.Conv1D(32, 1, activation='linear',padding='same'),\n","    #tf.keras.layers.Conv1D(64, 1, activation='linear',padding='same'),\n","    #tf.keras.layers.MaxPooling1D(padding='same'),\n","    tf.keras.layers.Flatten(),\n","    #tf.keras.layers.Dense(512),\n","    tf.keras.layers.Dense(128),\n","    tf.keras.layers.Dense(32),\n","    tf.keras.layers.Dense(1)]\n",")'''"]},{"cell_type":"markdown","id":"9df15d373974c38c","metadata":{"id":"9df15d373974c38c","pycharm":{"name":"#%% md\n"}},"source":[]},{"cell_type":"code","execution_count":null,"id":"18456f06f2a24a9f","metadata":{"ExecuteTime":{"end_time":"2023-09-30T02:35:20.018391200Z","start_time":"2023-09-30T02:35:20.005031900Z"},"id":"18456f06f2a24a9f","outputId":"4e8c7ae5-c777-445f-b358-0246a20d6c88","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["what\n"]}],"source":["print(\"what\") # what"]},{"cell_type":"markdown","id":"2e8db9046b3532a5","metadata":{"collapsed":false,"id":"2e8db9046b3532a5"},"source":["### Building a Submission\n","This part should have been easy, but the 'small sample' of sequences we need to test in './test_sequences.csv' is huge.\n","This entire process needs to be optimized. (and major code clean up on my behalf)"]},{"cell_type":"code","execution_count":null,"id":"d82ce11408c9d993","metadata":{"id":"d82ce11408c9d993","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["base_dict = {   'A': [1,0,0,0],\n","                'C': [0,1,0,0],\n","                'G': [0,0,1,0],\n","                'U': [0,0,0,1] }\n","def test_base(sequence, pos):\n","    test_b = []\n","    for n in range(-205,206):\n","        if 0 <= (n + pos) < len(sequence):\n","            test_b+=base_dict[sequence[pos+n]]\n","        else:\n","            test_b+=[0,0,0,1]\n","    return test_b\n","\n","# entry builder (for the feature dictionary method; inefficient, and will likely be replaced after converting preprocess pipeline)\n","def append_base(b, pos):\n","    if pos < 0:\n","        sign = 'n'\n","    else:\n","        sign = ''\n","    end = sign + str(abs(pos))\n","    if b == 'A':\n","        feat_dict['b_A_' + end].append(1)\n","        feat_dict['b_C_' + end].append(0)\n","        feat_dict['b_G_' + end].append(0)\n","        feat_dict['b_U_' + end].append(0)\n","    elif b == 'C':\n","        feat_dict['b_A_' + end].append(0)\n","        feat_dict['b_C_' + end].append(1)\n","        feat_dict['b_G_' + end].append(0)\n","        feat_dict['b_U_' + end].append(0)\n","    elif b == 'G':\n","        feat_dict['b_A_' + end].append(0)\n","        feat_dict['b_C_' + end].append(0)\n","        feat_dict['b_G_' + end].append(1)\n","        feat_dict['b_U_' + end].append(0)\n","    elif b == 'U':\n","        feat_dict['b_A_' + end].append(0)\n","        feat_dict['b_C_' + end].append(0)\n","        feat_dict['b_G_' + end].append(0)\n","        feat_dict['b_U_' + end].append(1)\n","    else:\n","        feat_dict['b_A_' + end].append(0)\n","        feat_dict['b_C_' + end].append(0)\n","        feat_dict['b_G_' + end].append(0)\n","        feat_dict['b_U_' + end].append(0)"]},{"cell_type":"code","execution_count":null,"id":"1968b305e5bb445d","metadata":{"id":"1968b305e5bb445d","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# submission builder\n","test_set = pd.read_csv('input/test_sequences.csv')\n","\n","line = \"id,reactivity_DMS_MaP,reactivity_2A3_MaP\"\n","sub_file = open(\"submission.csv\", \"w\")\n","sub_file.write(line)\n","sub_file.close()\n","\n","b_id = -1\n","feat_dict = dict()\n","for index, row in test_set.iterrows():\n","    seq = row['sequence']\n","    sub_file = open(\"submission.csv\", \"a\")\n","    for i in range(len(seq)):\n","        b_id+=1\n","        for x in features:\n","            feat_dict[x] = []\n","        for n in range(-205,206):\n","            if 0 <= (n + i) < len(seq):\n","                append_base(seq[i+n], n)\n","            else:\n","                append_base(0, n)\n","        single_entry_df = pd.DataFrame(feat_dict)\n","        react_DMS = model_DMS.predict(single_entry_df)\n","        react_2A3 = model_2A3.predict(single_entry_df)\n","        line = \"\\n\" + str(b_id) + \",\" + str(react_DMS[0][0]) + \",\" + str(react_2A3[0][0])\n","        sub_file.write(line)\n","    sub_file.close()\n","    print(f\"Bases finished: {b_id}\")"]},{"cell_type":"code","execution_count":null,"id":"beb4363796ac899a","metadata":{"id":"beb4363796ac899a","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["#todo; design a much faster submission builder than this one...\n","# resume submission builder\n","test_set = pd.read_csv('./input/test_sequences.csv')\n","\n","\n","start_id = '' #determine starting id\n","for line in list(open(\"submission.csv\"))[-1]:\n","    start_id += line\n","start_id = int(start_id.split(\",\")[0])+1\n","\n","b_id = -1\n","test_bases = []\n","for index, row in test_set.iterrows():\n","    seq = row['sequence']\n","    for i in range(len(seq)):\n","        b_id+=1\n","        test_bases+= [tuple(test_base(seq, i))]\n","    if ((b_id+1)-start_id) % 100000 == 0:\n","        rDMS, r2A3 = model_DMS.predict(np.array(test_bases)), model_2A3.predict(np.array(test_bases))\n","        sub_file = open(\"submission.csv\", \"a\")\n","        for i in range(len(rDMS)):\n","            sub_file.write(\"\\n\" + str(start_id) + \",\" + str(rDMS[i][0]) + \",\" + str(r2A3[i][0]))\n","            start_id+=1\n","        sub_file.close()\n","        test_bases = []\n","        print(f\"Bases finished: {b_id}\")\n","\n","if test_bases:\n","        rDMS, r2A3 = model_DMS.predict(np.array(test_bases)), model_2A3.predict(np.array(test_bases))\n","        sub_file = open(\"submission.csv\", \"a\")\n","        for i in range(len(rDMS)):\n","            sub_file.write(\"\\n\" + str(start_id) + \",\" + str(rDMS[i][0]) + \",\" + str(r2A3[i][0]))\n","            start_id+=1\n","        sub_file.close()\n","        test_bases = []"]},{"cell_type":"code","execution_count":null,"id":"61d01041a2cd5307","metadata":{"id":"61d01041a2cd5307","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["#submission builder speed test\n","test_set = pd.read_csv('./input/test_sequences.csv')\n","\n","line = \"id,reactivity_DMS_MaP,reactivity_2A3_MaP\"\n","sub_file = open(\"sub_speed_test.csv\", \"w\")\n","sub_file.write(line)\n","sub_file.close()\n","\n","start = time.time()\n","start_id = 0\n","b_id = -1\n","with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    test_bases = []\n","    for index, row in test_set.iterrows():\n","        seq = row['sequence']\n","        for i in range(len(seq)):\n","            b_id+=1\n","            print(f\"new bid: {b_id}\")\n","            test_bases+= [tuple(test_base(seq, i))]\n","            if b_id >= 100000:\n","                break\n","        if ((b_id+1)-start_id) % 100000 == 0 or b_id >= 100000:\n","            rDMS, r2A3 = model_DMS.predict(np.array(test_bases)), model_2A3.predict(np.array(test_bases))\n","            sub_file = open(\"sub_speed_test.csv\", \"a\")\n","            for i in range(len(rDMS)):\n","                sub_file.write(\"\\n\" + str(start_id) + \",\" + str(rDMS[i][0]) + \",\" + str(r2A3[i][0]))\n","                start_id+=1\n","            sub_file.close()\n","            test_bases = []\n","            if b_id >= 100000:\n","                break\n","\n","end = time.time()\n","elapsed = end - start\n","\n","line = \"\\nTime elapsed for 100000 bases: \" + str(elapsed)  + \" seconds.\"\n","sub_file = open(\"sub_speed_test.csv\", \"a\")\n","sub_file.write(line)\n","sub_file.close()\n","print(line)"]},{"cell_type":"code","execution_count":null,"id":"d1530b2f43fc51ec","metadata":{"id":"d1530b2f43fc51ec","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# progress check\n","final_id = 269796670 # the final b_id in test_sequences.csv\n","last_id = '' # determine last id written by model\n","for line in list(open(\"submission.csv\"))[-1]:\n","    last_id += line\n","last_id = int(last_id.split(\",\")[0])\n","\n","print(f\"Submission Completion:\\n{last_id}/{final_id}\\t{100*last_id/final_id:.2f}%\")"]},{"cell_type":"code","execution_count":null,"id":"29a637e8a70814e","metadata":{"id":"29a637e8a70814e","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["test = pd.read_csv('submission.csv')\n","test.head()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":5}
