{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ribonanza - Attempt 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second approach to the [Stanford Ribonanza problem](https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/) that builds off the first and second approaches.\n",
    "\n",
    "Major differences:\n",
    "- use of pytorch instead of tensorflow\n",
    "- use of attention model architecture\n",
    "- use bpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the attention architecture scores 0.19377"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- improve model\n",
    "- experiment with other types of attention\n",
    "- use full transformer model (with decoder) (not just encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filesystem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your project directory should look like this:\n",
    "\n",
    "- `(project directory)`\n",
    "    - `ribonanza2.ipynb`\n",
    "    - `train_data.csv`\n",
    "    - `test_data.csv` (optional)\n",
    "\n",
    "`train_data.csv` is the only file necessary for training, and it can be downloaded from the kaggle competition linked in the description.\n",
    "\n",
    "`test_data.csv` is only necessary if you intend to make and submit predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Seetup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to install pip packages:\n",
    "```sh\n",
    "pip install torch numpy seaborn xformers arnie datasets\n",
    "```\n",
    "Need to install conda packages for eternafold:\n",
    "```sh\n",
    "conda install -c conda-forge \"libgcc-ng>=12\" \"libstdcxx-ng>=12\"\n",
    "conda install -c bioconda eternafold\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "# for visualization\n",
    "import seaborn\n",
    "\n",
    "# typing hints\n",
    "from typing import List\n",
    "from collections.abc import Callable\n",
    "\n",
    "# used for better attention mechanisms\n",
    "import xformers.components.positional_embedding as embeddings\n",
    "import xformers.ops as xops\n",
    "import xformers.components.attention as attentions\n",
    "import xformers.components.attention.utils as att_utils\n",
    "import xformers.components as components\n",
    "\n",
    "# used for bpps\n",
    "from arnie.bpps import bpps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "# according to kaggle, this is the maximum # of reactivites to be used\n",
    "NUM_REACTIVITIES = 457\n",
    "\n",
    "# there are 4 different bases (AUCG)\n",
    "NUM_BASES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"  # if no gpu available, use cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(out: str, key: str, value: str, file_name: str, force: bool):\n",
    "    \"\"\"\n",
    "    Filters a file to only take datapoints\n",
    "    whose values of `key` are `value`.\n",
    "\n",
    "    Parameters:\n",
    "        - out: str - the name of the file that will store the filtered datapoints\n",
    "        - key: str - the name of the key to look at\n",
    "        - value: str - the value that the key should have\n",
    "        - file_name: str - the name of the file that contains all the datapoints.\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    if os.path.exists(out) and not force:\n",
    "        print(\"File already exists, not doing any work\")\n",
    "        return\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    # count how many lines we have in total\n",
    "    with open(file_name) as file:\n",
    "        line = file.readline()  # ignore the header\n",
    "        line = (\n",
    "            file.readline()\n",
    "        )  # take the first line since we increment count in the loop\n",
    "        while line != \"\":\n",
    "            count += 1\n",
    "            line = file.readline()\n",
    "\n",
    "    # use that knowledge for a progress bar\n",
    "    with open(file_name, \"r\") as file, open(out, \"w\") as outfile:\n",
    "        # write the header\n",
    "        header = file.readline()\n",
    "        outfile.write(header)\n",
    "\n",
    "        # get what index the SN_filter is\n",
    "        SN_idx = header.split(\",\").index(key)\n",
    "\n",
    "        # only take the approved filtered lines\n",
    "        for _ in tqdm(range(count)):\n",
    "            line = file.readline()\n",
    "            temp = line.split(\",\")\n",
    "            if temp[SN_idx] == value:\n",
    "                outfile.write(line)\n",
    "\n",
    "\n",
    "def filter_train_data(force: bool = False):\n",
    "    \"\"\"\n",
    "    Filters the immense train_data.csv to only take datapoints\n",
    "    whose SN_filter (Signal to Noise filter) is 1. In other words,\n",
    "    we only take good reads. These filtered datapoints are then\n",
    "    written to the file provided\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\"train_data_filtered.csv\", \"SN_filter\", \"1\", \"train_data.csv\", force)\n",
    "\n",
    "\n",
    "def filter_2A3(force: bool = False):\n",
    "    \"\"\"\n",
    "    Only take the 2A3 points\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\n",
    "        \"train_data_2a3.csv\",\n",
    "        \"experiment_type\",\n",
    "        \"2A3_MaP\",\n",
    "        \"train_data_filtered.csv\",\n",
    "        force,\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_DMS(force: bool = False):\n",
    "    \"\"\"\n",
    "    Only take the DMS points\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\n",
    "        \"train_data_dms.csv\",\n",
    "        \"experiment_type\",\n",
    "        \"DMS_MaP\",\n",
    "        \"train_data_filtered.csv\",\n",
    "        force,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# filter our data\n",
    "filter_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# take the 2a3 points\n",
    "filter_2A3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# take the dms points\n",
    "filter_DMS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode inputs as\n",
    "# A : 1\n",
    "# U : 2\n",
    "# C : 3\n",
    "# G : 4\n",
    "base_map = {\n",
    "    \"A\": 1,\n",
    "    \"U\": 2,\n",
    "    \"C\": 3,\n",
    "    \"G\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(row):\n",
    "    \"\"\"\n",
    "    Convert a row containing all csv columns in the original dataset\n",
    "    to a row containing only the columns:\n",
    "    - inputs\n",
    "    - outputs\n",
    "    - bpp\n",
    "    - output_masks\n",
    "    - reactivity_error\n",
    "    - bool_output_masks\n",
    "    \"\"\"\n",
    "    # initialize arrays\n",
    "    # note that we assume everything is masked until told otherwise\n",
    "    inputs = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "    bpp = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "    output_masks = np.ones((NUM_REACTIVITIES,), dtype=np.bool_)\n",
    "    reactivity_errors = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "    reactivities = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "\n",
    "    seq_len = len(row[\"sequence\"])\n",
    "\n",
    "    # encode the bases\n",
    "    inputs[:seq_len] = np.array(\n",
    "        list(map(lambda letter: base_map[letter], row[\"sequence\"]))\n",
    "    )\n",
    "\n",
    "    # get the probability that any of those bases are paired\n",
    "    bpp[:seq_len] = np.max(bpps(row[\"sequence\"], package=\"eternafold\"), axis=-1)\n",
    "\n",
    "    # get the reactivities and their errors\n",
    "    reactivities[:seq_len] = np.array(\n",
    "        list(\n",
    "            map(\n",
    "                lambda seq_idx: np.float32(\n",
    "                    row[\"reactivity_\" + str(seq_idx + 1).rjust(4, \"0\")]\n",
    "                ),\n",
    "                range(seq_len),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    reactivity_errors[:seq_len] = np.array(\n",
    "        list(\n",
    "            map(\n",
    "                lambda seq_idx: np.float32(\n",
    "                    row[\"reactivity_error_\" + str(seq_idx + 1).rjust(4, \"0\")]\n",
    "                ),\n",
    "                range(seq_len),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # replace reactivity error nans with 0s (assume no error)\n",
    "    reactivity_errors = np.where(np.isnan(reactivity_errors), 0.0, reactivity_errors)\n",
    "\n",
    "    # get where all the reactivities are nan\n",
    "    nan_locats = np.isnan(reactivities)\n",
    "\n",
    "    # where it is nan, store True, else False\n",
    "    output_masks[:seq_len] = nan_locats[:seq_len]\n",
    "\n",
    "    # where it is not nan, store the reactivity and error, else 0\n",
    "    reactivities[:seq_len] = np.where(\n",
    "        nan_locats[:seq_len] == False, reactivities[:seq_len], 0.0\n",
    "    )\n",
    "    reactivity_errors[:seq_len] = np.where(\n",
    "        nan_locats[:seq_len] == False, reactivity_errors[:seq_len], 0.0\n",
    "    )\n",
    "\n",
    "    # store the values\n",
    "    row = {}\n",
    "    row[\"inputs\"] = inputs\n",
    "    row[\"bpp\"] = bpp\n",
    "    row[\"outputs\"] = np.clip(reactivities, 0, 1)\n",
    "    row[\"output_masks\"] = np.clip(\n",
    "        np.where(output_masks, 0.0, 1.0) - np.abs(reactivity_errors), 0, 1\n",
    "    )\n",
    "    row[\"bool_output_masks\"] = output_masks\n",
    "    row[\"reactivity_errors\"] = np.abs(reactivity_errors)\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def process_data_test(row):\n",
    "    \"\"\"\n",
    "    Almost the same as process_data, except it only takes inputs and bpp\n",
    "    \"\"\"\n",
    "    # initialize arrays\n",
    "    # note that we assume everything is masked until told otherwise\n",
    "    inputs = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "    bpp = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "\n",
    "    seq_len = len(row[\"sequence\"])\n",
    "\n",
    "    # encode the bases\n",
    "    inputs[:seq_len] = np.array(\n",
    "        list(map(lambda letter: base_map[letter], row[\"sequence\"]))\n",
    "    )\n",
    "\n",
    "    # get the probability that any of those bases are paired\n",
    "    bpp[:seq_len] = np.max(bpps(row[\"sequence\"], package=\"eternafold\"), axis=-1)\n",
    "\n",
    "    row[\"inputs\"] = inputs\n",
    "    row[\"bpp\"] = bpp\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_csv(\n",
    "    out: str,\n",
    "    file_name: str,\n",
    "    n_proc: int = 12,\n",
    "    map_fn: Callable = process_data,\n",
    "    extra_cols_to_keep: List[str] = [],\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess the csv and save the preprocessed data as a dataset\n",
    "    that can be loaded via datasets.Dataset.load_from_file\n",
    "\n",
    "    The dataset contains the following items:\n",
    "        - bool_output_masks: Tensor(dtype=torch.bool) - the output masks.\n",
    "            If True, then that item should NOT be used to calculate loss.\n",
    "            If False, then that item should be used to calculate loss\n",
    "        - reactivity_errors: Tensor(dtype=torch.float32) - the reactivity errors\n",
    "        - output_masks: Tensor(dtype=torch.float32) - the elementwise weights to multiply the loss by to properly\n",
    "            account for masked items and reactivity errors\n",
    "        - inputs: tensor(dtype=torch.float32) - the input sequence, specifically of shape (None, NUM_REACTIVITIES)\n",
    "        - bpp: tensor(dtype=torch.float32)\n",
    "        - outputs: tensor(dtype=torch.float32) - the expected reactivities. Note that a simple MAE or MSE loss will not\n",
    "            suffice for training models on this dataset. Please use the output_masks tensor as well.\n",
    "\n",
    "    Parameters:\n",
    "        - out: str - the name of the file to save the arrays to\n",
    "        - file_name: str - the name of the input csv file\n",
    "        - n_proc: int - the number of processes to use while processing data\n",
    "        - map_fn: Callable - the function to apply to all dataset rows\n",
    "        - extra_cols_to_keep: List[str] - the names of any extra columns to keep in the dataset\n",
    "    \"\"\"\n",
    "    if os.path.exists(out):\n",
    "        print(\n",
    "            \"File already exists, not doing any work.\\n\"\n",
    "            + \"To force re-preprocessing, delete the dataset directory and restart the kernel.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    names_to_keep = [\n",
    "        \"reactivity_errors\",\n",
    "        \"bool_output_masks\",\n",
    "        \"output_masks\",\n",
    "        \"inputs\",\n",
    "        \"outputs\",\n",
    "        \"bpp\",\n",
    "    ] + extra_cols_to_keep\n",
    "\n",
    "    # load dataset and map it to our preprocess function\n",
    "    ds = Dataset.from_csv(file_name).map(map_fn, num_proc=n_proc)\n",
    "\n",
    "    # drop excess columns and save to disk\n",
    "    ds.remove_columns(\n",
    "        list(filter(lambda c: c not in names_to_keep, ds.column_names))\n",
    "    ).save_to_disk(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work.\n",
      "To force re-preprocessing, delete the dataset directory and restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\"train_data_2a3_preprocessed\", \"train_data_2a3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work.\n",
      "To force re-preprocessing, delete the dataset directory and restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\"train_data_dms_preprocessed\", \"train_data_dms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work.\n",
      "To force re-preprocessing, delete the dataset directory and restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\n",
    "    \"test_data_preprocessed\",\n",
    "    \"test_sequences.csv\",\n",
    "    map_fn=process_data_test,\n",
    "    extra_cols_to_keep=[\"id_min\", \"id_max\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the desired dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:33.551791Z",
     "start_time": "2023-10-05T17:09:33.535197Z"
    }
   },
   "outputs": [],
   "source": [
    "desired_dataset = \"2a3\"  # either \"2a3\" or \"dms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['inputs', 'bpp', 'outputs', 'output_masks', 'bool_output_masks', 'reactivity_errors'],\n",
       "    num_rows: 210992\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(\n",
    "    f\"train_data_{desired_dataset}_preprocessed\"\n",
    ").with_format(\"torch\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set is len 189892 and val dataset is len 21100\n"
     ]
    }
   ],
   "source": [
    "columns = [\"inputs\", \"outputs\", \"output_masks\", \"bpp\"]\n",
    "split = dataset.train_test_split(test_size=0.1).select_columns(columns)\n",
    "train_dataset = split[\"train\"]\n",
    "val_dataset = split[\"test\"]\n",
    "\n",
    "print(\n",
    "    \"train set is len\", len(train_dataset), \"and val dataset is len\", len(val_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataset preprocessed, we can visualize what the distribution looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.844615Z",
     "start_time": "2023-10-05T17:09:42.843236Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(*args):\n",
    "    \"\"\"\n",
    "    Gets the product of all arguments passed to it\n",
    "    \"\"\"\n",
    "    prod = 1\n",
    "    for item in args:\n",
    "        prod *= item\n",
    "    return prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0.21671% of the data\n"
     ]
    }
   ],
   "source": [
    "if visualize:\n",
    "    # select all the reactivities that are valid (that shouldn't be masked)\n",
    "    visualized_items = torch.masked_select(\n",
    "        dataset[\"outputs\"], dataset[\"bool_output_masks\"] == False\n",
    "    ).numpy()\n",
    "\n",
    "    # sanity check that we didn't take all the items\n",
    "    print(\n",
    "        f\"took {visualized_items.shape[0] / multiply(*dataset['outputs'].shape):.5f}% of the data\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.851272Z",
     "start_time": "2023-10-05T17:09:42.849526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if visualize:\n",
    "    seaborn.histplot(visualized_items, binwidth=0.1)\n",
    "else:\n",
    "    print(\"Not visualizing. Set `visualize` to `True` to visualize data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model our distribution, we have two models:\n",
    "- an AttentionModel that uses attention layers\n",
    "- a BaselineModel that we compare against that uses only a couple convolution layers and a Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEfficientSelfAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, latent_dim: int, n_heads: int, dropout: float, *args, **kwargs\n",
    "    ) -> None:\n",
    "        super(MemoryEfficientSelfAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        assert latent_dim % n_heads == 0\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        # B, Seq Len, embedding -> B, Seq Len, Heads, Embedding per head\n",
    "        x = x.reshape(x.shape[0], x.shape[1], self.n_heads, x.shape[2] // self.n_heads)\n",
    "        x = xops.memory_efficient_attention(x, x, x, p=self.dropout)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SDPAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int,\n",
    "        n_heads: int,\n",
    "        dropout: float,\n",
    "        device: str,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(SDPAttention, self).__init__()\n",
    "        self.mha = components.MultiHeadDispatch(\n",
    "            latent_dim,\n",
    "            num_heads=n_heads,\n",
    "            attention=attentions.ScaledDotProduct(dropout=dropout),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        return self.mha(x, att_mask=attention_mask)\n",
    "\n",
    "class SlidingWindowAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int,\n",
    "        n_heads: int,\n",
    "        dropout: float,\n",
    "        device: str,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(SlidingWindowAttention, self).__init__()\n",
    "        self.mha = components.MultiHeadDispatch(\n",
    "            latent_dim,\n",
    "            n_heads,\n",
    "            attentions.LocalAttention(\n",
    "                dropout=dropout, window_size=kwargs[\"context_window\"]\n",
    "            ),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        return self.mha(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerEncoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_type: torch.nn.Module,\n",
    "        latent_dim: int,\n",
    "        ff_dim: int,\n",
    "        n_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        device: str = DEVICE,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(CustomTransformerEncoderLayer, self).__init__()\n",
    "        self.attention = attention_type(\n",
    "            latent_dim=latent_dim,\n",
    "            n_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            device=device,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.layer_norm = torch.nn.LayerNorm(latent_dim).to(device)\n",
    "\n",
    "        self.ff1 = torch.nn.Linear(latent_dim, ff_dim).to(device)\n",
    "        self.ff2 = torch.nn.Linear(ff_dim, latent_dim).to(device)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        # MHA, add, norm\n",
    "        x = self.layer_norm(self.attention(x, attention_mask=attention_mask) + x)\n",
    "\n",
    "        # ff, add, norm\n",
    "        x = self.layer_norm(self.gelu(self.ff2(self.gelu(self.ff1(x)))) + x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomTransformerEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_type: torch.nn.Module,\n",
    "        n_layers: int,\n",
    "        latent_dim: int,\n",
    "        ff_dim: int,\n",
    "        n_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        device: str = DEVICE,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(CustomTransformerEncoder, self).__init__()\n",
    "        for i in range(n_layers):\n",
    "            self.add_module(\n",
    "                str(i),\n",
    "                CustomTransformerEncoderLayer(\n",
    "                    attention_type=attention_type,\n",
    "                    latent_dim=latent_dim,\n",
    "                    ff_dim=ff_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    dropout=dropout,\n",
    "                    device=device,\n",
    "                    **kwargs\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        for module in self._modules.values():\n",
    "            x = module(x, attention_mask=attention_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_type: torch.nn.Module,\n",
    "        context_window: int = 31,\n",
    "        latent_dim: int = 128,\n",
    "        ff_dim: int = 1024,\n",
    "        n_heads: int = 2,\n",
    "        enc_layers: int = 1,\n",
    "        device: str = DEVICE,\n",
    "    ) -> None:\n",
    "        super(AttentionModel, self).__init__()\n",
    "\n",
    "        # data\n",
    "        self.n_heads = n_heads\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # projection layer\n",
    "        self.proj = torch.nn.Linear(2, latent_dim).to(device)\n",
    "\n",
    "        # positional embedding and encoder layers\n",
    "        self.pos_embedding = embeddings.SinePositionalEmbedding(latent_dim).to(device)\n",
    "        self.encoder_layers = CustomTransformerEncoder(\n",
    "            latent_dim=latent_dim,\n",
    "            ff_dim=ff_dim,\n",
    "            n_heads=n_heads,\n",
    "            device=device,\n",
    "            attention_type=attention_type,\n",
    "            n_layers=enc_layers,\n",
    "            context_window=context_window,\n",
    "        )\n",
    "\n",
    "        # output head\n",
    "        self.head = torch.nn.Linear(latent_dim, 1).to(device)\n",
    "        self.final_result = torch.nn.Linear(NUM_REACTIVITIES, NUM_REACTIVITIES).to(\n",
    "            device\n",
    "        )\n",
    "\n",
    "        # activations\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.gelu = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mask = att_utils.maybe_merge_masks(\n",
    "            att_mask=None,\n",
    "            key_padding_mask=(x != 0).any(dim=-1),\n",
    "            batch_size=x.shape[0],\n",
    "            num_heads=self.n_heads,\n",
    "            src_len=x.shape[1],\n",
    "        )\n",
    "\n",
    "        # project to latent dimension\n",
    "        x = self.proj(x)\n",
    "\n",
    "        # embed and then perform attention\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.encoder_layers(x, attention_mask=mask)\n",
    "\n",
    "        # final result\n",
    "        x = self.relu(self.final_result(self.gelu(self.head(x).flatten(start_dim=1))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(torch.nn.Module):\n",
    "    def __init__(self, context_window: int = 31, device: str = DEVICE):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.preLayer = torch.nn.Linear(2, 2).to(device)\n",
    "        self.conv_layer = torch.nn.Conv1d(2, 2, context_window, padding=\"same\").to(\n",
    "            device\n",
    "        )\n",
    "        self.conv_layer_b = torch.nn.Conv1d(2, 2, context_window, padding=\"same\").to(\n",
    "            device\n",
    "        )\n",
    "        self.ff = torch.nn.Linear(NUM_REACTIVITIES * 2, NUM_REACTIVITIES).to(device)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.gelu(self.preLayer(x))\n",
    "        x = x.reshape(x.shape[0], -1, x.shape[1])\n",
    "\n",
    "        x = self.gelu(\n",
    "            self.conv_layer(x)\n",
    "            + torch.flip(self.conv_layer_b(torch.flip(x, dims=[2])), dims=[2])\n",
    "        )\n",
    "\n",
    "        return self.relu(self.ff(x.flatten(start_dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dms_kwargs = dict(\n",
    "    latent_dim=32, n_heads=2, enc_layers=4, ff_dim=1024, attention_type=SDPAttention\n",
    ")\n",
    "model_2a3_kwargs = dict(\n",
    "    latent_dim=32, n_heads=2, enc_layers=4, ff_dim=1024, attention_type=SDPAttention\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_baseline = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the model\n",
    "if use_baseline:\n",
    "    model = BaselineModel()\n",
    "elif desired_dataset == \"dms\":\n",
    "    model = AttentionModel(**model_dms_kwargs)\n",
    "elif desired_dataset == \"2a3\":\n",
    "    model = AttentionModel(**model_2a3_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load old weights if possible\n",
    "if os.path.exists(f\"{desired_dataset}_model\"):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(f\"{desired_dataset}_model\"))\n",
    "        print(\"loaded previous weights\")\n",
    "    except Exception as e:\n",
    "        print(\"not loading previous weights because\", e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8862e-02, 0.0000e+00, 1.3238e-02, 9.9083e-02, 0.0000e+00, 0.0000e+00,\n",
       "         1.2995e-02, 5.3335e-02, 0.0000e+00, 7.1854e-03, 5.0736e-02, 2.1865e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.3526e-01, 1.4605e-01, 3.4843e-02, 1.9201e-02, 0.0000e+00, 9.1232e-02,\n",
       "         1.3284e-02, 0.0000e+00, 3.3720e-02, 0.0000e+00, 4.7804e-02, 0.0000e+00,\n",
       "         6.9034e-02, 6.7937e-02, 0.0000e+00, 2.1181e-02, 1.0331e-01, 0.0000e+00,\n",
       "         5.6300e-02, 0.0000e+00, 0.0000e+00, 1.1115e-02, 9.6045e-02, 1.1666e-01,\n",
       "         3.0520e-02, 1.6724e-01, 0.0000e+00, 1.1699e-01, 0.0000e+00, 9.1432e-02,\n",
       "         5.5115e-02, 1.8664e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9941e-01,\n",
       "         2.2780e-02, 0.0000e+00, 8.0668e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         4.8273e-02, 6.2888e-02, 1.7202e-01, 0.0000e+00, 9.2324e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0141e-01, 0.0000e+00, 0.0000e+00,\n",
       "         8.8022e-02, 3.6785e-02, 0.0000e+00, 1.3113e-01, 4.1360e-02, 0.0000e+00,\n",
       "         0.0000e+00, 1.0480e-01, 1.5747e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 5.3287e-02, 1.6785e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1782e-01, 0.0000e+00,\n",
       "         0.0000e+00, 6.8592e-02, 7.1902e-02, 0.0000e+00, 6.0818e-02, 0.0000e+00,\n",
       "         7.2766e-03, 3.7887e-02, 4.1947e-02, 2.2887e-02, 0.0000e+00, 1.7311e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.3401e-01, 0.0000e+00, 5.2842e-02, 2.1209e-02, 0.0000e+00,\n",
       "         1.1774e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 9.0457e-02, 0.0000e+00, 6.2615e-02, 3.6631e-02,\n",
       "         0.0000e+00, 2.8775e-02, 5.9954e-02, 0.0000e+00, 1.1149e-01, 7.3256e-02,\n",
       "         2.3832e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0282e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.4841e-02,\n",
       "         1.0692e-01, 8.2788e-02, 4.9519e-02, 0.0000e+00, 1.9102e-01, 6.2733e-02,\n",
       "         0.0000e+00, 0.0000e+00, 1.9545e-01, 5.9949e-02, 1.1391e-01, 4.2842e-03,\n",
       "         8.1618e-03, 0.0000e+00, 0.0000e+00, 8.3888e-02, 0.0000e+00, 1.4541e-01,\n",
       "         0.0000e+00, 1.0686e-01, 0.0000e+00, 0.0000e+00, 1.0903e-01, 0.0000e+00,\n",
       "         7.0056e-02, 0.0000e+00, 2.6639e-02, 9.3645e-02, 4.0870e-02, 0.0000e+00,\n",
       "         8.1830e-03, 1.1850e-02, 9.3572e-03, 0.0000e+00, 1.9703e-01, 0.0000e+00,\n",
       "         1.4398e-01, 3.3881e-03, 1.0544e-01, 1.2527e-01, 0.0000e+00, 2.9097e-02,\n",
       "         0.0000e+00, 1.2320e-01, 0.0000e+00, 9.5494e-04, 9.4691e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3273e-03, 5.2048e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.0329e-01, 0.0000e+00, 6.9979e-03, 2.3095e-01,\n",
       "         1.4422e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.7881e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.4755e-03, 9.2079e-02, 4.4218e-02, 0.0000e+00, 4.1782e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1209e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.8480e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         8.9927e-02, 3.6268e-02, 7.7539e-02, 1.6258e-02, 6.4530e-02, 1.3708e-01,\n",
       "         0.0000e+00, 3.2455e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5482e-02,\n",
       "         0.0000e+00, 3.1986e-02, 9.8686e-02, 5.6117e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9728e-02, 2.6095e-02, 0.0000e+00,\n",
       "         8.8816e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.2699e-02, 0.0000e+00,\n",
       "         8.6476e-02, 0.0000e+00, 5.3424e-05, 0.0000e+00, 8.6712e-02, 1.0674e-01,\n",
       "         0.0000e+00, 7.0395e-02, 1.2006e-01, 2.2830e-02, 0.0000e+00, 0.0000e+00,\n",
       "         8.4147e-02, 5.6207e-02, 0.0000e+00, 0.0000e+00, 3.1446e-02, 0.0000e+00,\n",
       "         0.0000e+00, 6.4784e-02, 2.3801e-02, 0.0000e+00, 2.1670e-01, 0.0000e+00,\n",
       "         2.4714e-01, 3.4184e-02, 5.0852e-02, 0.0000e+00, 2.8915e-02, 5.6430e-02,\n",
       "         0.0000e+00, 4.6966e-02, 0.0000e+00, 0.0000e+00, 6.0649e-02, 0.0000e+00,\n",
       "         6.9758e-02, 0.0000e+00, 0.0000e+00, 1.8150e-02, 0.0000e+00, 3.5068e-02,\n",
       "         1.4363e-01, 3.8587e-02, 0.0000e+00, 0.0000e+00, 1.5721e-01, 5.3305e-02,\n",
       "         0.0000e+00, 1.8665e-02, 1.2310e-01, 0.0000e+00, 3.1363e-02, 0.0000e+00,\n",
       "         0.0000e+00, 3.1813e-02, 1.1863e-01, 0.0000e+00, 0.0000e+00, 1.3658e-02,\n",
       "         0.0000e+00, 0.0000e+00, 2.0082e-01, 9.7920e-02, 5.4654e-02, 0.0000e+00,\n",
       "         0.0000e+00, 1.1096e-01, 3.6693e-02, 6.2973e-02, 0.0000e+00, 0.0000e+00,\n",
       "         8.6651e-02, 5.6849e-02, 2.6805e-02, 9.0705e-02, 0.0000e+00, 3.2000e-02,\n",
       "         3.7938e-02, 2.8373e-02, 6.4375e-02, 4.5106e-02, 8.7380e-02, 0.0000e+00,\n",
       "         7.3960e-02, 1.5636e-01, 0.0000e+00, 1.0195e-01, 3.7365e-03, 6.0971e-02,\n",
       "         4.9574e-03, 2.2977e-01, 3.9472e-02, 2.3759e-02, 0.0000e+00, 1.3415e-01,\n",
       "         5.2513e-02, 8.9677e-02, 4.0673e-02, 2.8041e-02, 3.3323e-02, 4.5000e-02,\n",
       "         6.5508e-02, 1.9262e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.6773e-01, 0.0000e+00, 0.0000e+00, 1.9566e-02, 0.0000e+00,\n",
       "         2.0794e-02, 9.5484e-03, 1.0339e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         9.1387e-02, 3.7764e-02, 8.2921e-02, 3.6522e-02, 0.0000e+00, 5.4806e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 9.3819e-02, 4.0195e-02, 1.4967e-01,\n",
       "         0.0000e+00, 0.0000e+00, 2.5796e-02, 8.2877e-02, 1.1205e-01, 4.8248e-04,\n",
       "         1.0813e-01, 0.0000e+00, 6.5929e-02, 1.0583e-01, 4.9695e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.1755e-01, 3.2534e-03, 1.5273e-02, 1.4429e-01,\n",
       "         8.5805e-02, 0.0000e+00, 8.0984e-02, 0.0000e+00, 0.0000e+00, 4.0015e-02,\n",
       "         0.0000e+00, 0.0000e+00, 1.3190e-03, 1.0030e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.7271e-02, 1.8751e-02, 0.0000e+00, 1.2152e-02,\n",
       "         4.2593e-02, 8.7607e-03, 0.0000e+00, 2.9683e-03, 1.0941e-01, 0.0000e+00,\n",
       "         2.8043e-02, 0.0000e+00, 4.7277e-02, 2.5990e-03, 1.8441e-02, 1.7766e-02,\n",
       "         6.0553e-02, 0.0000e+00, 6.7399e-02, 5.4665e-02, 9.6232e-02, 1.5686e-01,\n",
       "         1.3966e-01],\n",
       "        [2.2985e-02, 0.0000e+00, 2.2402e-02, 9.3442e-02, 0.0000e+00, 0.0000e+00,\n",
       "         6.0006e-02, 3.9292e-02, 0.0000e+00, 4.5110e-02, 4.3416e-02, 2.5537e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.6627e-01, 1.2143e-01, 0.0000e+00, 7.7816e-02, 0.0000e+00, 6.7981e-02,\n",
       "         4.5084e-02, 0.0000e+00, 0.0000e+00, 4.5590e-02, 3.2535e-02, 0.0000e+00,\n",
       "         6.8342e-02, 2.0524e-02, 0.0000e+00, 0.0000e+00, 1.2941e-01, 0.0000e+00,\n",
       "         6.5664e-02, 0.0000e+00, 0.0000e+00, 5.1090e-02, 9.0114e-02, 1.1133e-01,\n",
       "         8.4336e-02, 1.7034e-01, 9.0387e-03, 9.8146e-02, 0.0000e+00, 1.3863e-01,\n",
       "         6.3544e-02, 5.7422e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1749e-01,\n",
       "         3.3050e-02, 3.8926e-03, 6.8250e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 7.1135e-02, 1.4958e-01, 0.0000e+00, 1.1074e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 5.7891e-02, 1.2583e-01, 0.0000e+00, 0.0000e+00,\n",
       "         9.0761e-02, 4.1661e-02, 0.0000e+00, 1.2112e-01, 9.9658e-02, 0.0000e+00,\n",
       "         0.0000e+00, 7.7959e-02, 7.1641e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.0587e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 3.0645e-02, 0.0000e+00, 5.0980e-02, 0.0000e+00,\n",
       "         0.0000e+00, 2.1529e-02, 1.0062e-01, 0.0000e+00, 4.8552e-02, 0.0000e+00,\n",
       "         0.0000e+00, 4.1529e-02, 1.0010e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.4873e-01, 0.0000e+00, 4.2725e-02, 6.3697e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.9404e-04, 1.8024e-02, 0.0000e+00, 0.0000e+00,\n",
       "         7.9067e-03, 9.7130e-03, 6.4418e-02, 0.0000e+00, 6.0714e-02, 2.9713e-02,\n",
       "         0.0000e+00, 0.0000e+00, 2.7111e-02, 0.0000e+00, 1.0014e-01, 1.0774e-01,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.8208e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6408e-02,\n",
       "         1.0830e-01, 7.1180e-02, 3.6542e-02, 0.0000e+00, 1.6775e-01, 8.7421e-02,\n",
       "         2.1661e-02, 0.0000e+00, 2.4061e-01, 7.7034e-02, 6.3369e-02, 0.0000e+00,\n",
       "         8.7605e-03, 0.0000e+00, 0.0000e+00, 2.7693e-02, 0.0000e+00, 1.2628e-01,\n",
       "         1.1202e-02, 8.6880e-02, 0.0000e+00, 0.0000e+00, 1.5397e-01, 0.0000e+00,\n",
       "         1.2786e-01, 0.0000e+00, 1.6829e-02, 8.4923e-02, 0.0000e+00, 0.0000e+00,\n",
       "         8.3832e-03, 0.0000e+00, 2.0590e-02, 0.0000e+00, 1.7148e-01, 0.0000e+00,\n",
       "         1.8652e-01, 3.0903e-04, 1.1782e-01, 1.1102e-01, 0.0000e+00, 3.7039e-02,\n",
       "         0.0000e+00, 1.1668e-01, 0.0000e+00, 4.3439e-02, 8.0409e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 4.4994e-05, 0.0000e+00, 4.5746e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.1916e-01, 0.0000e+00, 1.5931e-02, 2.4093e-01,\n",
       "         3.8837e-02, 0.0000e+00, 1.2982e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.5043e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 6.4451e-02, 3.9057e-02, 0.0000e+00, 1.4806e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 7.9547e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.4778e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.2935e-01, 3.7977e-02, 1.3335e-01, 0.0000e+00, 8.0533e-02, 1.8577e-01,\n",
       "         2.8146e-02, 4.8028e-02, 3.6140e-02, 0.0000e+00, 0.0000e+00, 2.4000e-02,\n",
       "         9.4150e-03, 4.4527e-02, 2.9149e-02, 3.6345e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0969e-02, 6.7193e-02, 4.5760e-02,\n",
       "         1.3054e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3713e-02, 0.0000e+00,\n",
       "         6.9519e-03, 0.0000e+00, 2.9810e-02, 0.0000e+00, 1.7548e-02, 8.6757e-02,\n",
       "         1.7782e-02, 8.5722e-02, 1.1348e-01, 2.8390e-03, 0.0000e+00, 0.0000e+00,\n",
       "         8.3877e-03, 2.3712e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 3.9818e-02, 1.1041e-01, 0.0000e+00, 2.3510e-01, 0.0000e+00,\n",
       "         2.6081e-01, 8.1339e-02, 0.0000e+00, 0.0000e+00, 1.9763e-02, 1.2451e-02,\n",
       "         0.0000e+00, 5.7978e-02, 0.0000e+00, 0.0000e+00, 3.4194e-02, 0.0000e+00,\n",
       "         4.8904e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0065e-03,\n",
       "         1.7860e-01, 3.5408e-02, 0.0000e+00, 0.0000e+00, 1.4010e-01, 5.8879e-02,\n",
       "         0.0000e+00, 0.0000e+00, 1.1959e-01, 2.1655e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 7.0206e-02, 1.7290e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.0049e-01, 8.6693e-02, 3.8669e-02, 0.0000e+00,\n",
       "         0.0000e+00, 1.2406e-01, 5.6974e-03, 3.9678e-02, 0.0000e+00, 0.0000e+00,\n",
       "         1.1222e-01, 3.9429e-02, 2.3477e-02, 9.6507e-02, 0.0000e+00, 0.0000e+00,\n",
       "         3.2877e-02, 1.0442e-02, 8.4182e-02, 0.0000e+00, 9.0755e-02, 0.0000e+00,\n",
       "         6.8018e-02, 1.3075e-01, 0.0000e+00, 9.0877e-02, 0.0000e+00, 6.9470e-02,\n",
       "         0.0000e+00, 2.4672e-01, 0.0000e+00, 8.2523e-02, 0.0000e+00, 1.1731e-01,\n",
       "         8.7061e-02, 6.9772e-02, 2.8609e-02, 2.4664e-02, 4.3996e-02, 5.1952e-02,\n",
       "         6.6435e-02, 4.6780e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.5432e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         2.4920e-02, 2.3642e-02, 9.5437e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.2476e-01, 7.9622e-02, 5.3382e-02, 3.9161e-02, 0.0000e+00, 7.3912e-02,\n",
       "         4.8973e-02, 0.0000e+00, 0.0000e+00, 1.3386e-01, 1.0144e-01, 1.6244e-01,\n",
       "         0.0000e+00, 0.0000e+00, 1.8629e-02, 5.0852e-02, 1.2057e-01, 2.2811e-03,\n",
       "         1.5858e-01, 0.0000e+00, 7.5386e-02, 1.3105e-01, 8.4506e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 7.5621e-02, 0.0000e+00, 2.8779e-02, 9.9171e-02,\n",
       "         9.8720e-02, 0.0000e+00, 2.3204e-02, 2.1072e-02, 0.0000e+00, 1.0132e-01,\n",
       "         1.3790e-02, 0.0000e+00, 0.0000e+00, 1.3653e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 5.6019e-02, 4.6503e-02, 0.0000e+00, 2.6695e-02,\n",
       "         8.1291e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3605e-02, 0.0000e+00,\n",
       "         2.1267e-02, 9.9641e-03, 0.0000e+00, 2.1673e-02, 5.9168e-02, 0.0000e+00,\n",
       "         3.6176e-02, 0.0000e+00, 7.1122e-02, 5.2695e-02, 1.1494e-01, 1.3770e-01,\n",
       "         1.9009e-01]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure that calling the model works as expected\n",
    "inp = torch.zeros((2, NUM_REACTIVITIES, 2))\n",
    "inp[:, 0, :] = 1\n",
    "\n",
    "model(inp.to(DEVICE)).cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionModel(\n",
      "  (proj): Linear(in_features=2, out_features=32, bias=True)\n",
      "  (pos_embedding): SinePositionalEmbedding()\n",
      "  (encoder_layers): CustomTransformerEncoder(\n",
      "    (0): CustomTransformerEncoderLayer(\n",
      "      (attention): SDPAttention(\n",
      "        (mha): MultiHeadDispatch(\n",
      "          (attention): ScaledDotProduct(\n",
      "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (in_proj_container): InputProjection(\n",
      "            (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=1024, bias=True)\n",
      "      (ff2): Linear(in_features=1024, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "    (1): CustomTransformerEncoderLayer(\n",
      "      (attention): SDPAttention(\n",
      "        (mha): MultiHeadDispatch(\n",
      "          (attention): ScaledDotProduct(\n",
      "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (in_proj_container): InputProjection(\n",
      "            (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=1024, bias=True)\n",
      "      (ff2): Linear(in_features=1024, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "    (2): CustomTransformerEncoderLayer(\n",
      "      (attention): SDPAttention(\n",
      "        (mha): MultiHeadDispatch(\n",
      "          (attention): ScaledDotProduct(\n",
      "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (in_proj_container): InputProjection(\n",
      "            (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=1024, bias=True)\n",
      "      (ff2): Linear(in_features=1024, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "    (3): CustomTransformerEncoderLayer(\n",
      "      (attention): SDPAttention(\n",
      "        (mha): MultiHeadDispatch(\n",
      "          (attention): ScaledDotProduct(\n",
      "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (in_proj_container): InputProjection(\n",
      "            (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=1024, bias=True)\n",
      "      (ff2): Linear(in_features=1024, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (head): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (final_result): Linear(in_features=457, out_features=457, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 492955\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"Total params:\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 2e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"2a3_4enc_1024ff_32lat_2hd\"\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "SHUFFLE = True\n",
    "\n",
    "# create dataloaders\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE\n",
    ")\n",
    "# no point in shuffling validation set\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedL1(\n",
    "    y_pred: torch.Tensor,\n",
    "    y_true: torch.Tensor,\n",
    "    weights: torch.Tensor,\n",
    "    l1=torch.nn.L1Loss(),\n",
    "):\n",
    "    \"\"\"\n",
    "    This is our custom loss function that takes into account sample weights\n",
    "    \"\"\"\n",
    "    return (l1(y_pred, y_true) * weights).sum(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:43.992618Z",
     "start_time": "2023-10-05T17:09:43.053099Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_batch(\n",
    "    m: torch.nn.Module, inps: torch.Tensor, outs: torch.Tensor, masks: torch.Tensor\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the loss on a batch and perform the corresponding weight updates.\n",
    "    Used for training purposes\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    loss = weightedL1(m(inps), outs, masks)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # calculate gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # return mae loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def noupdate_batch(\n",
    "    m: torch.nn.Module, inps: torch.Tensor, outs: torch.Tensor, masks: torch.Tensor\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the loss on a batch without performing any updates.\n",
    "    Used for validation purposes\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        loss = weightedL1(m(inps), outs, masks)\n",
    "\n",
    "    # return mae loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_train(\n",
    "    m: torch.nn.Module,\n",
    "    train_dataloader: data.DataLoader,\n",
    "    val_dataloader: data.DataLoader,\n",
    "    epochs: int = 1,\n",
    "    device: str = DEVICE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the given model.\n",
    "\n",
    "    Arguments:\n",
    "        - m: torch.nn.Module - the model to train.\n",
    "        - train_dataloader: data.Dataloader - the dataloader that provides the batched training data\n",
    "        - val_dataloader: data.Dataloader - the dataloader that provides the batched validation data\n",
    "        - epochs: int - how many epochs to train for. Defaults to `1`.\n",
    "        - device: str - the device to train on, defaults to `DEVICE`\n",
    "    \"\"\"\n",
    "    m = m.to(device)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        epoch_mae = 0.0\n",
    "\n",
    "        m = m.train()\n",
    "        for batch, tdata in enumerate(train_dataloader):\n",
    "            inps = torch.stack([tdata[\"inputs\"], tdata[\"bpp\"]], dim=-1)\n",
    "            outs = tdata[\"outputs\"]\n",
    "            masks = tdata[\"output_masks\"]\n",
    "\n",
    "            inps = inps.to(device)\n",
    "            outs = outs.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            mae_loss = train_batch(m, inps, outs, masks).detach().cpu()\n",
    "\n",
    "            epoch_mae += mae_loss\n",
    "\n",
    "            # log\n",
    "            print(\n",
    "                f\"Batch {batch+1}/{len(train_dataloader)}\\t- mae loss: {mae_loss:.5f}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "            # break  # used for sanity check\n",
    "        epoch_mae /= batch + 1\n",
    "\n",
    "        # do validation\n",
    "        val_mae = 0.0\n",
    "        m = m.eval()\n",
    "        for batch, vdata in enumerate(val_dataloader):\n",
    "            inps = torch.stack([vdata[\"inputs\"], vdata[\"bpp\"]], dim=-1)\n",
    "            outs = vdata[\"outputs\"]\n",
    "            masks = vdata[\"output_masks\"]\n",
    "\n",
    "            inps = inps.to(device)\n",
    "            outs = outs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            mae_loss = noupdate_batch(m, inps, outs, masks)\n",
    "\n",
    "            val_mae += mae_loss\n",
    "        val_mae /= len(val_dataloader)\n",
    "\n",
    "        print()\n",
    "        print(f\"Epoch MAE: {epoch_mae:.5f}\\tVal MAE: {val_mae:.5f}\")\n",
    "\n",
    "        writer.add_scalar(\"epoch_mae\", epoch_mae, global_step=epoch)\n",
    "        writer.add_scalar(\"val_mae\", val_mae, global_step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.128074Z",
     "start_time": "2023-10-05T17:09:43.998369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Batch 742/742\t- mae loss: 4.11679\n",
      "Epoch MAE: 4.67187\tVal MAE: 4.10320\n",
      "Epoch 2\n",
      "Batch 742/742\t- mae loss: 3.89348\n",
      "Epoch MAE: 3.99138\tVal MAE: 3.92325\n",
      "Epoch 3\n",
      "Batch 742/742\t- mae loss: 3.89573\n",
      "Epoch MAE: 3.88754\tVal MAE: 3.84639\n",
      "Epoch 4\n",
      "Batch 742/742\t- mae loss: 3.80027\n",
      "Epoch MAE: 3.83309\tVal MAE: 3.80667\n",
      "Epoch 5\n",
      "Batch 742/742\t- mae loss: 3.70595\n",
      "Epoch MAE: 3.79911\tVal MAE: 3.77492\n",
      "Epoch 6\n",
      "Batch 742/742\t- mae loss: 3.79956\n",
      "Epoch MAE: 3.77181\tVal MAE: 3.75434\n",
      "Epoch 7\n",
      "Batch 742/742\t- mae loss: 3.70231\n",
      "Epoch MAE: 3.74729\tVal MAE: 3.73875\n",
      "Epoch 8\n",
      "Batch 742/742\t- mae loss: 3.78910\n",
      "Epoch MAE: 3.72632\tVal MAE: 3.71057\n",
      "Epoch 9\n",
      "Batch 742/742\t- mae loss: 3.64161\n",
      "Epoch MAE: 3.70269\tVal MAE: 3.66665\n",
      "Epoch 10\n",
      "Batch 742/742\t- mae loss: 3.55265\n",
      "Epoch MAE: 3.67055\tVal MAE: 3.64964\n"
     ]
    }
   ],
   "source": [
    "# baseline gets ~ 3.02 on dms w/ 10 epochs, ~ 3.87 on 2a3 w/ 20 epochs\n",
    "masked_train(\n",
    "    model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained our model, we can save its weights and biases (\"state dict\") so that we can load them for later inferencing or further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.616528Z",
     "start_time": "2023-10-05T17:14:00.119617Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"{desired_dataset}_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have both models, it's time to create a submission file. \n",
    "This section creates a zipped csv submission file that can\n",
    "be submitted on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.624146Z",
     "start_time": "2023-10-05T17:14:00.601389Z"
    }
   },
   "outputs": [],
   "source": [
    "make_submissions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:01.233216Z",
     "start_time": "2023-10-05T17:14:00.608006Z"
    }
   },
   "outputs": [],
   "source": [
    "valid = False\n",
    "\n",
    "if (\n",
    "    os.path.exists(\"2a3_model\")\n",
    "    and os.path.exists(\"dms_model\")\n",
    "    and os.path.exists(\"test_sequences.csv\")\n",
    "    and make_submissions\n",
    "):\n",
    "    if use_baseline:\n",
    "        model_dms = BaselineModel()\n",
    "        model_2a3 = BaselineModel()\n",
    "    else:\n",
    "        model_2a3 = AttentionModel(**model_2a3_kwargs)\n",
    "        model_dms = AttentionModel(**model_dms_kwargs)\n",
    "\n",
    "    model_2a3.load_state_dict(torch.load(\"2a3_model\"))\n",
    "    model_dms.load_state_dict(torch.load(\"dms_model\"))\n",
    "\n",
    "    model_2a3.eval().to(DEVICE)\n",
    "    model_dms.eval().to(DEVICE)\n",
    "\n",
    "    valid = True\n",
    "else:\n",
    "    print(\"Not going to create submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "    model_2a3: torch.nn.Module,\n",
    "    model_dms: torch.nn.Module,\n",
    "    input_ds: str,\n",
    "    out: str,\n",
    "    batch_size: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Make predictions on the test dataset and write them to a csv file\n",
    "\n",
    "    Parameters:\n",
    "        - model_2a3: torch.nn.Module - the model trained on the 2a3 distribution\n",
    "        - model_dms: torch.nn.Module - the model trained on the dms distribution\n",
    "        - input_ds: str - name of the dataset to load\n",
    "        - out: str - name of the file to write to\n",
    "        - batch_size: int - size of the batches to use to process the data.\n",
    "            In general, larger batch sizes mean faster runtime\n",
    "    \"\"\"\n",
    "    ds = Dataset.load_from_disk(input_ds).with_format(\"torch\")\n",
    "    loader = data.DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    iterable = iter(loader)\n",
    "\n",
    "    with open(out, \"w\") as outfile:\n",
    "        # write the header\n",
    "        outfile.write(\"id,reactivity_DMS_MaP,reactivity_2A3_MaP\\n\")\n",
    "\n",
    "        for _ in tqdm(range(len(loader))):\n",
    "            # get the next group of data\n",
    "            tdata = next(iterable)\n",
    "            inputs = torch.stack([tdata[\"inputs\"], tdata[\"bpp\"]], dim=-1).to(DEVICE)\n",
    "            min_ids = tdata[\"id_min\"].numpy()\n",
    "            max_ids = tdata[\"id_max\"].numpy()\n",
    "\n",
    "            # make predictions w/o gradients\n",
    "            with torch.no_grad():\n",
    "                preds_2a3 = model_2a3(inputs).cpu().numpy()\n",
    "                preds_dms = model_dms(inputs).cpu().numpy()\n",
    "\n",
    "            # write preds\n",
    "            for i in range(inputs.shape[0]):\n",
    "                outfile.writelines(\n",
    "                    map(\n",
    "                        lambda seq_idx: f\"{seq_idx},{preds_dms[i, seq_idx-min_ids[i]]:.3f},{preds_2a3[i, seq_idx-min_ids[i]]:.3f}\\n\",\n",
    "                        # +1 since the id_max is inclusive\n",
    "                        range(min_ids[i], max_ids[i] + 1),\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-05T17:14:01.248308Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5250/5250 [30:39<00:00,  2.85it/s]\n"
     ]
    }
   ],
   "source": [
    "if valid:\n",
    "    pipeline(\n",
    "        model_2a3,\n",
    "        model_dms,\n",
    "        \"test_data_preprocessed\",\n",
    "        \"submission.csv\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "else:\n",
    "    print(\"Not going to create submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipping submissions. This may take a while...\n",
      "  adding: submission.csv (deflated 80%)\n",
      "Done zipping submissions!\n"
     ]
    }
   ],
   "source": [
    "if valid:\n",
    "    # zip our submission into an easily-uploadable zip file\n",
    "    print(\"zipping submissions. This may take a while...\")\n",
    "    os.system(\"zip submission.csv.zip submission.csv\")\n",
    "    print(\"Done zipping submissions!\")\n",
    "else:\n",
    "    print(\"Not going to zip submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
