{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ribonanza - Attempt 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second approach to the [Stanford Ribonanza problem](https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/) that builds off the first and second approaches.\n",
    "\n",
    "Major differences:\n",
    "- use of pytorch instead of tensorflow\n",
    "- use of attention model architecture\n",
    "- use bpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the attention architecture scores 0.19377"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- improve model\n",
    "- experiment with other types of attention\n",
    "- use full transformer model (with decoder) (not just encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filesystem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your project directory should look like this:\n",
    "\n",
    "- `(project directory)`\n",
    "    - `ribonanza2.ipynb`\n",
    "    - `train_data.csv`\n",
    "    - `test_data.csv` (optional)\n",
    "\n",
    "`train_data.csv` is the only file necessary for training, and it can be downloaded from the kaggle competition linked in the description.\n",
    "\n",
    "`test_data.csv` is only necessary if you intend to make and submit predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Seetup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to install pip packages:\n",
    "```sh\n",
    "pip install torch numpy seaborn xformers arnie datasets\n",
    "```\n",
    "Need to install conda packages for eternafold:\n",
    "```sh\n",
    "conda install -c conda-forge \"libgcc-ng>=12\" \"libstdcxx-ng>=12\"\n",
    "conda install -c bioconda eternafold\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "# for visualization\n",
    "import seaborn\n",
    "\n",
    "# typing hints\n",
    "from typing import List\n",
    "from collections.abc import Callable\n",
    "\n",
    "# used for better attention mechanisms\n",
    "import xformers.components.positional_embedding as embeddings\n",
    "import xformers.ops as xops\n",
    "import xformers.components.attention as attentions\n",
    "import xformers.components.attention.utils as att_utils\n",
    "import xformers.components as components\n",
    "\n",
    "# used for bpps\n",
    "from arnie.bpps import bpps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "# according to kaggle, this is the maximum # of reactivites to be used\n",
    "NUM_REACTIVITIES = 457\n",
    "\n",
    "# there are 4 different bases (AUCG)\n",
    "NUM_BASES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"  # if no gpu available, use cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(out: str, key: str, value: str, file_name: str, force: bool):\n",
    "    \"\"\"\n",
    "    Filters a file to only take datapoints\n",
    "    whose values of `key` are `value`.\n",
    "\n",
    "    Parameters:\n",
    "        - out: str - the name of the file that will store the filtered datapoints\n",
    "        - key: str - the name of the key to look at\n",
    "        - value: str - the value that the key should have\n",
    "        - file_name: str - the name of the file that contains all the datapoints.\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    if os.path.exists(out) and not force:\n",
    "        print(\"File already exists, not doing any work\")\n",
    "        return\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    # count how many lines we have in total\n",
    "    with open(file_name) as file:\n",
    "        line = file.readline()  # ignore the header\n",
    "        line = (\n",
    "            file.readline()\n",
    "        )  # take the first line since we increment count in the loop\n",
    "        while line != \"\":\n",
    "            count += 1\n",
    "            line = file.readline()\n",
    "\n",
    "    # use that knowledge for a progress bar\n",
    "    with open(file_name, \"r\") as file, open(out, \"w\") as outfile:\n",
    "        # write the header\n",
    "        header = file.readline()\n",
    "        outfile.write(header)\n",
    "\n",
    "        # get what index the SN_filter is\n",
    "        SN_idx = header.split(\",\").index(key)\n",
    "\n",
    "        # only take the approved filtered lines\n",
    "        for _ in tqdm(range(count)):\n",
    "            line = file.readline()\n",
    "            temp = line.split(\",\")\n",
    "            if temp[SN_idx] == value:\n",
    "                outfile.write(line)\n",
    "\n",
    "\n",
    "def filter_train_data(force: bool = False):\n",
    "    \"\"\"\n",
    "    Filters the immense train_data.csv to only take datapoints\n",
    "    whose SN_filter (Signal to Noise filter) is 1. In other words,\n",
    "    we only take good reads. These filtered datapoints are then\n",
    "    written to the file provided\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\"train_data_filtered.csv\", \"SN_filter\", \"1\", \"train_data.csv\", force)\n",
    "\n",
    "\n",
    "def filter_2A3(force: bool = False):\n",
    "    \"\"\"\n",
    "    Only take the 2A3 points\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\n",
    "        \"train_data_2a3.csv\",\n",
    "        \"experiment_type\",\n",
    "        \"2A3_MaP\",\n",
    "        \"train_data_filtered.csv\",\n",
    "        force,\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_DMS(force: bool = False):\n",
    "    \"\"\"\n",
    "    Only take the DMS points\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\n",
    "        \"train_data_dms.csv\",\n",
    "        \"experiment_type\",\n",
    "        \"DMS_MaP\",\n",
    "        \"train_data_filtered.csv\",\n",
    "        force,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# filter our data\n",
    "filter_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# take the 2a3 points\n",
    "filter_2A3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# take the dms points\n",
    "filter_DMS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode inputs as\n",
    "# A : 1\n",
    "# U : 2\n",
    "# C : 3\n",
    "# G : 4\n",
    "base_map = {\n",
    "    \"A\": 1,\n",
    "    \"U\": 2,\n",
    "    \"C\": 3,\n",
    "    \"G\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(row):\n",
    "    \"\"\"\n",
    "    Convert a row containing all csv columns in the original dataset\n",
    "    to a row containing only the columns:\n",
    "    - inputs\n",
    "    - outputs\n",
    "    - bpp\n",
    "    - output_masks\n",
    "    - reactivity_error\n",
    "    - bool_output_masks\n",
    "    \"\"\"\n",
    "    # initialize arrays\n",
    "    # note that we assume everything is masked until told otherwise\n",
    "    inputs = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "    bpp = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "    output_masks = np.ones((NUM_REACTIVITIES,), dtype=np.bool_)\n",
    "    reactivity_errors = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "    reactivities = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "\n",
    "    seq_len = len(row[\"sequence\"])\n",
    "\n",
    "    # encode the bases\n",
    "    inputs[:seq_len] = np.array(\n",
    "        list(map(lambda letter: base_map[letter], row[\"sequence\"]))\n",
    "    )\n",
    "\n",
    "    # get the probability that any of those bases are paired\n",
    "    bpp[:seq_len] = np.max(bpps(row[\"sequence\"], package=\"eternafold\"), axis=-1)\n",
    "\n",
    "    # get the reactivities and their errors\n",
    "    reactivities[:seq_len] = np.array(\n",
    "        list(\n",
    "            map(\n",
    "                lambda seq_idx: np.float32(\n",
    "                    row[\"reactivity_\" + str(seq_idx + 1).rjust(4, \"0\")]\n",
    "                ),\n",
    "                range(seq_len),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    reactivity_errors[:seq_len] = np.array(\n",
    "        list(\n",
    "            map(\n",
    "                lambda seq_idx: np.float32(\n",
    "                    row[\"reactivity_error_\" + str(seq_idx + 1).rjust(4, \"0\")]\n",
    "                ),\n",
    "                range(seq_len),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # replace reactivity error nans with 0s (assume no error)\n",
    "    reactivity_errors = np.where(np.isnan(reactivity_errors), 0.0, reactivity_errors)\n",
    "\n",
    "    # get where all the reactivities are nan\n",
    "    nan_locats = np.isnan(reactivities)\n",
    "\n",
    "    # where it is nan, store True, else False\n",
    "    output_masks[:seq_len] = nan_locats[:seq_len]\n",
    "\n",
    "    # where it is not nan, store the reactivity and error, else 0\n",
    "    reactivities[:seq_len] = np.where(\n",
    "        nan_locats[:seq_len] == False, reactivities[:seq_len], 0.0\n",
    "    )\n",
    "    reactivity_errors[:seq_len] = np.where(\n",
    "        nan_locats[:seq_len] == False, reactivity_errors[:seq_len], 0.0\n",
    "    )\n",
    "\n",
    "    # store the values\n",
    "    row = {}\n",
    "    row[\"inputs\"] = inputs\n",
    "    row[\"bpp\"] = bpp\n",
    "    row[\"outputs\"] = np.clip(reactivities, 0, 1)\n",
    "    row[\"output_masks\"] = np.clip(\n",
    "        np.where(output_masks, 0.0, 1.0) - np.abs(reactivity_errors), 0, 1\n",
    "    )\n",
    "    row[\"bool_output_masks\"] = output_masks\n",
    "    row[\"reactivity_errors\"] = np.abs(reactivity_errors)\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def process_data_test(row):\n",
    "    \"\"\"\n",
    "    Almost the same as process_data, except it only takes inputs and bpp\n",
    "    \"\"\"\n",
    "    # initialize arrays\n",
    "    # note that we assume everything is masked until told otherwise\n",
    "    inputs = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "    bpp = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "\n",
    "    seq_len = len(row[\"sequence\"])\n",
    "\n",
    "    # encode the bases\n",
    "    inputs[:seq_len] = np.array(\n",
    "        list(map(lambda letter: base_map[letter], row[\"sequence\"]))\n",
    "    )\n",
    "\n",
    "    # get the probability that any of those bases are paired\n",
    "    bpp[:seq_len] = np.max(bpps(row[\"sequence\"], package=\"eternafold\"), axis=-1)\n",
    "\n",
    "    row[\"inputs\"] = inputs\n",
    "    row[\"bpp\"] = bpp\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_csv(\n",
    "    out: str,\n",
    "    file_name: str,\n",
    "    n_proc: int = 12,\n",
    "    map_fn: Callable = process_data,\n",
    "    extra_cols_to_keep: List[str] = [],\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess the csv and save the preprocessed data as a dataset\n",
    "    that can be loaded via datasets.Dataset.load_from_file\n",
    "\n",
    "    The dataset contains the following items:\n",
    "        - bool_output_masks: Tensor(dtype=torch.bool) - the output masks.\n",
    "            If True, then that item should NOT be used to calculate loss.\n",
    "            If False, then that item should be used to calculate loss\n",
    "        - reactivity_errors: Tensor(dtype=torch.float32) - the reactivity errors\n",
    "        - output_masks: Tensor(dtype=torch.float32) - the elementwise weights to multiply the loss by to properly\n",
    "            account for masked items and reactivity errors\n",
    "        - inputs: tensor(dtype=torch.float32) - the input sequence, specifically of shape (None, NUM_REACTIVITIES)\n",
    "        - bpp: tensor(dtype=torch.float32)\n",
    "        - outputs: tensor(dtype=torch.float32) - the expected reactivities. Note that a simple MAE or MSE loss will not\n",
    "            suffice for training models on this dataset. Please use the output_masks tensor as well.\n",
    "\n",
    "    Parameters:\n",
    "        - out: str - the name of the file to save the arrays to\n",
    "        - file_name: str - the name of the input csv file\n",
    "        - n_proc: int - the number of processes to use while processing data\n",
    "        - map_fn: Callable - the function to apply to all dataset rows\n",
    "        - extra_cols_to_keep: List[str] - the names of any extra columns to keep in the dataset\n",
    "    \"\"\"\n",
    "    if os.path.exists(out):\n",
    "        print(\n",
    "            \"File already exists, not doing any work.\\n\"\n",
    "            + \"To force re-preprocessing, delete the dataset directory and restart the kernel.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    names_to_keep = [\n",
    "        \"reactivity_errors\",\n",
    "        \"bool_output_masks\",\n",
    "        \"output_masks\",\n",
    "        \"inputs\",\n",
    "        \"outputs\",\n",
    "        \"bpp\",\n",
    "    ] + extra_cols_to_keep\n",
    "\n",
    "    # load dataset and map it to our preprocess function\n",
    "    ds = Dataset.from_csv(file_name).map(map_fn, num_proc=n_proc)\n",
    "\n",
    "    # drop excess columns and save to disk\n",
    "    ds.remove_columns(\n",
    "        list(filter(lambda c: c not in names_to_keep, ds.column_names))\n",
    "    ).save_to_disk(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work.\n",
      "To force re-preprocessing, delete the dataset directory and restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\"train_data_2a3_preprocessed\", \"train_data_2a3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work.\n",
      "To force re-preprocessing, delete the dataset directory and restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\"train_data_dms_preprocessed\", \"train_data_dms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work.\n",
      "To force re-preprocessing, delete the dataset directory and restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\n",
    "    \"test_data_preprocessed\",\n",
    "    \"test_sequences.csv\",\n",
    "    map_fn=process_data_test,\n",
    "    extra_cols_to_keep=[\"id_min\", \"id_max\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the desired dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:33.551791Z",
     "start_time": "2023-10-05T17:09:33.535197Z"
    }
   },
   "outputs": [],
   "source": [
    "desired_dataset = \"dms\"  # either \"2a3\" or \"dms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['inputs', 'bpp', 'outputs', 'output_masks', 'bool_output_masks', 'reactivity_errors'],\n",
       "    num_rows: 226925\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(\n",
    "    f\"train_data_{desired_dataset}_preprocessed\"\n",
    ").with_format(\"torch\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set is len 204232 and val dataset is len 22693\n"
     ]
    }
   ],
   "source": [
    "columns = [\"inputs\", \"outputs\", \"output_masks\", \"bpp\"]\n",
    "split = (\n",
    "    dataset.train_test_split(test_size=0.1).with_format(\"torch\").select_columns(columns)\n",
    ")\n",
    "train_dataset = split[\"train\"]\n",
    "val_dataset = split[\"test\"]\n",
    "\n",
    "print(\n",
    "    \"train set is len\", len(train_dataset), \"and val dataset is len\", len(val_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataset preprocessed, we can visualize what the distribution looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.844615Z",
     "start_time": "2023-10-05T17:09:42.843236Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(*args):\n",
    "    \"\"\"\n",
    "    Gets the product of all arguments passed to it\n",
    "    \"\"\"\n",
    "    prod = 1\n",
    "    for item in args:\n",
    "        prod *= item\n",
    "    return prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0.21688% of the data\n"
     ]
    }
   ],
   "source": [
    "if visualize:\n",
    "    # select all the reactivities that are valid (that shouldn't be masked)\n",
    "    visualized_items = torch.masked_select(\n",
    "        dataset[\"outputs\"], dataset[\"bool_output_masks\"] == False\n",
    "    ).numpy()\n",
    "\n",
    "    # sanity check that we didn't take all the items\n",
    "    print(\n",
    "        f\"took {visualized_items.shape[0] / multiply(*dataset['outputs'].shape):.5f}% of the data\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.851272Z",
     "start_time": "2023-10-05T17:09:42.849526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if visualize:\n",
    "    seaborn.histplot(visualized_items, binwidth=0.1)\n",
    "else:\n",
    "    print(\"Not visualizing. Set `visualize` to `True` to visualize data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model our distribution, we have two models:\n",
    "- an AttentionModel that uses attention layers\n",
    "- a BaselineModel that we compare against that uses only a couple convolution layers and a Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEfficientSelfAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, latent_dim: int, n_heads: int, dropout: float, *args, **kwargs\n",
    "    ) -> None:\n",
    "        super(MemoryEfficientSelfAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        assert latent_dim % n_heads == 0\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        # B, Seq Len, embedding -> B, Seq Len, Heads, Embedding per head\n",
    "        x = x.reshape(x.shape[0], x.shape[1], self.n_heads, x.shape[2] // self.n_heads)\n",
    "        x = xops.memory_efficient_attention(x, x, x, p=self.dropout)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SDPAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int,\n",
    "        n_heads: int,\n",
    "        dropout: float,\n",
    "        device: str,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(SDPAttention, self).__init__()\n",
    "        self.mha = components.MultiHeadDispatch(\n",
    "            latent_dim,\n",
    "            num_heads=n_heads,\n",
    "            attention=attentions.ScaledDotProduct(dropout=dropout),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        return self.mha(x, att_mask=attention_mask)\n",
    "\n",
    "\n",
    "class SlidingWindowAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int,\n",
    "        n_heads: int,\n",
    "        dropout: float,\n",
    "        device: str,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(SlidingWindowAttention, self).__init__()\n",
    "        self.mha = components.MultiHeadDispatch(\n",
    "            latent_dim,\n",
    "            n_heads,\n",
    "            attentions.LocalAttention(\n",
    "                dropout=dropout, window_size=kwargs[\"context_window\"]\n",
    "            ),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        return self.mha(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerEncoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_type: torch.nn.Module,\n",
    "        latent_dim: int,\n",
    "        ff_dim: int,\n",
    "        n_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        device: str = DEVICE,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(CustomTransformerEncoderLayer, self).__init__()\n",
    "        self.attention = attention_type(\n",
    "            latent_dim=latent_dim,\n",
    "            n_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            device=device,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.layer_norm = torch.nn.LayerNorm(latent_dim).to(device)\n",
    "\n",
    "        self.ff1 = torch.nn.Linear(latent_dim, ff_dim).to(device)\n",
    "        self.ff2 = torch.nn.Linear(ff_dim, latent_dim).to(device)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        # MHA, add, norm\n",
    "        x = self.layer_norm(self.attention(x, attention_mask=attention_mask) + x)\n",
    "\n",
    "        # ff, add, norm\n",
    "        x = self.layer_norm(self.gelu(self.ff2(self.gelu(self.ff1(x)))) + x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomTransformerEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_type: torch.nn.Module,\n",
    "        n_layers: int,\n",
    "        latent_dim: int,\n",
    "        ff_dim: int,\n",
    "        n_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        device: str = DEVICE,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(CustomTransformerEncoder, self).__init__()\n",
    "        for i in range(n_layers):\n",
    "            self.add_module(\n",
    "                str(i),\n",
    "                CustomTransformerEncoderLayer(\n",
    "                    attention_type=attention_type,\n",
    "                    latent_dim=latent_dim,\n",
    "                    ff_dim=ff_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    dropout=dropout,\n",
    "                    device=device,\n",
    "                    **kwargs\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        for module in self._modules.values():\n",
    "            x = module(x, attention_mask=attention_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_type: torch.nn.Module,\n",
    "        context_window: int = 31,\n",
    "        latent_dim: int = 128,\n",
    "        ff_dim: int = 1024,\n",
    "        n_heads: int = 2,\n",
    "        enc_layers: int = 1,\n",
    "        device: str = DEVICE,\n",
    "    ) -> None:\n",
    "        super(AttentionModel, self).__init__()\n",
    "\n",
    "        # data\n",
    "        self.n_heads = n_heads\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # projection layer\n",
    "        self.proj = torch.nn.Linear(2, latent_dim).to(device)\n",
    "\n",
    "        # positional embedding and encoder layers\n",
    "        self.pos_embedding = embeddings.SinePositionalEmbedding(latent_dim).to(device)\n",
    "        self.encoder_layers = CustomTransformerEncoder(\n",
    "            latent_dim=latent_dim,\n",
    "            ff_dim=ff_dim,\n",
    "            n_heads=n_heads,\n",
    "            device=device,\n",
    "            attention_type=attention_type,\n",
    "            n_layers=enc_layers,\n",
    "            context_window=context_window,\n",
    "        )\n",
    "\n",
    "        # output head\n",
    "        self.head = torch.nn.Linear(latent_dim, 1).to(device)\n",
    "        self.final_result = torch.nn.Linear(NUM_REACTIVITIES, NUM_REACTIVITIES).to(\n",
    "            device\n",
    "        )\n",
    "\n",
    "        # activations\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.gelu = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mask = att_utils.maybe_merge_masks(\n",
    "            att_mask=None,\n",
    "            key_padding_mask=(x != 0).any(dim=-1),\n",
    "            batch_size=x.shape[0],\n",
    "            num_heads=self.n_heads,\n",
    "            src_len=x.shape[1],\n",
    "        )\n",
    "\n",
    "        # project to latent dimension\n",
    "        x = self.proj(x)\n",
    "\n",
    "        # embed and then perform attention\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.encoder_layers(x, attention_mask=mask)\n",
    "\n",
    "        # final result\n",
    "        x = self.relu(self.final_result(self.gelu(self.head(x).flatten(start_dim=1))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(torch.nn.Module):\n",
    "    def __init__(self, context_window: int = 31, device: str = DEVICE):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.preLayer = torch.nn.Linear(2, 2).to(device)\n",
    "        self.conv_layer = torch.nn.Conv1d(2, 2, context_window, padding=\"same\").to(\n",
    "            device\n",
    "        )\n",
    "        self.conv_layer_b = torch.nn.Conv1d(2, 2, context_window, padding=\"same\").to(\n",
    "            device\n",
    "        )\n",
    "        self.ff = torch.nn.Linear(NUM_REACTIVITIES * 2, NUM_REACTIVITIES).to(device)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.gelu(self.preLayer(x))\n",
    "        x = x.reshape(x.shape[0], -1, x.shape[1])\n",
    "\n",
    "        x = self.gelu(\n",
    "            self.conv_layer(x)\n",
    "            + torch.flip(self.conv_layer_b(torch.flip(x, dims=[2])), dims=[2])\n",
    "        )\n",
    "\n",
    "        return self.relu(self.ff(x.flatten(start_dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dms_kwargs = dict(\n",
    "    latent_dim=32, n_heads=2, enc_layers=2, ff_dim=512, attention_type=SDPAttention\n",
    ")\n",
    "model_2a3_kwargs = dict(\n",
    "    latent_dim=32, n_heads=2, enc_layers=2, ff_dim=512, attention_type=SDPAttention\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_baseline = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the model\n",
    "if use_baseline:\n",
    "    model = BaselineModel()\n",
    "elif desired_dataset == \"dms\":\n",
    "    model = AttentionModel(**model_dms_kwargs)\n",
    "elif desired_dataset == \"2a3\":\n",
    "    model = AttentionModel(**model_2a3_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load old weights if possible\n",
    "if os.path.exists(f\"{desired_dataset}_model\"):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(f\"{desired_dataset}_model\"))\n",
    "        print(\"loaded previous weights\")\n",
    "    except Exception as e:\n",
    "        print(\"not loading previous weights because\", e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0788e-01, 0.0000e+00,\n",
       "         3.5060e-01, 9.0110e-02, 0.0000e+00, 0.0000e+00, 8.8806e-02, 8.3788e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 9.7450e-02, 1.4103e-01, 0.0000e+00, 0.0000e+00,\n",
       "         2.0704e-01, 0.0000e+00, 1.6694e-01, 1.2967e-02, 2.6939e-02, 2.0857e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0542e-01,\n",
       "         1.3771e-01, 0.0000e+00, 6.8213e-02, 2.1123e-01, 0.0000e+00, 0.0000e+00,\n",
       "         5.8415e-02, 2.2796e-01, 0.0000e+00, 1.1921e-01, 7.1999e-02, 0.0000e+00,\n",
       "         0.0000e+00, 1.0940e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         9.7227e-02, 0.0000e+00, 4.3816e-02, 0.0000e+00, 2.7261e-02, 6.2028e-02,\n",
       "         0.0000e+00, 0.0000e+00, 2.0569e-02, 0.0000e+00, 9.5087e-02, 1.2087e-01,\n",
       "         8.2543e-02, 0.0000e+00, 7.2169e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 6.2392e-02, 2.6027e-02, 8.6658e-03,\n",
       "         0.0000e+00, 0.0000e+00, 6.6145e-02, 0.0000e+00, 8.4197e-02, 3.8259e-03,\n",
       "         1.8554e-01, 0.0000e+00, 2.5823e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.8582e-02, 0.0000e+00, 0.0000e+00, 1.6134e-01, 4.8612e-02, 4.0927e-02,\n",
       "         1.9434e-01, 0.0000e+00, 0.0000e+00, 5.8576e-02, 1.6590e-02, 1.9759e-01,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 8.1970e-03, 2.5649e-01, 2.3969e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 3.8751e-02, 1.1379e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2730e-01, 1.8021e-01, 1.0360e-01,\n",
       "         1.5372e-01, 5.8816e-02, 0.0000e+00, 1.9323e-01, 0.0000e+00, 0.0000e+00,\n",
       "         1.7284e-01, 1.2733e-01, 1.8285e-01, 0.0000e+00, 0.0000e+00, 8.4993e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7633e-01, 8.9586e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9155e-02, 7.8270e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 6.4634e-02, 9.5660e-02, 1.3141e-02,\n",
       "         2.3588e-01, 3.0780e-02, 0.0000e+00, 2.1654e-02, 7.8683e-02, 0.0000e+00,\n",
       "         0.0000e+00, 8.6232e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 4.7248e-02, 0.0000e+00, 1.8604e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 8.5153e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.4341e-01, 0.0000e+00, 1.9568e-01, 0.0000e+00, 2.5066e-01, 5.3765e-02,\n",
       "         0.0000e+00, 9.2732e-02, 0.0000e+00, 2.0241e-01, 0.0000e+00, 0.0000e+00,\n",
       "         1.3248e-01, 3.5011e-02, 0.0000e+00, 2.0766e-01, 2.1081e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 4.3180e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         7.9635e-03, 2.3396e-02, 0.0000e+00, 1.2584e-01, 1.7162e-01, 0.0000e+00,\n",
       "         1.0710e-01, 8.6755e-02, 9.1613e-02, 1.8373e-02, 3.1052e-01, 0.0000e+00,\n",
       "         0.0000e+00, 1.7648e-01, 1.2662e-01, 1.6198e-01, 0.0000e+00, 5.3640e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8258e-01, 3.5587e-02, 4.0348e-02,\n",
       "         7.0296e-03, 0.0000e+00, 1.3854e-01, 0.0000e+00, 9.5694e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.4368e-01, 9.1414e-02, 0.0000e+00, 0.0000e+00,\n",
       "         1.4552e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         3.9886e-02, 7.0841e-02, 0.0000e+00, 1.8231e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.4261e-01, 2.8308e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         3.2814e-01, 9.5920e-02, 1.3125e-01, 0.0000e+00, 1.1776e-02, 0.0000e+00,\n",
       "         1.0221e-02, 0.0000e+00, 1.2630e-01, 0.0000e+00, 1.9238e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0980e-02, 1.4332e-01, 0.0000e+00,\n",
       "         0.0000e+00, 1.2434e-01, 1.4406e-01, 2.8413e-01, 9.3719e-02, 1.1697e-01,\n",
       "         0.0000e+00, 3.8671e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         8.4978e-02, 1.1293e-01, 0.0000e+00, 1.4256e-01, 0.0000e+00, 4.1921e-02,\n",
       "         0.0000e+00, 9.3196e-02, 5.9843e-02, 3.8992e-03, 0.0000e+00, 1.9666e-01,\n",
       "         0.0000e+00, 1.5439e-02, 0.0000e+00, 9.9417e-02, 1.0328e-01, 3.5612e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6850e-02, 0.0000e+00,\n",
       "         0.0000e+00, 1.3829e-01, 0.0000e+00, 0.0000e+00, 7.6704e-02, 4.1175e-02,\n",
       "         1.8819e-01, 0.0000e+00, 5.1151e-02, 0.0000e+00, 0.0000e+00, 5.6559e-02,\n",
       "         6.9035e-02, 0.0000e+00, 0.0000e+00, 1.0755e-01, 1.4347e-01, 0.0000e+00,\n",
       "         0.0000e+00, 7.2063e-02, 0.0000e+00, 0.0000e+00, 1.0880e-01, 1.8581e-01,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0628e-01, 4.1007e-01, 4.1235e-02,\n",
       "         0.0000e+00, 1.7299e-02, 0.0000e+00, 0.0000e+00, 1.1486e-01, 0.0000e+00,\n",
       "         0.0000e+00, 2.4822e-02, 5.4185e-02, 0.0000e+00, 0.0000e+00, 2.4643e-01,\n",
       "         8.3126e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1669e-02,\n",
       "         1.2127e-01, 1.1657e-01, 1.4456e-01, 6.3731e-02, 0.0000e+00, 0.0000e+00,\n",
       "         1.5265e-01, 0.0000e+00, 5.3530e-02, 8.1603e-02, 1.1727e-01, 0.0000e+00,\n",
       "         9.5600e-02, 7.7489e-02, 0.0000e+00, 0.0000e+00, 8.7881e-02, 0.0000e+00,\n",
       "         1.0823e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.5513e-02,\n",
       "         8.9579e-02, 1.0005e-01, 1.0540e-01, 0.0000e+00, 2.1282e-02, 0.0000e+00,\n",
       "         0.0000e+00, 1.0751e-01, 8.0550e-02, 1.5275e-01, 0.0000e+00, 7.1458e-02,\n",
       "         0.0000e+00, 9.5765e-02, 0.0000e+00, 0.0000e+00, 1.8113e-04, 0.0000e+00,\n",
       "         6.1233e-02, 9.5367e-02, 2.3825e-02, 0.0000e+00, 2.5275e-02, 0.0000e+00,\n",
       "         0.0000e+00, 1.3198e-01, 1.2338e-01, 0.0000e+00, 6.9319e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 5.0524e-02, 1.0057e-01, 0.0000e+00, 1.2445e-01,\n",
       "         7.0964e-03, 4.8989e-02, 0.0000e+00, 5.3241e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.3553e-02, 0.0000e+00, 0.0000e+00, 1.0915e-01, 4.8557e-02,\n",
       "         1.1883e-01, 1.2138e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.4536e-01, 0.0000e+00, 5.0868e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1556e-01, 4.6948e-03,\n",
       "         3.4954e-01, 1.0516e-01, 0.0000e+00, 0.0000e+00, 7.8151e-02, 9.1309e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 4.6486e-02, 1.0954e-01, 0.0000e+00, 0.0000e+00,\n",
       "         1.8044e-01, 0.0000e+00, 1.2958e-01, 2.7948e-02, 5.3434e-02, 4.5399e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1536e-01,\n",
       "         1.3934e-01, 0.0000e+00, 8.0119e-02, 1.9098e-01, 0.0000e+00, 0.0000e+00,\n",
       "         4.5548e-02, 1.8471e-01, 0.0000e+00, 5.7571e-02, 8.3488e-02, 0.0000e+00,\n",
       "         0.0000e+00, 1.2760e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4448e-02,\n",
       "         1.1658e-01, 0.0000e+00, 1.0942e-02, 0.0000e+00, 5.4775e-02, 2.5196e-02,\n",
       "         0.0000e+00, 0.0000e+00, 1.3962e-02, 0.0000e+00, 6.9599e-02, 1.0623e-01,\n",
       "         6.0318e-02, 0.0000e+00, 1.4796e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4562e-02, 1.1881e-02, 0.0000e+00,\n",
       "         1.6911e-02, 0.0000e+00, 8.1139e-02, 0.0000e+00, 7.8613e-02, 2.1864e-02,\n",
       "         1.6813e-01, 0.0000e+00, 2.4912e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2631e-01, 3.6921e-02, 3.8743e-02,\n",
       "         2.2836e-01, 0.0000e+00, 0.0000e+00, 7.0217e-02, 0.0000e+00, 2.1906e-01,\n",
       "         0.0000e+00, 9.9477e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.6369e-01, 2.1366e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.9459e-04, 9.1194e-02, 0.0000e+00, 9.4289e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8620e-01, 1.2449e-01, 1.3622e-01,\n",
       "         1.2483e-01, 7.4353e-02, 0.0000e+00, 1.9007e-01, 0.0000e+00, 0.0000e+00,\n",
       "         1.1226e-01, 1.7194e-01, 1.6587e-01, 0.0000e+00, 7.9225e-04, 5.0033e-02,\n",
       "         1.6914e-02, 0.0000e+00, 0.0000e+00, 1.2814e-01, 9.1287e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0996e-02, 8.8870e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 8.2875e-02, 1.2502e-01, 1.3669e-02,\n",
       "         2.2364e-01, 6.5603e-03, 0.0000e+00, 2.9367e-02, 6.5212e-02, 0.0000e+00,\n",
       "         0.0000e+00, 1.5251e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 7.6121e-03, 0.0000e+00, 1.8058e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 4.5109e-02, 1.3789e-03, 0.0000e+00, 0.0000e+00,\n",
       "         2.1197e-01, 0.0000e+00, 2.3242e-01, 0.0000e+00, 2.5480e-01, 8.1837e-02,\n",
       "         0.0000e+00, 8.9287e-02, 0.0000e+00, 1.8462e-01, 3.5736e-03, 4.0676e-02,\n",
       "         7.6191e-02, 4.5756e-02, 0.0000e+00, 2.0702e-01, 2.4712e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         4.2298e-02, 5.0594e-02, 0.0000e+00, 1.1946e-01, 1.7316e-01, 0.0000e+00,\n",
       "         1.1566e-01, 1.3194e-01, 8.8417e-02, 7.3019e-03, 3.3788e-01, 0.0000e+00,\n",
       "         0.0000e+00, 2.5226e-01, 1.6627e-01, 1.8972e-01, 0.0000e+00, 5.8691e-02,\n",
       "         0.0000e+00, 1.0164e-02, 0.0000e+00, 1.5658e-01, 1.4483e-02, 2.5421e-02,\n",
       "         3.7833e-02, 0.0000e+00, 5.5461e-02, 1.2391e-02, 6.0876e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.6533e-01, 8.4447e-02, 0.0000e+00, 0.0000e+00,\n",
       "         1.9288e-01, 0.0000e+00, 2.1693e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 6.9921e-02, 0.0000e+00, 3.3096e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.6105e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1743e-02, 0.0000e+00,\n",
       "         2.6514e-01, 1.4039e-01, 1.4977e-01, 0.0000e+00, 3.0453e-03, 0.0000e+00,\n",
       "         4.8932e-02, 0.0000e+00, 1.4073e-01, 0.0000e+00, 1.4743e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6895e-02, 1.8129e-01, 2.5662e-02,\n",
       "         0.0000e+00, 8.5242e-02, 1.6714e-01, 2.9321e-01, 5.9516e-02, 4.8655e-02,\n",
       "         0.0000e+00, 5.1962e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         9.6774e-02, 1.5703e-01, 0.0000e+00, 1.3235e-01, 0.0000e+00, 5.1139e-02,\n",
       "         0.0000e+00, 5.8843e-02, 6.3307e-02, 1.3025e-02, 0.0000e+00, 1.8444e-01,\n",
       "         0.0000e+00, 1.8367e-02, 0.0000e+00, 1.1867e-01, 1.0905e-01, 6.0208e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5198e-02, 0.0000e+00,\n",
       "         0.0000e+00, 1.3585e-01, 0.0000e+00, 0.0000e+00, 6.8536e-02, 6.0033e-02,\n",
       "         1.6173e-01, 0.0000e+00, 1.2108e-01, 0.0000e+00, 0.0000e+00, 6.4946e-02,\n",
       "         5.3675e-02, 2.2957e-03, 0.0000e+00, 7.1226e-02, 4.4439e-02, 0.0000e+00,\n",
       "         0.0000e+00, 2.3169e-02, 0.0000e+00, 0.0000e+00, 8.3951e-02, 1.5953e-01,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 5.3111e-02, 3.7491e-01, 7.7240e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0565e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 9.9004e-02, 0.0000e+00, 0.0000e+00, 2.2428e-01,\n",
       "         3.5001e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2206e-02, 0.0000e+00,\n",
       "         1.4068e-01, 8.8132e-02, 1.5616e-01, 3.5031e-02, 0.0000e+00, 3.8975e-03,\n",
       "         2.0663e-01, 0.0000e+00, 8.4574e-02, 4.2199e-02, 1.1245e-01, 0.0000e+00,\n",
       "         9.7464e-02, 6.2229e-02, 0.0000e+00, 0.0000e+00, 7.7076e-02, 0.0000e+00,\n",
       "         2.1195e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4571e-02,\n",
       "         1.0063e-01, 1.6842e-01, 1.4254e-01, 0.0000e+00, 4.6948e-02, 0.0000e+00,\n",
       "         0.0000e+00, 1.7728e-01, 1.0130e-01, 1.5846e-01, 0.0000e+00, 3.9033e-02,\n",
       "         0.0000e+00, 9.7510e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         7.2193e-02, 1.1785e-01, 5.4834e-02, 0.0000e+00, 1.9980e-02, 0.0000e+00,\n",
       "         1.8844e-02, 8.8154e-02, 8.4845e-02, 1.2164e-02, 1.0904e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 4.2805e-02, 7.7877e-02, 3.5190e-02, 1.3654e-01,\n",
       "         0.0000e+00, 3.4781e-02, 0.0000e+00, 3.6433e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.6898e-02, 0.0000e+00, 0.0000e+00, 9.3770e-02, 5.6064e-02,\n",
       "         7.4298e-02, 1.1793e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8904e-02,\n",
       "         1.7019e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure that calling the model works as expected\n",
    "inp = torch.zeros((2, NUM_REACTIVITIES, 2))\n",
    "inp[:, 0, :] = 1\n",
    "\n",
    "model(inp.to(DEVICE)).cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionModel(\n",
      "  (proj): Linear(in_features=2, out_features=32, bias=True)\n",
      "  (pos_embedding): SinePositionalEmbedding()\n",
      "  (encoder_layers): CustomTransformerEncoder(\n",
      "    (0): CustomTransformerEncoderLayer(\n",
      "      (attention): SDPAttention(\n",
      "        (mha): MultiHeadDispatch(\n",
      "          (attention): ScaledDotProduct(\n",
      "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (in_proj_container): InputProjection(\n",
      "            (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=512, bias=True)\n",
      "      (ff2): Linear(in_features=512, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "    (1): CustomTransformerEncoderLayer(\n",
      "      (attention): SDPAttention(\n",
      "        (mha): MultiHeadDispatch(\n",
      "          (attention): ScaledDotProduct(\n",
      "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (in_proj_container): InputProjection(\n",
      "            (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=512, bias=True)\n",
      "      (ff2): Linear(in_features=512, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (head): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (final_result): Linear(in_features=457, out_features=457, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 284635\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"Total params:\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 2e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "SHUFFLE = True\n",
    "\n",
    "# create dataloaders\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE\n",
    ")\n",
    "# no point in shuffling validation set\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedL1(\n",
    "    y_pred: torch.Tensor,\n",
    "    y_true: torch.Tensor,\n",
    "    weights: torch.Tensor,\n",
    "    l1=torch.nn.L1Loss(),\n",
    "):\n",
    "    \"\"\"\n",
    "    This is our custom loss function that takes into account sample weights\n",
    "    \"\"\"\n",
    "    return (l1(y_pred, y_true) * weights).sum(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:43.992618Z",
     "start_time": "2023-10-05T17:09:43.053099Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_batch(\n",
    "    m: torch.nn.Module, inps: torch.Tensor, outs: torch.Tensor, masks: torch.Tensor\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the loss on a batch and perform the corresponding weight updates.\n",
    "    Used for training purposes\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    loss = weightedL1(m(inps), outs, masks)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # calculate gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # return mae loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def noupdate_batch(\n",
    "    m: torch.nn.Module, inps: torch.Tensor, outs: torch.Tensor, masks: torch.Tensor\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the loss on a batch without performing any updates.\n",
    "    Used for validation purposes\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        loss = weightedL1(m(inps), outs, masks)\n",
    "\n",
    "    # return mae loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_train(\n",
    "    m: torch.nn.Module,\n",
    "    train_dataloader: data.DataLoader,\n",
    "    val_dataloader: data.DataLoader,\n",
    "    epochs: int = 1,\n",
    "    device: str = DEVICE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the given model.\n",
    "\n",
    "    Arguments:\n",
    "        - m: torch.nn.Module - the model to train.\n",
    "        - train_dataloader: data.Dataloader - the dataloader that provides the batched training data\n",
    "        - val_dataloader: data.Dataloader - the dataloader that provides the batched validation data\n",
    "        - epochs: int - how many epochs to train for. Defaults to `1`.\n",
    "        - device: str - the device to train on, defaults to `DEVICE`\n",
    "    \"\"\"\n",
    "    m = m.to(device)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        epoch_mae = 0.0\n",
    "\n",
    "        m = m.train()\n",
    "        for batch, tdata in enumerate(train_dataloader):\n",
    "            inps = torch.stack([tdata[\"inputs\"], tdata[\"bpp\"]], dim=-1)\n",
    "            outs = tdata[\"outputs\"]\n",
    "            masks = tdata[\"output_masks\"]\n",
    "\n",
    "            inps = inps.to(device)\n",
    "            outs = outs.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            mae_loss = train_batch(m, inps, outs, masks).detach().cpu()\n",
    "\n",
    "            epoch_mae += mae_loss\n",
    "\n",
    "            # log\n",
    "            print(\n",
    "                f\"Batch {batch+1}/{len(train_dataloader)}\\t- mae loss: {mae_loss:.5f}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "            # break  # used for sanity check\n",
    "        epoch_mae /= batch + 1\n",
    "\n",
    "        # do validation\n",
    "        val_mae = 0.0\n",
    "        m = m.eval()\n",
    "        for batch, vdata in enumerate(val_dataloader):\n",
    "            inps = torch.stack([vdata[\"inputs\"], vdata[\"bpp\"]], dim=-1)\n",
    "            outs = vdata[\"outputs\"]\n",
    "            masks = vdata[\"output_masks\"]\n",
    "\n",
    "            inps = inps.to(device)\n",
    "            outs = outs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            mae_loss = noupdate_batch(m, inps, outs, masks)\n",
    "\n",
    "            val_mae += mae_loss\n",
    "        val_mae /= len(val_dataloader)\n",
    "\n",
    "        print()\n",
    "        print(f\"Epoch MAE: {epoch_mae:.5f}\\tVal MAE: {val_mae:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.128074Z",
     "start_time": "2023-10-05T17:09:43.998369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 798/798\t- mae loss: 3.83902\n",
      "Epoch MAE: 4.33241\tVal MAE: 3.76460\n",
      "Epoch 2\n",
      "Batch 798/798\t- mae loss: 3.46185\n",
      "Epoch MAE: 3.52152\tVal MAE: 3.35403\n",
      "Epoch 3\n",
      "Batch 798/798\t- mae loss: 3.20625\n",
      "Epoch MAE: 3.25697\tVal MAE: 3.18290\n",
      "Epoch 4\n",
      "Batch 798/798\t- mae loss: 3.21597\n",
      "Epoch MAE: 3.14935\tVal MAE: 3.11953\n",
      "Epoch 5\n",
      "Batch 798/798\t- mae loss: 3.00551\n",
      "Epoch MAE: 3.09311\tVal MAE: 3.06649\n",
      "Epoch 6\n",
      "Batch 798/798\t- mae loss: 3.10778\n",
      "Epoch MAE: 3.05541\tVal MAE: 3.03269\n",
      "Epoch 7\n",
      "Batch 798/798\t- mae loss: 2.97073\n",
      "Epoch MAE: 3.02610\tVal MAE: 3.01211\n",
      "Epoch 8\n",
      "Batch 798/798\t- mae loss: 2.91261\n",
      "Epoch MAE: 3.00734\tVal MAE: 2.99639\n",
      "Epoch 9\n",
      "Batch 798/798\t- mae loss: 2.82940\n",
      "Epoch MAE: 2.99543\tVal MAE: 2.98463\n",
      "Epoch 10\n",
      "Batch 798/798\t- mae loss: 2.86579\n",
      "Epoch MAE: 2.98582\tVal MAE: 2.98000\n",
      "Epoch 11\n",
      "Batch 798/798\t- mae loss: 3.03437\n",
      "Epoch MAE: 2.97808\tVal MAE: 2.97148\n",
      "Epoch 12\n",
      "Batch 798/798\t- mae loss: 2.96285\n",
      "Epoch MAE: 2.97133\tVal MAE: 2.96769\n",
      "Epoch 13\n",
      "Batch 798/798\t- mae loss: 2.95154\n",
      "Epoch MAE: 2.96607\tVal MAE: 2.96203\n",
      "Epoch 14\n",
      "Batch 798/798\t- mae loss: 2.82993\n",
      "Epoch MAE: 2.96191\tVal MAE: 2.95282\n",
      "Epoch 15\n",
      "Batch 798/798\t- mae loss: 2.96407\n",
      "Epoch MAE: 2.95667\tVal MAE: 2.95677\n",
      "Epoch 16\n",
      "Batch 798/798\t- mae loss: 2.96355\n",
      "Epoch MAE: 2.95250\tVal MAE: 2.94771\n",
      "Epoch 17\n",
      "Batch 798/798\t- mae loss: 2.97756\n",
      "Epoch MAE: 2.94760\tVal MAE: 2.94633\n",
      "Epoch 18\n",
      "Batch 798/798\t- mae loss: 3.06181\n",
      "Epoch MAE: 2.94440\tVal MAE: 2.94484\n",
      "Epoch 19\n",
      "Batch 798/798\t- mae loss: 3.03359\n",
      "Epoch MAE: 2.94089\tVal MAE: 2.93981\n",
      "Epoch 20\n",
      "Batch 798/798\t- mae loss: 2.78693\n",
      "Epoch MAE: 2.93695\tVal MAE: 2.93312\n"
     ]
    }
   ],
   "source": [
    "# baseline gets ~ 3.02 on dms w/ 10 epochs, ~ 3.87 on 2a3 w/ 20 epochs\n",
    "masked_train(\n",
    "    model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained our model, we can save its weights and biases (\"state dict\") so that we can load them for later inferencing or further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.616528Z",
     "start_time": "2023-10-05T17:14:00.119617Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"{desired_dataset}_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have both models, it's time to create a submission file. \n",
    "This section creates a zipped csv submission file that can\n",
    "be submitted on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.624146Z",
     "start_time": "2023-10-05T17:14:00.601389Z"
    }
   },
   "outputs": [],
   "source": [
    "make_submissions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:01.233216Z",
     "start_time": "2023-10-05T17:14:00.608006Z"
    }
   },
   "outputs": [],
   "source": [
    "valid = False\n",
    "\n",
    "if (\n",
    "    os.path.exists(\"2a3_model\")\n",
    "    and os.path.exists(\"dms_model\")\n",
    "    and os.path.exists(\"test_sequences.csv\")\n",
    "    and make_submissions\n",
    "):\n",
    "    if use_baseline:\n",
    "        model_dms = BaselineModel()\n",
    "        model_2a3 = BaselineModel()\n",
    "    else:\n",
    "        model_2a3 = AttentionModel(**model_2a3_kwargs)\n",
    "        model_dms = AttentionModel(**model_dms_kwargs)\n",
    "\n",
    "    model_2a3.load_state_dict(torch.load(\"2a3_model\"))\n",
    "    model_dms.load_state_dict(torch.load(\"dms_model\"))\n",
    "\n",
    "    model_2a3.eval().to(DEVICE)\n",
    "    model_dms.eval().to(DEVICE)\n",
    "\n",
    "    valid = True\n",
    "else:\n",
    "    print(\"Not going to create submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "    model_2a3: torch.nn.Module,\n",
    "    model_dms: torch.nn.Module,\n",
    "    input_ds: str,\n",
    "    out: str,\n",
    "    batch_size: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Make predictions on the test dataset and write them to a csv file\n",
    "\n",
    "    Parameters:\n",
    "        - model_2a3: torch.nn.Module - the model trained on the 2a3 distribution\n",
    "        - model_dms: torch.nn.Module - the model trained on the dms distribution\n",
    "        - input_ds: str - name of the dataset to load\n",
    "        - out: str - name of the file to write to\n",
    "        - batch_size: int - size of the batches to use to process the data.\n",
    "            In general, larger batch sizes mean faster runtime\n",
    "    \"\"\"\n",
    "    ds = Dataset.load_from_disk(input_ds).with_format(\"torch\")\n",
    "    loader = data.DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    iterable = iter(loader)\n",
    "\n",
    "    with open(out, \"w\") as outfile:\n",
    "        # write the header\n",
    "        outfile.write(\"id,reactivity_DMS_MaP,reactivity_2A3_MaP\\n\")\n",
    "\n",
    "        for _ in tqdm(range(len(loader))):\n",
    "            # get the next group of data\n",
    "            tdata = next(iterable)\n",
    "            inputs = torch.stack([tdata[\"inputs\"], tdata[\"bpp\"]], dim=-1).to(DEVICE)\n",
    "            min_ids = tdata[\"id_min\"].numpy()\n",
    "            max_ids = tdata[\"id_max\"].numpy()\n",
    "\n",
    "            # make predictions w/o gradients\n",
    "            with torch.no_grad():\n",
    "                preds_2a3 = model_2a3(inputs).cpu().numpy()\n",
    "                preds_dms = model_dms(inputs).cpu().numpy()\n",
    "\n",
    "            # write preds\n",
    "            for i in range(inputs.shape[0]):\n",
    "                outfile.writelines(\n",
    "                    map(\n",
    "                        lambda seq_idx: f\"{seq_idx},{preds_dms[i, seq_idx-min_ids[i]]:.3f},{preds_2a3[i, seq_idx-min_ids[i]]:.3f}\\n\",\n",
    "                        # +1 since the id_max is inclusive\n",
    "                        range(min_ids[i], max_ids[i] + 1),\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-05T17:14:01.248308Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5250/5250 [14:38<00:00,  5.97it/s]\n"
     ]
    }
   ],
   "source": [
    "if valid:\n",
    "    pipeline(\n",
    "        model_2a3,\n",
    "        model_dms,\n",
    "        \"test_data_preprocessed\",\n",
    "        \"submission.csv\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "else:\n",
    "    print(\"Not going to create submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipping submissions. This may take a while...\n",
      "updating: submission.csv (deflated 80%)\n",
      "Done zipping submissions!\n"
     ]
    }
   ],
   "source": [
    "if valid:\n",
    "    # zip our submission into an easily-uploadable zip file\n",
    "    print(\"zipping submissions. This may take a while...\")\n",
    "    os.system(\"zip submission.csv.zip submission.csv\")\n",
    "    print(\"Done zipping submissions!\")\n",
    "else:\n",
    "    print(\"Not going to zip submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
