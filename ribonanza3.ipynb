{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ribonanza - Attempt 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second approach to the [Stanford Ribonanza problem](https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/) that builds off the first and second approaches.\n",
    "\n",
    "Major differences:\n",
    "- use of pytorch instead of tensorflow\n",
    "- use of attention model architecture\n",
    "- use bpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the attention architecture scores 0.182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- experiment with other types of attention\n",
    "- try using LinearFold instead of EternaFold\n",
    "- try using ThreshKnot instead of EternaFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filesystem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your project directory should look like this:\n",
    "\n",
    "- `(project directory)`\n",
    "    - `ribonanza2.ipynb`\n",
    "    - `train_data.csv`\n",
    "    - `test_data.csv` (optional)\n",
    "\n",
    "`train_data.csv` is the only file necessary for training, and it can be downloaded from the kaggle competition linked in the description.\n",
    "\n",
    "`test_data.csv` is only necessary if you intend to make and submit predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Seetup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to install pip packages:\n",
    "```sh\n",
    "pip install torch numpy seaborn xformers arnie datasets tensorboard\n",
    "```\n",
    "Need to install conda packages for eternafold:\n",
    "```sh\n",
    "conda install -c conda-forge \"libgcc-ng>=12\" \"libstdcxx-ng>=12\"\n",
    "conda install -c bioconda eternafold\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "# for visualization\n",
    "import seaborn\n",
    "\n",
    "# typing hints\n",
    "from typing import List\n",
    "from collections.abc import Callable\n",
    "\n",
    "# used for better attention mechanisms\n",
    "import xformers.components.positional_embedding as embeddings\n",
    "import xformers.ops as xops\n",
    "import xformers.components.attention as attentions\n",
    "import xformers.components.attention.utils as att_utils\n",
    "import xformers.components as components\n",
    "\n",
    "# used for bpps\n",
    "from arnie.bpps import bpps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "# according to kaggle, this is the maximum # of reactivites to be used\n",
    "NUM_REACTIVITIES = 457\n",
    "\n",
    "# there are 4 different bases (AUCG)\n",
    "NUM_BASES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if no gpu available, use cpu\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(out: str, key: str, value: str, file_name: str, force: bool):\n",
    "    \"\"\"\n",
    "    Filters a file to only take datapoints\n",
    "    whose values of `key` are `value`.\n",
    "\n",
    "    Parameters:\n",
    "        - out: str - the name of the file that will store the filtered datapoints\n",
    "        - key: str - the name of the key to look at\n",
    "        - value: str - the value that the key should have\n",
    "        - file_name: str - the name of the file that contains all the datapoints.\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    if os.path.exists(out) and not force:\n",
    "        print(\"File already exists, not doing any work\")\n",
    "        return\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    # count how many lines we have in total\n",
    "    with open(file_name) as file:\n",
    "        line = file.readline()  # ignore the header\n",
    "        line = (\n",
    "            file.readline()\n",
    "        )  # take the first line since we increment count in the loop\n",
    "        while line != \"\":\n",
    "            count += 1\n",
    "            line = file.readline()\n",
    "\n",
    "    # use that knowledge for a progress bar\n",
    "    with open(file_name, \"r\") as file, open(out, \"w\") as outfile:\n",
    "        # write the header\n",
    "        header = file.readline()\n",
    "        outfile.write(header)\n",
    "\n",
    "        # get what index the SN_filter is\n",
    "        SN_idx = header.split(\",\").index(key)\n",
    "\n",
    "        # only take the approved filtered lines\n",
    "        for _ in tqdm(range(count)):\n",
    "            line = file.readline()\n",
    "            temp = line.split(\",\")\n",
    "            if temp[SN_idx] == value:\n",
    "                outfile.write(line)\n",
    "\n",
    "\n",
    "def filter_train_data(force: bool = False):\n",
    "    \"\"\"\n",
    "    Filters the immense train_data.csv to only take datapoints\n",
    "    whose SN_filter (Signal to Noise filter) is 1. In other words,\n",
    "    we only take good reads. These filtered datapoints are then\n",
    "    written to the file provided\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\"train_data_filtered.csv\", \"SN_filter\", \"1\", \"train_data.csv\", force)\n",
    "\n",
    "\n",
    "def filter_2A3(force: bool = False):\n",
    "    \"\"\"\n",
    "    Only take the 2A3 points\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\n",
    "        \"train_data_2a3.csv\",\n",
    "        \"experiment_type\",\n",
    "        \"2A3_MaP\",\n",
    "        \"train_data_filtered.csv\",\n",
    "        force,\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_DMS(force: bool = False):\n",
    "    \"\"\"\n",
    "    Only take the DMS points\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\n",
    "        \"train_data_dms.csv\",\n",
    "        \"experiment_type\",\n",
    "        \"DMS_MaP\",\n",
    "        \"train_data_filtered.csv\",\n",
    "        force,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# filter our data\n",
    "filter_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# take the 2a3 points\n",
    "filter_2A3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# take the dms points\n",
    "filter_DMS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode inputs as\n",
    "# A : 1\n",
    "# U : 2\n",
    "# C : 3\n",
    "# G : 4\n",
    "base_map = {\n",
    "    \"A\": 1,\n",
    "    \"U\": 2,\n",
    "    \"C\": 3,\n",
    "    \"G\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(row):\n",
    "    \"\"\"\n",
    "    Convert a row containing all csv columns in the original dataset\n",
    "    to a row containing only the columns:\n",
    "    - inputs\n",
    "    - outputs\n",
    "    - bpp\n",
    "    - output_masks\n",
    "    - reactivity_error\n",
    "    - bool_output_masks\n",
    "    \"\"\"\n",
    "    # initialize arrays\n",
    "    # note that we assume everything is masked until told otherwise\n",
    "    inputs = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "    bpp = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "    output_masks = np.ones((NUM_REACTIVITIES,), dtype=np.bool_)\n",
    "    reactivity_errors = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "    reactivities = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "\n",
    "    seq_len = len(row[\"sequence\"])\n",
    "\n",
    "    # encode the bases\n",
    "    inputs[:seq_len] = np.array(\n",
    "        list(map(lambda letter: base_map[letter], row[\"sequence\"]))\n",
    "    )\n",
    "\n",
    "    # get the probability that any of those bases are paired\n",
    "    bpp[:seq_len] = np.max(bpps(row[\"sequence\"], package=\"eternafold\"), axis=-1)\n",
    "\n",
    "    # get the reactivities and their errors\n",
    "    reactivities[:seq_len] = np.array(\n",
    "        list(\n",
    "            map(\n",
    "                lambda seq_idx: np.float32(\n",
    "                    row[\"reactivity_\" + str(seq_idx + 1).rjust(4, \"0\")]\n",
    "                ),\n",
    "                range(seq_len),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    reactivity_errors[:seq_len] = np.array(\n",
    "        list(\n",
    "            map(\n",
    "                lambda seq_idx: np.float32(\n",
    "                    row[\"reactivity_error_\" + str(seq_idx + 1).rjust(4, \"0\")]\n",
    "                ),\n",
    "                range(seq_len),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # replace reactivity error nans with 0s (assume no error)\n",
    "    reactivity_errors = np.where(np.isnan(reactivity_errors), 0.0, reactivity_errors)\n",
    "\n",
    "    # get where all the reactivities are nan\n",
    "    nan_locats = np.isnan(reactivities)\n",
    "\n",
    "    # where it is nan, store True, else False\n",
    "    output_masks[:seq_len] = nan_locats[:seq_len]\n",
    "\n",
    "    # where it is not nan, store the reactivity and error, else 0\n",
    "    reactivities[:seq_len] = np.where(\n",
    "        nan_locats[:seq_len] == False, reactivities[:seq_len], 0.0\n",
    "    )\n",
    "    reactivity_errors[:seq_len] = np.where(\n",
    "        nan_locats[:seq_len] == False, reactivity_errors[:seq_len], 0.0\n",
    "    )\n",
    "\n",
    "    # store the values\n",
    "    row = {}\n",
    "    row[\"inputs\"] = inputs\n",
    "    row[\"bpp\"] = bpp\n",
    "    row[\"outputs\"] = np.clip(reactivities, 0, 1)\n",
    "    row[\"output_masks\"] = np.clip(\n",
    "        np.where(output_masks, 0.0, 1.0) - np.abs(reactivity_errors), 0, 1\n",
    "    )\n",
    "    row[\"bool_output_masks\"] = output_masks\n",
    "    row[\"reactivity_errors\"] = np.abs(reactivity_errors)\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def process_data_test(row):\n",
    "    \"\"\"\n",
    "    Almost the same as process_data, except it only takes inputs and bpp\n",
    "    \"\"\"\n",
    "    # initialize arrays\n",
    "    # note that we assume everything is masked until told otherwise\n",
    "    inputs = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "    bpp = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "\n",
    "    seq_len = len(row[\"sequence\"])\n",
    "\n",
    "    # encode the bases\n",
    "    inputs[:seq_len] = np.array(\n",
    "        list(map(lambda letter: base_map[letter], row[\"sequence\"]))\n",
    "    )\n",
    "\n",
    "    # get the probability that any of those bases are paired\n",
    "    bpp[:seq_len] = np.max(bpps(row[\"sequence\"], package=\"eternafold\"), axis=-1)\n",
    "\n",
    "    row[\"inputs\"] = inputs\n",
    "    row[\"bpp\"] = bpp\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_csv(\n",
    "    out: str,\n",
    "    file_name: str,\n",
    "    n_proc: int = 12,\n",
    "    map_fn: Callable = process_data,\n",
    "    extra_cols_to_keep: List[str] = [],\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess the csv and save the preprocessed data as a dataset\n",
    "    that can be loaded via datasets.Dataset.load_from_file\n",
    "\n",
    "    The dataset contains the following items:\n",
    "        - bool_output_masks: Tensor(dtype=torch.bool) - the output masks.\n",
    "            If True, then that item should NOT be used to calculate loss.\n",
    "            If False, then that item should be used to calculate loss\n",
    "        - reactivity_errors: Tensor(dtype=torch.float32) - the reactivity errors\n",
    "        - output_masks: Tensor(dtype=torch.float32) - the elementwise weights to multiply the loss by to properly\n",
    "            account for masked items and reactivity errors\n",
    "        - inputs: tensor(dtype=torch.float32) - the input sequence, specifically of shape (None, NUM_REACTIVITIES)\n",
    "        - bpp: tensor(dtype=torch.float32)\n",
    "        - outputs: tensor(dtype=torch.float32) - the expected reactivities. Note that a simple MAE or MSE loss will not\n",
    "            suffice for training models on this dataset. Please use the output_masks tensor as well.\n",
    "\n",
    "    Parameters:\n",
    "        - out: str - the name of the file to save the arrays to\n",
    "        - file_name: str - the name of the input csv file\n",
    "        - n_proc: int - the number of processes to use while processing data\n",
    "        - map_fn: Callable - the function to apply to all dataset rows\n",
    "        - extra_cols_to_keep: List[str] - the names of any extra columns to keep in the dataset\n",
    "    \"\"\"\n",
    "    if os.path.exists(out):\n",
    "        print(\n",
    "            \"File already exists, not doing any work.\\n\"\n",
    "            + \"To force re-preprocessing, delete the dataset directory and restart the kernel.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    names_to_keep = [\n",
    "        \"reactivity_errors\",\n",
    "        \"bool_output_masks\",\n",
    "        \"output_masks\",\n",
    "        \"inputs\",\n",
    "        \"outputs\",\n",
    "        \"bpp\",\n",
    "    ] + extra_cols_to_keep\n",
    "\n",
    "    # load dataset and map it to our preprocess function\n",
    "    ds = Dataset.from_csv(file_name).map(map_fn, num_proc=n_proc)\n",
    "\n",
    "    # drop excess columns and save to disk\n",
    "    ds.remove_columns(\n",
    "        list(filter(lambda c: c not in names_to_keep, ds.column_names))\n",
    "    ).save_to_disk(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work.\n",
      "To force re-preprocessing, delete the dataset directory and restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\"train_data_2a3_preprocessed\", \"train_data_2a3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work.\n",
      "To force re-preprocessing, delete the dataset directory and restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\"train_data_dms_preprocessed\", \"train_data_dms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work.\n",
      "To force re-preprocessing, delete the dataset directory and restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\n",
    "    \"test_data_preprocessed\",\n",
    "    \"test_sequences.csv\",\n",
    "    map_fn=process_data_test,\n",
    "    extra_cols_to_keep=[\"id_min\", \"id_max\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the desired dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:33.551791Z",
     "start_time": "2023-10-05T17:09:33.535197Z"
    }
   },
   "outputs": [],
   "source": [
    "desired_dataset = \"2a3\"  # either \"2a3\" or \"dms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['inputs', 'bpp', 'outputs', 'output_masks', 'bool_output_masks', 'reactivity_errors'],\n",
       "    num_rows: 210992\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(\n",
    "    f\"train_data_{desired_dataset}_preprocessed\"\n",
    ").with_format(\"torch\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set is len 189892 and val dataset is len 21100\n"
     ]
    }
   ],
   "source": [
    "columns = [\"inputs\", \"outputs\", \"output_masks\", \"bpp\"]\n",
    "split = dataset.train_test_split(test_size=0.1).select_columns(columns)\n",
    "train_dataset = split[\"train\"]\n",
    "val_dataset = split[\"test\"]\n",
    "\n",
    "print(\n",
    "    \"train set is len\", len(train_dataset), \"and val dataset is len\", len(val_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataset preprocessed, we can visualize what the distribution looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.844615Z",
     "start_time": "2023-10-05T17:09:42.843236Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(*args):\n",
    "    \"\"\"\n",
    "    Gets the product of all arguments passed to it\n",
    "    \"\"\"\n",
    "    prod = 1\n",
    "    for item in args:\n",
    "        prod *= item\n",
    "    return prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize:\n",
    "    # select all the reactivities that are valid (that shouldn't be masked)\n",
    "    visualized_items = torch.masked_select(\n",
    "        dataset[\"outputs\"], dataset[\"bool_output_masks\"] == False\n",
    "    ).numpy()\n",
    "\n",
    "    # sanity check that we didn't take all the items\n",
    "    print(\n",
    "        f\"took {visualized_items.shape[0] / multiply(*dataset['outputs'].shape):.5f}% of the data\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.851272Z",
     "start_time": "2023-10-05T17:09:42.849526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not visualizing. Set `visualize` to `True` to visualize data\n"
     ]
    }
   ],
   "source": [
    "if visualize:\n",
    "    seaborn.histplot(visualized_items, binwidth=0.1)\n",
    "else:\n",
    "    print(\"Not visualizing. Set `visualize` to `True` to visualize data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model our distribution, we have two models:\n",
    "- an AttentionModel that uses attention layers\n",
    "- a BaselineModel that we compare against that uses only a couple convolution layers and a Linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we establish a baseline model comprised of Linear and Convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(torch.nn.Module):\n",
    "    def __init__(self, context_window: int = 31, device: str = DEVICE):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.preLayer = torch.nn.Linear(2, 2).to(device)\n",
    "        self.conv_layer = torch.nn.Conv1d(2, 2, context_window, padding=\"same\").to(\n",
    "            device\n",
    "        )\n",
    "        self.conv_layer_b = torch.nn.Conv1d(2, 2, context_window, padding=\"same\").to(\n",
    "            device\n",
    "        )\n",
    "        self.ff = torch.nn.Linear(NUM_REACTIVITIES * 2, NUM_REACTIVITIES).to(device)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.gelu(self.preLayer(x))\n",
    "        x = x.reshape(x.shape[0], -1, x.shape[1])\n",
    "\n",
    "        x = self.gelu(\n",
    "            self.conv_layer(x)\n",
    "            + torch.flip(self.conv_layer_b(torch.flip(x, dims=[2])), dims=[2])\n",
    "        )\n",
    "\n",
    "        return self.relu(self.ff(x.flatten(start_dim=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write our attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerEncoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention: components.Attention,\n",
    "        latent_dim: int,\n",
    "        ff_dim: int,\n",
    "        n_heads: int,\n",
    "        device: str = DEVICE,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(CustomTransformerEncoderLayer, self).__init__()\n",
    "        self.attention = components.MultiHeadDispatch(\n",
    "            dim_model=latent_dim, num_heads=n_heads, attention=attention, **kwargs\n",
    "        ).to(device)\n",
    "        self.layer_norm = torch.nn.LayerNorm(latent_dim).to(device)\n",
    "\n",
    "        self.ff1 = torch.nn.Linear(latent_dim, ff_dim).to(device)\n",
    "        self.ff2 = torch.nn.Linear(ff_dim, latent_dim).to(device)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        # MHA self attention, add, norm\n",
    "        x = self.layer_norm(self.attention(x, att_mask=attention_mask) + x)\n",
    "\n",
    "        # ff, add, norm\n",
    "        x = self.layer_norm(self.gelu(self.ff2(self.gelu(self.ff1(x)))) + x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomTransformerDecoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention: components.Attention,\n",
    "        latent_dim: int,\n",
    "        ff_dim: int,\n",
    "        n_heads: int,\n",
    "        device: str = DEVICE,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(CustomTransformerDecoderLayer, self).__init__()\n",
    "        self.crossattention = components.MultiHeadDispatch(\n",
    "            dim_model=latent_dim, num_heads=n_heads, attention=attention, **kwargs\n",
    "        ).to(device)\n",
    "\n",
    "        self.selfattention = components.MultiHeadDispatch(\n",
    "            dim_model=latent_dim, num_heads=n_heads, attention=attention, **kwargs\n",
    "        ).to(device)\n",
    "        self.layer_norm = torch.nn.LayerNorm(latent_dim).to(device)\n",
    "\n",
    "        self.ff1 = torch.nn.Linear(latent_dim, ff_dim).to(device)\n",
    "        self.ff2 = torch.nn.Linear(ff_dim, latent_dim).to(device)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, ctx: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        # MHA self attention, add norm\n",
    "        x = self.layer_norm(self.selfattention(x) + x)\n",
    "\n",
    "        # MHA cross attention, add, norm\n",
    "        x = self.layer_norm(\n",
    "            self.crossattention(key=ctx, query=ctx, value=x, att_mask=attention_mask)\n",
    "            + x\n",
    "        )\n",
    "\n",
    "        # ff, add, norm\n",
    "        x = self.layer_norm(self.gelu(self.ff2(self.gelu(self.ff1(x)))) + x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomTransformerEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_type: components.Attention,\n",
    "        n_layers: int,\n",
    "        latent_dim: int,\n",
    "        ff_dim: int,\n",
    "        n_heads: int,\n",
    "        device: str = DEVICE,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(CustomTransformerEncoder, self).__init__()\n",
    "        for i in range(n_layers):\n",
    "            self.add_module(\n",
    "                str(i),\n",
    "                CustomTransformerEncoderLayer(\n",
    "                    attention=attention_type,\n",
    "                    latent_dim=latent_dim,\n",
    "                    ff_dim=ff_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    device=device,\n",
    "                    **kwargs\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        for module in self._modules.values():\n",
    "            x = module(x, attention_mask=attention_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomTransformerDecoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_type: components.Attention,\n",
    "        n_layers: int,\n",
    "        latent_dim: int,\n",
    "        ff_dim: int,\n",
    "        n_heads: int,\n",
    "        device: str = DEVICE,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(CustomTransformerDecoder, self).__init__()\n",
    "        for i in range(n_layers):\n",
    "            self.add_module(\n",
    "                str(i),\n",
    "                CustomTransformerDecoderLayer(\n",
    "                    attention=attention_type,\n",
    "                    latent_dim=latent_dim,\n",
    "                    ff_dim=ff_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    device=device,\n",
    "                    **kwargs\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, ctx: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        for module in self._modules.values():\n",
    "            x = module(x, ctx, attention_mask=attention_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_type: attentions.Attention = attentions.ScaledDotProduct(dropout=0.1),\n",
    "        latent_dim: int = 128,\n",
    "        ff_dim: int = 1024,\n",
    "        n_heads: int = 2,\n",
    "        enc_layers: int = 1,\n",
    "        dec_layers: int = 1,\n",
    "        device: str = DEVICE,\n",
    "    ) -> None:\n",
    "        super(AttentionModel, self).__init__()\n",
    "\n",
    "        # data\n",
    "        self.n_heads = n_heads\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # projection layer\n",
    "        self.proj = torch.nn.Linear(2, latent_dim).to(device)\n",
    "\n",
    "        # positional embedding encoder/decoder layers\n",
    "        self.pos_embedding = embeddings.SinePositionalEmbedding(latent_dim).to(device)\n",
    "        self.has_encoder = enc_layers >= 1\n",
    "        self.has_decoder = dec_layers >= 1\n",
    "        if self.has_encoder:\n",
    "            self.encoder_layers = CustomTransformerEncoder(\n",
    "                latent_dim=latent_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                n_heads=n_heads,\n",
    "                device=device,\n",
    "                attention_type=attention_type,\n",
    "                n_layers=enc_layers,\n",
    "            )\n",
    "        if self.has_decoder:\n",
    "            self.decoder_layers = CustomTransformerDecoder(\n",
    "                latent_dim=latent_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                n_heads=n_heads,\n",
    "                attention_type=attention_type,\n",
    "                n_layers=dec_layers,\n",
    "            )\n",
    "\n",
    "        # output head\n",
    "        self.head = torch.nn.Linear(latent_dim, 1).to(device)\n",
    "        self.final_result = torch.nn.Linear(NUM_REACTIVITIES, NUM_REACTIVITIES).to(\n",
    "            device\n",
    "        )\n",
    "\n",
    "        # activations\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.gelu = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mask = att_utils.maybe_merge_masks(\n",
    "            att_mask=None,\n",
    "            key_padding_mask=(x != 0).any(dim=-1),\n",
    "            batch_size=x.shape[0],\n",
    "            num_heads=self.n_heads,\n",
    "            src_len=x.shape[1],\n",
    "        )\n",
    "\n",
    "        # project to latent dimension\n",
    "        x = self.proj(x)\n",
    "\n",
    "        # embed and then perform attention\n",
    "        x = self.pos_embedding(x)\n",
    "        if self.has_decoder and self.has_encoder:\n",
    "            x = self.decoder_layers(\n",
    "                x, ctx=self.encoder_layers(x, attention_mask=mask), attention_mask=mask\n",
    "            )\n",
    "        elif self.has_encoder:\n",
    "            x = self.encoder_layers(x, attention_mask=mask)\n",
    "        elif self.has_decoder:\n",
    "            x = self.decoder_layers(x, ctx=x, attention_mask=mask)\n",
    "\n",
    "        # final result\n",
    "        x = self.relu(self.final_result(self.gelu(self.head(x).flatten(start_dim=1))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_baseline = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dms_kwargs = dict(\n",
    "    latent_dim=32,\n",
    "    n_heads=1,\n",
    "    enc_layers=4,\n",
    "    dec_layers=4,\n",
    "    ff_dim=2048,\n",
    ")\n",
    "model_2a3_kwargs = dict(\n",
    "    latent_dim=32,\n",
    "    n_heads=1,\n",
    "    enc_layers=4,\n",
    "    dec_layers=4,\n",
    "    ff_dim=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the model\n",
    "if use_baseline:\n",
    "    model = BaselineModel()\n",
    "elif desired_dataset == \"dms\":\n",
    "    model = AttentionModel(**model_dms_kwargs)\n",
    "elif desired_dataset == \"2a3\":\n",
    "    model = AttentionModel(**model_2a3_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded previous weights\n"
     ]
    }
   ],
   "source": [
    "# load old weights if possible\n",
    "if os.path.exists(f\"{desired_dataset}_model\"):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(f\"{desired_dataset}_model\"))\n",
    "        print(\"loaded previous weights\")\n",
    "    except Exception as e:\n",
    "        print(\"not loading previous weights because\", e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 2.9502e-02, 3.2924e-02, 0.0000e+00, 3.1256e-02,\n",
       "         2.7086e-02, 1.0857e-02, 2.4872e-02, 3.2272e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1683e-02, 3.2852e-02,\n",
       "         8.5152e-03, 2.9365e-02, 2.4005e-02, 2.1168e-02, 1.8684e-02, 3.7977e-02,\n",
       "         3.7581e-02, 0.0000e+00, 3.0670e-01, 8.0938e-02, 1.2839e-01, 1.8455e-01,\n",
       "         7.5870e-02, 2.2543e-01, 2.3599e-01, 2.4518e-01, 2.7627e-01, 2.6089e-01,\n",
       "         2.1842e-01, 2.2787e-01, 2.6565e-01, 2.3789e-01, 2.3019e-01, 2.5626e-01,\n",
       "         2.6424e-01, 2.9859e-01, 2.8677e-01, 2.7021e-01, 3.0774e-01, 2.9375e-01,\n",
       "         2.8281e-01, 3.2400e-01, 2.3871e-01, 2.8888e-01, 2.5705e-01, 2.7922e-01,\n",
       "         2.6251e-01, 2.6378e-01, 2.8091e-01, 2.4998e-01, 2.3434e-01, 1.7966e-01,\n",
       "         1.7853e-01, 2.0429e-01, 2.2204e-01, 2.8498e-01, 2.7079e-01, 2.4806e-01,\n",
       "         2.6214e-01, 2.5215e-01, 2.6187e-01, 2.8171e-01, 2.5798e-01, 2.7527e-01,\n",
       "         3.1098e-01, 2.3426e-01, 1.7541e-01, 2.4524e-01, 1.8791e-01, 2.8977e-01,\n",
       "         2.9460e-01, 3.4833e-01, 3.1110e-01, 2.9235e-01, 2.9679e-01, 2.8340e-01,\n",
       "         2.8001e-01, 1.5584e-01, 2.4432e-01, 2.4152e-01, 2.2608e-01, 3.1061e-01,\n",
       "         2.3124e-01, 2.3866e-01, 1.9757e-01, 2.6906e-01, 1.7255e-01, 1.4784e-01,\n",
       "         2.0760e-01, 1.8871e-01, 2.1016e-01, 1.2645e-01, 1.9883e-01, 1.9207e-01,\n",
       "         1.5817e-01, 2.0787e-01, 2.1518e-01, 2.4099e-01, 2.0982e-01, 1.4778e-01,\n",
       "         1.8428e-01, 2.7484e-01, 1.6264e-01, 2.4103e-01, 1.6135e-01, 9.8420e-02,\n",
       "         2.0457e-01, 2.4168e-01, 2.2854e-01, 2.6172e-01, 2.1078e-01, 2.6249e-01,\n",
       "         2.5094e-01, 2.0033e-01, 1.8780e-01, 1.4992e-01, 2.1473e-01, 1.9655e-01,\n",
       "         3.0704e-01, 2.7412e-01, 3.0340e-01, 2.5604e-01, 2.4087e-01, 1.8353e-01,\n",
       "         2.4791e-01, 4.0394e-01, 6.1711e-01, 0.0000e+00, 2.3937e-02, 2.1552e-02,\n",
       "         6.7184e-02, 0.0000e+00, 3.1408e-02, 3.8549e-02, 0.0000e+00, 8.5996e-03,\n",
       "         7.3587e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4277e-02, 1.9109e-03,\n",
       "         5.7652e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8371e-02, 1.1011e-01,\n",
       "         2.3546e-01, 6.7698e-02, 2.6592e-02, 1.0389e-02, 2.5181e-01, 0.0000e+00,\n",
       "         4.1336e-02, 8.6761e-02, 0.0000e+00, 0.0000e+00, 3.5520e-02, 6.5526e-03,\n",
       "         2.2315e-02, 1.5293e-02, 7.5384e-03, 1.4262e-02, 0.0000e+00, 2.5168e-02,\n",
       "         2.8065e-02, 7.7048e-03, 3.4325e-02, 0.0000e+00, 4.2667e-02, 0.0000e+00,\n",
       "         4.0989e-02, 0.0000e+00, 3.8735e-02, 3.9360e-02, 0.0000e+00, 2.4749e-02,\n",
       "         0.0000e+00, 1.0532e-02, 6.2588e-03, 3.4347e-02, 0.0000e+00, 3.0300e-02,\n",
       "         1.4673e-02, 2.8690e-02, 0.0000e+00, 3.2648e-02, 1.8305e-02, 0.0000e+00,\n",
       "         2.5968e-02, 0.0000e+00, 2.5549e-02, 3.3987e-02, 1.3234e-04, 1.8606e-02,\n",
       "         2.2340e-02, 2.4473e-02, 0.0000e+00, 3.3838e-03, 5.3351e-03, 2.4539e-02,\n",
       "         7.8270e-03, 0.0000e+00, 0.0000e+00, 1.5910e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3217e-02,\n",
       "         0.0000e+00, 2.0951e-02, 0.0000e+00, 2.8530e-03, 0.0000e+00, 3.1408e-02,\n",
       "         1.8677e-02, 0.0000e+00, 7.3195e-03, 6.0637e-03, 6.4815e-03, 1.4088e-02,\n",
       "         2.6674e-03, 0.0000e+00, 0.0000e+00, 2.8267e-02, 0.0000e+00, 1.9672e-02,\n",
       "         0.0000e+00, 0.0000e+00, 7.3376e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6519e-02, 1.8280e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7956e-02, 3.3999e-03,\n",
       "         0.0000e+00, 3.1921e-02, 0.0000e+00, 0.0000e+00, 1.5267e-02, 0.0000e+00,\n",
       "         0.0000e+00, 1.6290e-02, 2.7217e-03, 0.0000e+00, 2.0051e-02, 3.0145e-02,\n",
       "         0.0000e+00, 1.5528e-02, 9.2178e-03, 3.5108e-02, 1.9511e-02, 0.0000e+00,\n",
       "         0.0000e+00, 2.0546e-02, 0.0000e+00, 0.0000e+00, 7.5868e-03, 1.1082e-03,\n",
       "         0.0000e+00, 0.0000e+00, 2.9402e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         3.1529e-03, 7.6161e-03, 2.1884e-02, 0.0000e+00, 1.3715e-02, 0.0000e+00,\n",
       "         0.0000e+00, 2.7177e-02, 0.0000e+00, 9.0265e-03, 2.6195e-02, 0.0000e+00,\n",
       "         3.5576e-02, 5.3580e-04, 2.4123e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         8.9412e-03, 0.0000e+00, 8.3482e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.4584e-03, 2.9574e-02, 1.3639e-02, 0.0000e+00,\n",
       "         1.4750e-02, 2.2132e-02, 7.5201e-03, 1.8964e-02, 0.0000e+00, 0.0000e+00,\n",
       "         8.6478e-03, 2.1297e-02, 1.3354e-02, 1.6167e-02, 3.1430e-02, 2.7105e-02,\n",
       "         3.6914e-02, 8.7214e-03, 5.7389e-03, 0.0000e+00, 0.0000e+00, 1.5780e-02,\n",
       "         1.3731e-03, 1.8161e-02, 2.0412e-02, 2.1221e-02, 9.4690e-03, 0.0000e+00,\n",
       "         0.0000e+00, 3.5903e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.9057e-02,\n",
       "         0.0000e+00, 0.0000e+00, 2.0986e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         3.2220e-02, 2.1643e-02, 0.0000e+00, 3.2703e-02, 7.4433e-03, 0.0000e+00,\n",
       "         1.8202e-02, 4.6135e-03, 6.3972e-04, 0.0000e+00, 0.0000e+00, 3.2458e-03,\n",
       "         0.0000e+00, 0.0000e+00, 6.9075e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.2908e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8198e-02, 3.2158e-02,\n",
       "         0.0000e+00, 7.6906e-03, 5.1279e-04, 8.7586e-03, 2.6129e-02, 1.6657e-02,\n",
       "         1.8375e-03, 1.1348e-03, 1.5133e-02, 2.1677e-02, 9.5801e-03, 0.0000e+00,\n",
       "         3.8465e-02, 0.0000e+00, 0.0000e+00, 3.1459e-02, 4.0023e-02, 1.8417e-03,\n",
       "         1.4974e-03, 0.0000e+00, 0.0000e+00, 3.2919e-02, 1.6541e-02, 1.1504e-02,\n",
       "         6.1246e-03, 3.3034e-02, 3.5568e-02, 0.0000e+00, 1.4567e-02, 0.0000e+00,\n",
       "         0.0000e+00, 2.2720e-02, 1.1413e-02, 3.0746e-02, 0.0000e+00, 1.2017e-02,\n",
       "         0.0000e+00, 6.7467e-03, 4.2521e-02, 2.5362e-02, 2.1843e-02, 1.5802e-02,\n",
       "         1.4183e-02, 2.6964e-02, 0.0000e+00, 1.9072e-02, 1.8582e-02, 2.5660e-02,\n",
       "         0.0000e+00, 2.7973e-02, 2.8203e-02, 0.0000e+00, 2.8400e-02, 1.0973e-02,\n",
       "         1.3402e-02, 1.7752e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4967e-02,\n",
       "         2.0985e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.7252e-02, 0.0000e+00, 4.9517e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 3.2170e-03, 3.4971e-02, 6.2276e-03, 2.7183e-02,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 2.7255e-02, 2.7728e-02, 0.0000e+00, 2.7668e-02,\n",
       "         2.3364e-02, 1.2548e-02, 2.5637e-02, 3.1828e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9634e-02, 2.9382e-02,\n",
       "         1.4448e-02, 2.2633e-02, 2.8953e-02, 1.8798e-02, 2.5137e-02, 4.2446e-02,\n",
       "         3.6156e-02, 0.0000e+00, 1.4112e-01, 1.5287e-01, 1.6993e-01, 2.1822e-01,\n",
       "         1.6570e-01, 2.6286e-01, 2.5712e-01, 2.4541e-01, 2.6485e-01, 3.0659e-01,\n",
       "         2.4828e-01, 2.2404e-01, 2.2924e-01, 2.1983e-01, 2.2434e-01, 2.5411e-01,\n",
       "         2.5834e-01, 3.1049e-01, 3.1897e-01, 3.3678e-01, 3.4215e-01, 3.2262e-01,\n",
       "         3.1194e-01, 3.4237e-01, 2.7787e-01, 1.2787e-01, 2.1356e-01, 2.1248e-01,\n",
       "         1.7897e-01, 2.4220e-01, 2.6472e-01, 2.4909e-01, 1.7829e-01, 1.1402e-01,\n",
       "         1.4204e-01, 1.8696e-01, 2.0135e-01, 2.4615e-01, 2.2484e-01, 1.8521e-01,\n",
       "         2.4857e-01, 2.6494e-01, 2.6724e-01, 2.3980e-01, 2.2817e-01, 2.5688e-01,\n",
       "         2.3587e-01, 1.3298e-01, 1.5649e-01, 1.1895e-01, 1.7227e-01, 3.0098e-01,\n",
       "         3.1040e-01, 2.9772e-01, 2.2316e-01, 2.3926e-01, 2.4315e-01, 1.9338e-01,\n",
       "         1.9672e-01, 4.1527e-02, 4.2902e-02, 2.3933e-01, 1.5562e-01, 1.3996e-01,\n",
       "         1.8355e-01, 2.2665e-01, 2.2621e-01, 2.6643e-01, 1.5711e-01, 1.5702e-01,\n",
       "         2.3390e-01, 2.1807e-01, 2.0062e-01, 1.9134e-01, 2.5838e-01, 2.4118e-01,\n",
       "         2.6543e-01, 2.4021e-01, 2.8171e-01, 2.8969e-01, 2.9407e-01, 2.4510e-01,\n",
       "         2.2551e-01, 2.8405e-01, 2.0201e-01, 2.8045e-01, 2.2893e-01, 3.1313e-01,\n",
       "         2.2546e-01, 2.5987e-01, 2.5063e-01, 2.7301e-01, 1.3545e-01, 2.6040e-01,\n",
       "         2.6966e-01, 2.3441e-01, 1.8717e-01, 1.2318e-01, 2.0481e-01, 2.9327e-01,\n",
       "         4.0730e-01, 2.8250e-01, 3.6693e-01, 3.0161e-01, 2.3739e-01, 1.7083e-01,\n",
       "         1.9051e-01, 4.1366e-01, 6.0608e-01, 0.0000e+00, 3.1910e-02, 2.5041e-02,\n",
       "         6.9127e-02, 0.0000e+00, 3.4731e-02, 4.8032e-02, 0.0000e+00, 9.3719e-03,\n",
       "         1.2663e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.3670e-02, 1.2253e-02,\n",
       "         7.0574e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0530e-02, 1.4860e-01,\n",
       "         2.4498e-01, 7.9523e-02, 3.1093e-02, 1.6172e-02, 2.7431e-01, 0.0000e+00,\n",
       "         7.6312e-02, 9.4568e-02, 0.0000e+00, 0.0000e+00, 3.2896e-02, 6.4849e-03,\n",
       "         2.3202e-02, 2.0652e-02, 7.7685e-03, 8.8708e-03, 0.0000e+00, 2.5422e-02,\n",
       "         2.8227e-02, 7.1489e-03, 3.4303e-02, 0.0000e+00, 4.5655e-02, 0.0000e+00,\n",
       "         3.9190e-02, 0.0000e+00, 3.7018e-02, 3.3659e-02, 0.0000e+00, 2.0919e-02,\n",
       "         0.0000e+00, 1.2094e-02, 1.1746e-02, 3.4700e-02, 0.0000e+00, 2.8101e-02,\n",
       "         1.3834e-02, 2.6633e-02, 0.0000e+00, 3.4048e-02, 1.6392e-02, 0.0000e+00,\n",
       "         3.4214e-02, 0.0000e+00, 2.6772e-02, 3.2066e-02, 1.9563e-03, 1.4755e-02,\n",
       "         1.9936e-02, 2.3514e-02, 0.0000e+00, 0.0000e+00, 7.0461e-03, 3.1562e-02,\n",
       "         4.5627e-03, 0.0000e+00, 0.0000e+00, 1.6694e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4536e-02,\n",
       "         0.0000e+00, 2.4473e-02, 0.0000e+00, 2.1569e-03, 0.0000e+00, 3.2551e-02,\n",
       "         2.1195e-02, 0.0000e+00, 6.1397e-03, 1.0762e-02, 1.0999e-02, 1.1493e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6751e-02, 0.0000e+00, 2.2223e-02,\n",
       "         0.0000e+00, 0.0000e+00, 7.5380e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0606e-02, 1.9877e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2828e-02, 6.7656e-03,\n",
       "         0.0000e+00, 2.7512e-02, 0.0000e+00, 0.0000e+00, 1.3168e-02, 0.0000e+00,\n",
       "         0.0000e+00, 2.2294e-02, 3.2884e-03, 0.0000e+00, 2.2187e-02, 3.3519e-02,\n",
       "         0.0000e+00, 1.6624e-02, 3.3419e-03, 3.2802e-02, 1.9467e-02, 0.0000e+00,\n",
       "         0.0000e+00, 1.7080e-02, 0.0000e+00, 0.0000e+00, 1.9303e-03, 2.4459e-03,\n",
       "         0.0000e+00, 0.0000e+00, 2.4497e-02, 0.0000e+00, 0.0000e+00, 2.5561e-03,\n",
       "         3.9147e-03, 8.2065e-03, 2.2009e-02, 0.0000e+00, 1.4469e-02, 0.0000e+00,\n",
       "         0.0000e+00, 2.2719e-02, 0.0000e+00, 5.3495e-03, 2.6327e-02, 0.0000e+00,\n",
       "         3.1804e-02, 1.5365e-03, 2.5324e-02, 0.0000e+00, 1.3877e-03, 0.0000e+00,\n",
       "         1.0389e-02, 0.0000e+00, 3.5273e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.7837e-03, 1.1034e-04, 3.3531e-02, 1.2515e-02, 0.0000e+00,\n",
       "         1.3589e-02, 1.7190e-02, 1.1783e-02, 1.8215e-02, 0.0000e+00, 0.0000e+00,\n",
       "         1.5910e-02, 2.8182e-02, 1.4841e-02, 1.8043e-02, 3.1449e-02, 2.1255e-02,\n",
       "         3.5063e-02, 1.1497e-02, 1.8210e-03, 0.0000e+00, 0.0000e+00, 1.4180e-02,\n",
       "         3.8175e-03, 1.5270e-02, 2.1685e-02, 2.2293e-02, 9.3606e-03, 0.0000e+00,\n",
       "         0.0000e+00, 3.9206e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1273e-02,\n",
       "         0.0000e+00, 0.0000e+00, 1.8021e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         3.3060e-02, 2.3591e-02, 0.0000e+00, 3.1949e-02, 1.5632e-03, 0.0000e+00,\n",
       "         1.7369e-02, 7.6969e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1073e-03,\n",
       "         0.0000e+00, 1.5457e-03, 1.1869e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         9.7725e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0489e-02, 3.1048e-02,\n",
       "         0.0000e+00, 1.1315e-02, 1.2558e-03, 6.0194e-03, 2.3356e-02, 1.1436e-02,\n",
       "         1.2628e-03, 3.8006e-03, 1.1876e-02, 2.0898e-02, 1.3271e-02, 0.0000e+00,\n",
       "         3.7409e-02, 0.0000e+00, 0.0000e+00, 3.0690e-02, 3.5415e-02, 4.2778e-03,\n",
       "         6.9121e-03, 0.0000e+00, 0.0000e+00, 3.1217e-02, 9.3876e-03, 1.0574e-02,\n",
       "         1.1387e-02, 3.2482e-02, 3.6557e-02, 0.0000e+00, 1.4482e-02, 0.0000e+00,\n",
       "         0.0000e+00, 2.3800e-02, 1.1899e-02, 3.2006e-02, 0.0000e+00, 7.0410e-03,\n",
       "         0.0000e+00, 3.1299e-03, 4.3273e-02, 2.6409e-02, 1.3337e-02, 8.1348e-03,\n",
       "         1.9249e-02, 3.1633e-02, 0.0000e+00, 2.0331e-02, 2.5265e-02, 2.4210e-02,\n",
       "         0.0000e+00, 3.0722e-02, 2.8175e-02, 0.0000e+00, 3.0060e-02, 1.5844e-02,\n",
       "         1.1746e-02, 1.8458e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9985e-02,\n",
       "         2.4722e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.8888e-02, 0.0000e+00, 3.7461e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 8.1929e-03, 3.9865e-02, 8.5463e-03, 2.1050e-02,\n",
       "         0.0000e+00]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure that calling the model works as expected\n",
    "inp = torch.zeros((2, NUM_REACTIVITIES, 2))\n",
    "inp[:, 0, :] = 1\n",
    "\n",
    "model(inp.to(DEVICE)).cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionModel(\n",
      "  (proj): Linear(in_features=2, out_features=32, bias=True)\n",
      "  (pos_embedding): SinePositionalEmbedding()\n",
      "  (encoder_layers): CustomTransformerEncoder(\n",
      "    (0): CustomTransformerEncoderLayer(\n",
      "      (attention): MultiHeadDispatch(\n",
      "        (attention): ScaledDotProduct(\n",
      "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (in_proj_container): InputProjection(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=2048, bias=True)\n",
      "      (ff2): Linear(in_features=2048, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "    (1): CustomTransformerEncoderLayer(\n",
      "      (attention): MultiHeadDispatch(\n",
      "        (attention): ScaledDotProduct(\n",
      "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (in_proj_container): InputProjection(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=2048, bias=True)\n",
      "      (ff2): Linear(in_features=2048, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "    (2): CustomTransformerEncoderLayer(\n",
      "      (attention): MultiHeadDispatch(\n",
      "        (attention): ScaledDotProduct(\n",
      "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (in_proj_container): InputProjection(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=2048, bias=True)\n",
      "      (ff2): Linear(in_features=2048, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "    (3): CustomTransformerEncoderLayer(\n",
      "      (attention): MultiHeadDispatch(\n",
      "        (attention): ScaledDotProduct(\n",
      "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (in_proj_container): InputProjection(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=2048, bias=True)\n",
      "      (ff2): Linear(in_features=2048, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (decoder_layers): CustomTransformerDecoder(\n",
      "    (0): CustomTransformerDecoderLayer(\n",
      "      (crossattention): MultiHeadDispatch(\n",
      "        (attention): ScaledDotProduct(\n",
      "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (in_proj_container): InputProjection(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (selfattention): MultiHeadDispatch(\n",
      "        (attention): ScaledDotProduct(\n",
      "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (in_proj_container): InputProjection(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=2048, bias=True)\n",
      "      (ff2): Linear(in_features=2048, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "    (1): CustomTransformerDecoderLayer(\n",
      "      (crossattention): MultiHeadDispatch(\n",
      "        (attention): ScaledDotProduct(\n",
      "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (in_proj_container): InputProjection(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (selfattention): MultiHeadDispatch(\n",
      "        (attention): ScaledDotProduct(\n",
      "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (in_proj_container): InputProjection(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=2048, bias=True)\n",
      "      (ff2): Linear(in_features=2048, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "    (2): CustomTransformerDecoderLayer(\n",
      "      (crossattention): MultiHeadDispatch(\n",
      "        (attention): ScaledDotProduct(\n",
      "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (in_proj_container): InputProjection(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (selfattention): MultiHeadDispatch(\n",
      "        (attention): ScaledDotProduct(\n",
      "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (in_proj_container): InputProjection(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=2048, bias=True)\n",
      "      (ff2): Linear(in_features=2048, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "    (3): CustomTransformerDecoderLayer(\n",
      "      (crossattention): MultiHeadDispatch(\n",
      "        (attention): ScaledDotProduct(\n",
      "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (in_proj_container): InputProjection(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (selfattention): MultiHeadDispatch(\n",
      "        (attention): ScaledDotProduct(\n",
      "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (in_proj_container): InputProjection(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=2048, bias=True)\n",
      "      (ff2): Linear(in_features=2048, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (head): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (final_result): Linear(in_features=457, out_features=457, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 1325851\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"Total params:\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 1e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"2a3_SN_4enc_4dec\"\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE = True\n",
    "\n",
    "# create dataloaders\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE\n",
    ")\n",
    "# no point in shuffling validation set\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unweightedL1(\n",
    "    y_pred: torch.Tensor,\n",
    "    y_true: torch.Tensor,\n",
    "    weights: torch.Tensor,\n",
    "    l1=torch.nn.L1Loss(reduction=\"none\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    MAE Loss function where sample weights are only used to determine masks.\n",
    "    \"\"\"\n",
    "    return (l1(y_pred, y_true))[weights != 0].mean()\n",
    "\n",
    "\n",
    "def weightedL1(\n",
    "    y_pred: torch.Tensor,\n",
    "    y_true: torch.Tensor,\n",
    "    weights: torch.Tensor,\n",
    "    l1=torch.nn.L1Loss(reduction=\"none\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    MAE loss function that takes into account sample weights\n",
    "    \"\"\"\n",
    "    return (l1(y_pred, y_true) * weights)[weights != 0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:43.992618Z",
     "start_time": "2023-10-05T17:09:43.053099Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_batch(\n",
    "    m: torch.nn.Module, inps: torch.Tensor, outs: torch.Tensor, masks: torch.Tensor\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the loss on a batch and perform the corresponding weight updates.\n",
    "    Used for training purposes\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    preds = m(inps)\n",
    "\n",
    "    # get the weighted mae\n",
    "    weighted_loss = weightedL1(preds, outs, masks)\n",
    "    weighted_loss.backward()\n",
    "\n",
    "    # calculate gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        unweighted_loss = unweightedL1(preds, outs, masks)\n",
    "\n",
    "    # return weighted and unweighted mae loss\n",
    "    return weighted_loss.detach().cpu(), unweighted_loss.detach().cpu()\n",
    "\n",
    "\n",
    "def noupdate_batch(\n",
    "    m: torch.nn.Module, inps: torch.Tensor, outs: torch.Tensor, masks: torch.Tensor\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the loss on a batch without performing any updates.\n",
    "    Used for validation purposes\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        preds = m(inps)\n",
    "        weighted_loss = weightedL1(preds, outs, masks)\n",
    "        unweighted_loss = unweightedL1(preds, outs, masks)\n",
    "\n",
    "    # return weighted and unweighted mae loss\n",
    "    return weighted_loss.cpu(), unweighted_loss.cpu()\n",
    "\n",
    "\n",
    "def masked_train(\n",
    "    m: torch.nn.Module,\n",
    "    train_dataloader: data.DataLoader,\n",
    "    val_dataloader: data.DataLoader,\n",
    "    epochs: int = 1,\n",
    "    device: torch.device = DEVICE,\n",
    "    log: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the given model.\n",
    "\n",
    "    Arguments:\n",
    "        - m: torch.nn.Module - the model to train.\n",
    "        - train_dataloader: data.Dataloader - the dataloader that provides the batched training data\n",
    "        - val_dataloader: data.Dataloader - the dataloader that provides the batched validation data\n",
    "        - epochs: int - how many epochs to train for. Defaults to `1`.\n",
    "        - device: torch.device - the device to train on, defaults to `DEVICE`\n",
    "        - log: bool - whether or not to log to `writer`\n",
    "    \"\"\"\n",
    "    m = m.to(device)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        epoch_mae = 0.0\n",
    "        epoch_weighted_mae = 0.0\n",
    "\n",
    "        m = m.train()\n",
    "        for tdata in (prog := tqdm(train_dataloader, desc=\"batch\")):\n",
    "            inps = torch.stack([tdata[\"inputs\"], tdata[\"bpp\"]], dim=-1)\n",
    "            outs = tdata[\"outputs\"]\n",
    "            masks = tdata[\"output_masks\"]\n",
    "\n",
    "            inps = inps.to(device)\n",
    "            outs = outs.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            weighted_mae, mae = train_batch(m, inps, outs, masks)\n",
    "\n",
    "            epoch_weighted_mae += weighted_mae\n",
    "            epoch_mae += mae\n",
    "\n",
    "            # log\n",
    "            prog.set_postfix_str(\n",
    "                f\"mae_loss: {mae:.5f}, weighted_mae_loss: {weighted_mae:.5f}\"\n",
    "            )\n",
    "\n",
    "            # break  # used for sanity check\n",
    "        epoch_weighted_mae /= len(train_dataloader)\n",
    "        epoch_mae /= len(train_dataloader)\n",
    "\n",
    "        # do validation\n",
    "        val_mae = 0.0\n",
    "        val_weighted_mae = 0.0\n",
    "        m = m.eval()\n",
    "        for vdata in val_dataloader:\n",
    "            inps = torch.stack([vdata[\"inputs\"], vdata[\"bpp\"]], dim=-1)\n",
    "            outs = vdata[\"outputs\"]\n",
    "            masks = vdata[\"output_masks\"]\n",
    "\n",
    "            inps = inps.to(device)\n",
    "            outs = outs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            weighted_mae, mae = noupdate_batch(m, inps, outs, masks)\n",
    "\n",
    "            val_weighted_mae += weighted_mae\n",
    "            val_mae += mae\n",
    "        val_weighted_mae /= len(val_dataloader)\n",
    "        val_mae /= len(val_dataloader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch MAE: {epoch_mae:.5f}\\tEpoch Weighted MAE: {epoch_weighted_mae:.5f}\\t\"\n",
    "            + f\"Val MAE: {val_mae:.5f}\\tVal Weighted MAE: {val_weighted_mae:.5f}\"\n",
    "        )\n",
    "\n",
    "        if log:\n",
    "            writer.add_scalar(\"epoch_mae\", epoch_mae, global_step=epoch)\n",
    "            writer.add_scalar(\"val_mae\", val_mae, global_step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.128074Z",
     "start_time": "2023-10-05T17:09:43.998369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 2968/2968 [08:44<00:00,  5.66it/s, mae_loss: 0.21019, weighted_mae_loss: 0.18523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch MAE: 0.19372\tEpoch Weighted MAE: 0.15266\tVal MAE: 0.19210\tVal Weighted MAE: 0.15071\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 2968/2968 [08:47<00:00,  5.63it/s, mae_loss: 0.23693, weighted_mae_loss: 0.15563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch MAE: 0.19339\tEpoch Weighted MAE: 0.15238\tVal MAE: 0.19188\tVal Weighted MAE: 0.15054\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 2968/2968 [08:51<00:00,  5.58it/s, mae_loss: 0.11292, weighted_mae_loss: 0.10283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch MAE: 0.19312\tEpoch Weighted MAE: 0.15218\tVal MAE: 0.19175\tVal Weighted MAE: 0.15044\n"
     ]
    }
   ],
   "source": [
    "# baseline gets ~ 3.02 on dms w/ 10 epochs, ~ 3.87 on 2a3 w/ 20 epochs\n",
    "masked_train(\n",
    "    model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    epochs=3,\n",
    "    log=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(m: torch.nn.Module, t0: Dataset, device: torch.device = DEVICE):\n",
    "    \"\"\"\n",
    "    Label T with predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def predict(row):\n",
    "        with torch.no_grad():\n",
    "            row[\"outputs\"] = m(\n",
    "                torch.stack([row[\"inputs\"], row[\"bpp\"]], dim=-1).to(device)\n",
    "            ).cpu()\n",
    "        return row\n",
    "\n",
    "    t0 = t0.map(\n",
    "        predict, batched=True, batch_size=BATCH_SIZE, load_from_cache_file=False\n",
    "    )\n",
    "    return t0\n",
    "\n",
    "\n",
    "def train_pseudo(\n",
    "    m: torch.nn.Module,\n",
    "    t0: Dataset,\n",
    "    epochs: int = 1,\n",
    "    device: torch.device = DEVICE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Make pseudo-labeled data and train on that pseudo-labeled data\n",
    "\n",
    "    Arguments:\n",
    "        - m: torch.nn.Module - the model to train\n",
    "        - t0: Dataset - the dataset containing `inputs` and `bpp` on which to pseudo-label and train on\n",
    "        - epochs: int - how many epochs to do on the pseudo-labeled data\n",
    "        - device: torch.device - the device to train on, defaults to `DEVICE`\n",
    "    \"\"\"\n",
    "    m = m.train().to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "\n",
    "        t0 = label(m, t0, device)\n",
    "        dataloader = data.DataLoader(t0, batch_size=BATCH_SIZE)\n",
    "\n",
    "        epoch_weighted_mae = 0\n",
    "        epoch_mae = 0\n",
    "\n",
    "        for pdata in (prog := tqdm(dataloader)):\n",
    "            inps = torch.stack([pdata[\"inputs\"], pdata[\"bpp\"]], dim=-1).to(device)\n",
    "            outs = pdata[\"outputs\"].to(device)\n",
    "\n",
    "            weights = torch.where(inps[:, :, 0] != 0, 1, 0).to(device)\n",
    "            weighted_mae, mae = train_batch(m, inps, outs, weights)\n",
    "\n",
    "            epoch_weighted_mae += weighted_mae\n",
    "            epoch_mae += mae\n",
    "\n",
    "            # log\n",
    "            prog.set_postfix_str(\n",
    "                f\"mae_loss: {mae:.5f}, weighted_mae_loss: {weighted_mae:.5f}\"\n",
    "            )\n",
    "\n",
    "        epoch_weighted_mae /= len(dataloader)\n",
    "        epoch_mae /= len(dataloader)\n",
    "\n",
    "        print(\n",
    "            f\"Pseudo Epoch MAE: {epoch_mae:.5f}\\tEpoch Weighted MAE: {epoch_weighted_mae:.5f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_train(\n",
    "    m: torch.nn.Module,\n",
    "    train_dataloader: data.DataLoader,\n",
    "    val_dataloader: data.DataLoader,\n",
    "    t0: str = \"test_data_preprocessed\",\n",
    "    epochs: int = 1,\n",
    "    s_epochs: int = 1,\n",
    "    p_epochs: int = 1,\n",
    "    device: torch.device = DEVICE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model via Semi Supervised Learning.\n",
    "\n",
    "    The process is to train it on the T0 (the unlabeled data) for `pseudo_epochs` epochs\n",
    "    before training it on real data for `real_epochs` epochs.\n",
    "\n",
    "    Arguments:\n",
    "        - m: torch.nn.Module - the model to train.\n",
    "        - train_dataloader: data.Dataloader - the dataloader that provides the batched training data\n",
    "        - val_dataloader: data.Dataloader - the dataloader that provides the batched validation data\n",
    "        - t0: str - the dataset that contains T0 (unlabeled data). Defaults to `test_data_preprocessed`\n",
    "        - epochs: int - how many epochs to train for. Defaults to `1`. Note that one epoch involves\n",
    "            training on BOTH real and pseudo datasets\n",
    "        - s_epochs: int - how many sub-epochs to do on the train dataset (the supervised dataset). Defaults to `1`.\n",
    "        - p_epochs: int - how many sub-epochs to do on the pseudo-labeled data. Defaults to `1`.\n",
    "        - device: torch.device - the device to train on, defaults to `DEVICE`\n",
    "    \"\"\"\n",
    "\n",
    "    t0 = Dataset.load_from_disk(t0).with_format(\"torch\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"=\" * 64)\n",
    "        print(f\"Macro Epoch {epoch}/{epochs}\")\n",
    "        print(\"=\" * 32)\n",
    "        print(f\"Pseudo-labeled Epochs\")\n",
    "\n",
    "        # do pseudo training first\n",
    "        train_pseudo(m, t0, p_epochs, device)\n",
    "        print(\"=\" * 32)\n",
    "        print(f\"Supervised Epochs\")\n",
    "\n",
    "        # do real training\n",
    "        masked_train(m, train_dataloader, val_dataloader, s_epochs, device, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "Epoch 0 Pseudo\n",
      "Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223f8ee73c74425b88e455f510ff1dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1343823 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20998/20998 [1:02:38<00:00,  5.59it/s, mae_loss: 0.00612, weighted_mae_loss: 0.00612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo Epoch MAE: 0.01669\tEpoch Weighted MAE: 0.01669\n",
      "================================================================\n",
      "Epoch 0 Real\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:  53%|█████▎    | 1566/2968 [06:18<04:31,  5.16it/s, mae_loss: 0.18101, weighted_mae_loss: 0.14370]"
     ]
    }
   ],
   "source": [
    "ssl_train(model, train_dataloader, val_dataloader, epochs=20, s_epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained our model, we can save its weights and biases (\"state dict\") so that we can load them for later inferencing or further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"{desired_dataset}_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have both models, it's time to create a submission file. \n",
    "This section creates a zipped csv submission file that can\n",
    "be submitted on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.624146Z",
     "start_time": "2023-10-05T17:14:00.601389Z"
    }
   },
   "outputs": [],
   "source": [
    "make_submissions = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:01.233216Z",
     "start_time": "2023-10-05T17:14:00.608006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not going to create submissions.\n"
     ]
    }
   ],
   "source": [
    "valid = False\n",
    "\n",
    "if (\n",
    "    os.path.exists(\"2a3_model\")\n",
    "    and os.path.exists(\"dms_model\")\n",
    "    and os.path.exists(\"test_sequences.csv\")\n",
    "    and make_submissions\n",
    "):\n",
    "    if use_baseline:\n",
    "        model_dms = BaselineModel()\n",
    "        model_2a3 = BaselineModel()\n",
    "    else:\n",
    "        model_2a3 = AttentionModel(**model_2a3_kwargs)\n",
    "        model_dms = AttentionModel(**model_dms_kwargs)\n",
    "\n",
    "    model_2a3.load_state_dict(torch.load(\"2a3_model\"))\n",
    "    model_dms.load_state_dict(torch.load(\"dms_model\"))\n",
    "\n",
    "    model_2a3.eval().to(DEVICE)\n",
    "    model_dms.eval().to(DEVICE)\n",
    "\n",
    "    valid = True\n",
    "else:\n",
    "    print(\"Not going to create submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "    model_2a3: torch.nn.Module,\n",
    "    model_dms: torch.nn.Module,\n",
    "    input_ds: str,\n",
    "    out: str,\n",
    "    batch_size: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Make predictions on the test dataset and write them to a csv file\n",
    "\n",
    "    Parameters:\n",
    "        - model_2a3: torch.nn.Module - the model trained on the 2a3 distribution\n",
    "        - model_dms: torch.nn.Module - the model trained on the dms distribution\n",
    "        - input_ds: str - name of the dataset to load\n",
    "        - out: str - name of the file to write to\n",
    "        - batch_size: int - size of the batches to use to process the data.\n",
    "            In general, larger batch sizes mean faster runtime\n",
    "    \"\"\"\n",
    "    ds = Dataset.load_from_disk(input_ds).with_format(\"torch\")\n",
    "    loader = data.DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    iterable = iter(loader)\n",
    "\n",
    "    with open(out, \"w\") as outfile:\n",
    "        # write the header\n",
    "        outfile.write(\"id,reactivity_DMS_MaP,reactivity_2A3_MaP\\n\")\n",
    "\n",
    "        for _ in tqdm(range(len(loader))):\n",
    "            # get the next group of data\n",
    "            tdata = next(iterable)\n",
    "            inputs = torch.stack([tdata[\"inputs\"], tdata[\"bpp\"]], dim=-1).to(DEVICE)\n",
    "            min_ids = tdata[\"id_min\"].numpy()\n",
    "            max_ids = tdata[\"id_max\"].numpy()\n",
    "\n",
    "            # make predictions w/o gradients\n",
    "            with torch.no_grad():\n",
    "                preds_2a3 = model_2a3(inputs).cpu().numpy()\n",
    "                preds_dms = model_dms(inputs).cpu().numpy()\n",
    "\n",
    "            # write preds\n",
    "            for i in range(inputs.shape[0]):\n",
    "                outfile.writelines(\n",
    "                    map(\n",
    "                        lambda seq_idx: f\"{seq_idx},{preds_dms[i, seq_idx-min_ids[i]]:.3f},{preds_2a3[i, seq_idx-min_ids[i]]:.3f}\\n\",\n",
    "                        # +1 since the id_max is inclusive\n",
    "                        range(min_ids[i], max_ids[i] + 1),\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-05T17:14:01.248308Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not going to create submissions.\n"
     ]
    }
   ],
   "source": [
    "if valid:\n",
    "    pipeline(\n",
    "        model_2a3,\n",
    "        model_dms,\n",
    "        \"test_data_preprocessed\",\n",
    "        \"submission.csv\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "else:\n",
    "    print(\"Not going to create submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not going to zip submissions.\n"
     ]
    }
   ],
   "source": [
    "if valid:\n",
    "    # zip our submission into an easily-uploadable zip file\n",
    "    print(\"zipping submissions. This may take a while...\")\n",
    "    os.system(\"zip submission.csv.zip submission.csv\")\n",
    "    print(\"Done zipping submissions!\")\n",
    "else:\n",
    "    print(\"Not going to zip submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
