{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ribonanza - Attempt 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second approach to the [Stanford Ribonanza problem](https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/) that builds off the first and second approaches.\n",
    "\n",
    "Major differences:\n",
    "- use of pytorch instead of tensorflow\n",
    "- use of attention model architecture\n",
    "- use bpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the attention architecture scores 0.19377"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- experiment with other types of attention\n",
    "- use full transformer model (with decoder) (not just encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filesystem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your project directory should look like this:\n",
    "\n",
    "- `(project directory)`\n",
    "    - `ribonanza2.ipynb`\n",
    "    - `train_data.csv`\n",
    "    - `test_data.csv` (optional)\n",
    "\n",
    "`train_data.csv` is the only file necessary for training, and it can be downloaded from the kaggle competition linked in the description.\n",
    "\n",
    "`test_data.csv` is only necessary if you intend to make and submit predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Seetup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to install pip packages:\n",
    "```sh\n",
    "pip install torch numpy seaborn xformers arnie datasets\n",
    "```\n",
    "Need to install conda packages for eternafold:\n",
    "```sh\n",
    "conda install -c conda-forge \"libgcc-ng>=12\" \"libstdcxx-ng>=12\"\n",
    "conda install -c bioconda eternafold\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cuda as cudabackends\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "# for visualization\n",
    "import seaborn\n",
    "\n",
    "# typing hints\n",
    "from typing import List\n",
    "from collections.abc import Callable\n",
    "\n",
    "# used for better attention mechanisms\n",
    "import xformers.components.positional_embedding as embeddings\n",
    "import xformers.ops as xops\n",
    "import xformers.components.attention as attentions\n",
    "import xformers.components.attention.utils as att_utils\n",
    "import xformers.components as components\n",
    "\n",
    "# used for bpps\n",
    "from arnie.bpps import bpps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "# according to kaggle, this is the maximum # of reactivites to be used\n",
    "NUM_REACTIVITIES = 457\n",
    "\n",
    "# there are 4 different bases (AUCG)\n",
    "NUM_BASES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"  # if no gpu available, use cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(out: str, key: str, value: str, file_name: str, force: bool):\n",
    "    \"\"\"\n",
    "    Filters a file to only take datapoints\n",
    "    whose values of `key` are `value`.\n",
    "\n",
    "    Parameters:\n",
    "        - out: str - the name of the file that will store the filtered datapoints\n",
    "        - key: str - the name of the key to look at\n",
    "        - value: str - the value that the key should have\n",
    "        - file_name: str - the name of the file that contains all the datapoints.\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    if os.path.exists(out) and not force:\n",
    "        print(\"File already exists, not doing any work\")\n",
    "        return\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    # count how many lines we have in total\n",
    "    with open(file_name) as file:\n",
    "        line = file.readline()  # ignore the header\n",
    "        line = (\n",
    "            file.readline()\n",
    "        )  # take the first line since we increment count in the loop\n",
    "        while line != \"\":\n",
    "            count += 1\n",
    "            line = file.readline()\n",
    "\n",
    "    # use that knowledge for a progress bar\n",
    "    with open(file_name, \"r\") as file, open(out, \"w\") as outfile:\n",
    "        # write the header\n",
    "        header = file.readline()\n",
    "        outfile.write(header)\n",
    "\n",
    "        # get what index the SN_filter is\n",
    "        SN_idx = header.split(\",\").index(key)\n",
    "\n",
    "        # only take the approved filtered lines\n",
    "        for _ in tqdm(range(count)):\n",
    "            line = file.readline()\n",
    "            temp = line.split(\",\")\n",
    "            if temp[SN_idx] == value:\n",
    "                outfile.write(line)\n",
    "\n",
    "\n",
    "def filter_train_data(force: bool = False):\n",
    "    \"\"\"\n",
    "    Filters the immense train_data.csv to only take datapoints\n",
    "    whose SN_filter (Signal to Noise filter) is 1. In other words,\n",
    "    we only take good reads. These filtered datapoints are then\n",
    "    written to the file provided\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\"train_data_filtered.csv\", \"SN_filter\", \"1\", \"train_data.csv\", force)\n",
    "\n",
    "\n",
    "def filter_2A3(force: bool = False):\n",
    "    \"\"\"\n",
    "    Only take the 2A3 points\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\n",
    "        \"train_data_2a3.csv\",\n",
    "        \"experiment_type\",\n",
    "        \"2A3_MaP\",\n",
    "        \"train_data_filtered.csv\",\n",
    "        force,\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_DMS(force: bool = False):\n",
    "    \"\"\"\n",
    "    Only take the DMS points\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\n",
    "        \"train_data_dms.csv\",\n",
    "        \"experiment_type\",\n",
    "        \"DMS_MaP\",\n",
    "        \"train_data_filtered.csv\",\n",
    "        force,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# filter our data\n",
    "filter_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# take the 2a3 points\n",
    "filter_2A3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# take the dms points\n",
    "filter_DMS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode inputs as\n",
    "# A : 1\n",
    "# U : 2\n",
    "# C : 3\n",
    "# G : 4\n",
    "base_map = {\n",
    "    \"A\": 1,\n",
    "    \"U\": 2,\n",
    "    \"C\": 3,\n",
    "    \"G\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(row):\n",
    "    \"\"\"\n",
    "    Convert a row containing all csv columns in the original dataset\n",
    "    to a row containing only the columns:\n",
    "    - inputs\n",
    "    - outputs\n",
    "    - bpp\n",
    "    - output_masks\n",
    "    - reactivity_error\n",
    "    - bool_output_masks\n",
    "    \"\"\"\n",
    "    # initialize arrays\n",
    "    # note that we assume everything is masked until told otherwise\n",
    "    inputs = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "    bpp = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "    output_masks = np.ones((NUM_REACTIVITIES,), dtype=np.bool_)\n",
    "    reactivity_errors = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "    reactivities = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "\n",
    "    seq_len = len(row[\"sequence\"])\n",
    "\n",
    "    # encode the bases\n",
    "    inputs[:seq_len] = np.array(\n",
    "        list(map(lambda letter: base_map[letter], row[\"sequence\"]))\n",
    "    )\n",
    "\n",
    "    # get the probability that any of those bases are paired\n",
    "    bpp[:seq_len] = np.max(bpps(row[\"sequence\"], package=\"eternafold\"), axis=-1)\n",
    "\n",
    "    # get the reactivities and their errors\n",
    "    reactivities[:seq_len] = np.array(\n",
    "        list(\n",
    "            map(\n",
    "                lambda seq_idx: np.float32(\n",
    "                    row[\"reactivity_\" + str(seq_idx + 1).rjust(4, \"0\")]\n",
    "                ),\n",
    "                range(seq_len),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    reactivity_errors[:seq_len] = np.array(\n",
    "        list(\n",
    "            map(\n",
    "                lambda seq_idx: np.float32(\n",
    "                    row[\"reactivity_error_\" + str(seq_idx + 1).rjust(4, \"0\")]\n",
    "                ),\n",
    "                range(seq_len),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # replace reactivity error nans with 0s (assume no error)\n",
    "    reactivity_errors = np.where(np.isnan(reactivity_errors), 0.0, reactivity_errors)\n",
    "\n",
    "    # get where all the reactivities are nan\n",
    "    nan_locats = np.isnan(reactivities)\n",
    "\n",
    "    # where it is nan, store True, else False\n",
    "    output_masks[:seq_len] = nan_locats[:seq_len]\n",
    "\n",
    "    # where it is not nan, store the reactivity and error, else 0\n",
    "    reactivities[:seq_len] = np.where(\n",
    "        nan_locats[:seq_len] == False, reactivities[:seq_len], 0.0\n",
    "    )\n",
    "    reactivity_errors[:seq_len] = np.where(\n",
    "        nan_locats[:seq_len] == False, reactivity_errors[:seq_len], 0.0\n",
    "    )\n",
    "\n",
    "    # store the values\n",
    "    row = {}\n",
    "    row[\"inputs\"] = inputs\n",
    "    row[\"bpp\"] = bpp\n",
    "    row[\"outputs\"] = np.clip(reactivities, 0, 1)\n",
    "    row[\"output_masks\"] = np.clip(\n",
    "        np.where(output_masks, 0.0, 1.0) - np.abs(reactivity_errors), 0, 1\n",
    "    )\n",
    "    row[\"bool_output_masks\"] = output_masks\n",
    "    row[\"reactivity_errors\"] = np.abs(reactivity_errors)\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def process_data_test(row):\n",
    "    \"\"\"\n",
    "    Almost the same as process_data, except it only takes inputs and bpp\n",
    "    \"\"\"\n",
    "    # initialize arrays\n",
    "    # note that we assume everything is masked until told otherwise\n",
    "    inputs = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "    bpp = np.zeros((NUM_REACTIVITIES,), dtype=np.float32)\n",
    "\n",
    "    seq_len = len(row[\"sequence\"])\n",
    "\n",
    "    # encode the bases\n",
    "    inputs[:seq_len] = np.array(\n",
    "        list(map(lambda letter: base_map[letter], row[\"sequence\"]))\n",
    "    )\n",
    "\n",
    "    # get the probability that any of those bases are paired\n",
    "    bpp[:seq_len] = np.max(bpps(row[\"sequence\"], package=\"eternafold\"), axis=-1)\n",
    "\n",
    "    row[\"inputs\"] = inputs\n",
    "    row[\"bpp\"] = bpp\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_csv(\n",
    "    out: str,\n",
    "    file_name: str,\n",
    "    n_proc: int = 12,\n",
    "    map_fn: Callable = process_data,\n",
    "    extra_cols_to_keep: List[str] = [],\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess the csv and save the preprocessed data as a dataset\n",
    "    that can be loaded via datasets.Dataset.load_from_file\n",
    "\n",
    "    The dataset contains the following items:\n",
    "        - bool_output_masks: Tensor(dtype=torch.bool) - the output masks.\n",
    "            If True, then that item should NOT be used to calculate loss.\n",
    "            If False, then that item should be used to calculate loss\n",
    "        - reactivity_errors: Tensor(dtype=torch.float32) - the reactivity errors\n",
    "        - output_masks: Tensor(dtype=torch.float32) - the elementwise weights to multiply the loss by to properly\n",
    "            account for masked items and reactivity errors\n",
    "        - inputs: tensor(dtype=torch.float32) - the input sequence, specifically of shape (None, NUM_REACTIVITIES)\n",
    "        - bpp: tensor(dtype=torch.float32)\n",
    "        - outputs: tensor(dtype=torch.float32) - the expected reactivities. Note that a simple MAE or MSE loss will not\n",
    "            suffice for training models on this dataset. Please use the output_masks tensor as well.\n",
    "\n",
    "    Parameters:\n",
    "        - out: str - the name of the file to save the arrays to\n",
    "        - file_name: str - the name of the input csv file\n",
    "        - n_proc: int - the number of processes to use while processing data\n",
    "        - map_fn: Callable - the function to apply to all dataset rows\n",
    "        - extra_cols_to_keep: List[str] - the names of any extra columns to keep in the dataset\n",
    "    \"\"\"\n",
    "    if os.path.exists(out):\n",
    "        print(\n",
    "            \"File already exists, not doing any work.\\n\"\n",
    "            + \"To force re-preprocessing, delete the dataset directory and restart the kernel.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    names_to_keep = [\n",
    "        \"reactivity_errors\",\n",
    "        \"bool_output_masks\",\n",
    "        \"output_masks\",\n",
    "        \"inputs\",\n",
    "        \"outputs\",\n",
    "        \"bpp\",\n",
    "    ] + extra_cols_to_keep\n",
    "\n",
    "    # load dataset and map it to our preprocess function\n",
    "    ds = Dataset.from_csv(file_name).map(map_fn, num_proc=n_proc)\n",
    "\n",
    "    # drop excess columns and save to disk\n",
    "    ds.remove_columns(\n",
    "        list(filter(lambda c: c not in names_to_keep, ds.column_names))\n",
    "    ).save_to_disk(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work.\n",
      "To force re-preprocessing, delete the dataset directory and restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\"train_data_2a3_preprocessed\", \"train_data_2a3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work.\n",
      "To force re-preprocessing, delete the dataset directory and restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\"train_data_dms_preprocessed\", \"train_data_dms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work.\n",
      "To force re-preprocessing, delete the dataset directory and restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\n",
    "    \"test_data_preprocessed\",\n",
    "    \"test_sequences.csv\",\n",
    "    map_fn=process_data_test,\n",
    "    extra_cols_to_keep=[\"id_min\", \"id_max\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the desired dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:33.551791Z",
     "start_time": "2023-10-05T17:09:33.535197Z"
    }
   },
   "outputs": [],
   "source": [
    "desired_dataset = \"dms\"  # either \"2a3\" or \"dms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['inputs', 'bpp', 'outputs', 'output_masks', 'bool_output_masks', 'reactivity_errors'],\n",
       "    num_rows: 226925\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(\n",
    "    f\"train_data_{desired_dataset}_preprocessed\"\n",
    ").with_format(\"torch\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set is len 204232 and val dataset is len 22693\n"
     ]
    }
   ],
   "source": [
    "columns = [\"inputs\", \"outputs\", \"output_masks\", \"bpp\"]\n",
    "split = dataset.train_test_split(test_size=0.1).select_columns(columns)\n",
    "train_dataset = split[\"train\"]\n",
    "val_dataset = split[\"test\"]\n",
    "\n",
    "print(\n",
    "    \"train set is len\", len(train_dataset), \"and val dataset is len\", len(val_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataset preprocessed, we can visualize what the distribution looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.844615Z",
     "start_time": "2023-10-05T17:09:42.843236Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(*args):\n",
    "    \"\"\"\n",
    "    Gets the product of all arguments passed to it\n",
    "    \"\"\"\n",
    "    prod = 1\n",
    "    for item in args:\n",
    "        prod *= item\n",
    "    return prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize:\n",
    "    # select all the reactivities that are valid (that shouldn't be masked)\n",
    "    visualized_items = torch.masked_select(\n",
    "        dataset[\"outputs\"], dataset[\"bool_output_masks\"] == False\n",
    "    ).numpy()\n",
    "\n",
    "    # sanity check that we didn't take all the items\n",
    "    print(\n",
    "        f\"took {visualized_items.shape[0] / multiply(*dataset['outputs'].shape):.5f}% of the data\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.851272Z",
     "start_time": "2023-10-05T17:09:42.849526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not visualizing. Set `visualize` to `True` to visualize data\n"
     ]
    }
   ],
   "source": [
    "if visualize:\n",
    "    seaborn.histplot(visualized_items, binwidth=0.1)\n",
    "else:\n",
    "    print(\"Not visualizing. Set `visualize` to `True` to visualize data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model our distribution, we have two models:\n",
    "- an AttentionModel that uses attention layers\n",
    "- a BaselineModel that we compare against that uses only a couple convolution layers and a Linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we establish a baseline model comprised of Linear and Convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(torch.nn.Module):\n",
    "    def __init__(self, context_window: int = 31, device: str = DEVICE):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.preLayer = torch.nn.Linear(2, 2).to(device)\n",
    "        self.conv_layer = torch.nn.Conv1d(2, 2, context_window, padding=\"same\").to(\n",
    "            device\n",
    "        )\n",
    "        self.conv_layer_b = torch.nn.Conv1d(2, 2, context_window, padding=\"same\").to(\n",
    "            device\n",
    "        )\n",
    "        self.ff = torch.nn.Linear(NUM_REACTIVITIES * 2, NUM_REACTIVITIES).to(device)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.gelu(self.preLayer(x))\n",
    "        x = x.reshape(x.shape[0], -1, x.shape[1])\n",
    "\n",
    "        x = self.gelu(\n",
    "            self.conv_layer(x)\n",
    "            + torch.flip(self.conv_layer_b(torch.flip(x, dims=[2])), dims=[2])\n",
    "        )\n",
    "\n",
    "        return self.relu(self.ff(x.flatten(start_dim=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write our attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerEncoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention: components.Attention,\n",
    "        latent_dim: int,\n",
    "        ff_dim: int,\n",
    "        n_heads: int,\n",
    "        device: str = DEVICE,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(CustomTransformerEncoderLayer, self).__init__()\n",
    "        self.attention = components.MultiHeadDispatch(\n",
    "            dim_model=latent_dim, num_heads=n_heads, attention=attention, **kwargs\n",
    "        ).to(device)\n",
    "        self.layer_norm = torch.nn.LayerNorm(latent_dim).to(device)\n",
    "\n",
    "        self.ff1 = torch.nn.Linear(latent_dim, ff_dim).to(device)\n",
    "        self.ff2 = torch.nn.Linear(ff_dim, latent_dim).to(device)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        # MHA self attention, add, norm\n",
    "        x = self.layer_norm(self.attention(x, att_mask=attention_mask) + x)\n",
    "\n",
    "        # ff, add, norm\n",
    "        x = self.layer_norm(self.gelu(self.ff2(self.gelu(self.ff1(x)))) + x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomTransformerDecoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention: components.Attention,\n",
    "        latent_dim: int,\n",
    "        ff_dim: int,\n",
    "        n_heads: int,\n",
    "        device: str = DEVICE,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(CustomTransformerDecoderLayer, self).__init__()\n",
    "        self.crossattention = components.MultiHeadDispatch(\n",
    "            dim_model=latent_dim, num_heads=n_heads, attention=attention, **kwargs\n",
    "        ).to(device)\n",
    "\n",
    "        self.selfattention = components.MultiHeadDispatch(\n",
    "            dim_model=latent_dim, num_heads=n_heads, attention=attention, **kwargs\n",
    "        ).to(device)\n",
    "        self.layer_norm = torch.nn.LayerNorm(latent_dim).to(device)\n",
    "\n",
    "        self.ff1 = torch.nn.Linear(latent_dim, ff_dim).to(device)\n",
    "        self.ff2 = torch.nn.Linear(ff_dim, latent_dim).to(device)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, ctx: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        # MHA self attention, add norm\n",
    "        x = self.layer_norm(self.selfattention(x) + x)\n",
    "\n",
    "        # MHA cross attention, add, norm\n",
    "        x = self.layer_norm(\n",
    "            self.crossattention(key=ctx, query=ctx, value=x, att_mask=attention_mask)\n",
    "            + x\n",
    "        )\n",
    "\n",
    "        # ff, add, norm\n",
    "        x = self.layer_norm(self.gelu(self.ff2(self.gelu(self.ff1(x)))) + x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomTransformerEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_type: components.Attention,\n",
    "        n_layers: int,\n",
    "        latent_dim: int,\n",
    "        ff_dim: int,\n",
    "        n_heads: int,\n",
    "        device: str = DEVICE,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(CustomTransformerEncoder, self).__init__()\n",
    "        for i in range(n_layers):\n",
    "            self.add_module(\n",
    "                str(i),\n",
    "                CustomTransformerEncoderLayer(\n",
    "                    attention=attention_type,\n",
    "                    latent_dim=latent_dim,\n",
    "                    ff_dim=ff_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    device=device,\n",
    "                    **kwargs\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        for module in self._modules.values():\n",
    "            x = module(x, attention_mask=attention_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomTransformerDecoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_type: components.Attention,\n",
    "        n_layers: int,\n",
    "        latent_dim: int,\n",
    "        ff_dim: int,\n",
    "        n_heads: int,\n",
    "        device: str = DEVICE,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(CustomTransformerDecoder, self).__init__()\n",
    "        for i in range(n_layers):\n",
    "            self.add_module(\n",
    "                str(i),\n",
    "                CustomTransformerDecoderLayer(\n",
    "                    attention=attention_type,\n",
    "                    latent_dim=latent_dim,\n",
    "                    ff_dim=ff_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    device=device,\n",
    "                    **kwargs\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, ctx: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        for module in self._modules.values():\n",
    "            x = module(x, ctx, attention_mask=attention_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_type: attentions.Attention = attentions.ScaledDotProduct(dropout=0.1),\n",
    "        latent_dim: int = 128,\n",
    "        ff_dim: int = 1024,\n",
    "        n_heads: int = 2,\n",
    "        enc_layers: int = 1,\n",
    "        dec_layers: int = 1,\n",
    "        device: str = DEVICE,\n",
    "    ) -> None:\n",
    "        super(AttentionModel, self).__init__()\n",
    "\n",
    "        # data\n",
    "        self.n_heads = n_heads\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # projection layer\n",
    "        self.proj = torch.nn.Linear(2, latent_dim).to(device)\n",
    "\n",
    "        # positional embedding encoder/decoder layers\n",
    "        self.pos_embedding = embeddings.SinePositionalEmbedding(latent_dim).to(device)\n",
    "        self.has_encoder = enc_layers >= 1\n",
    "        self.has_decoder = dec_layers >= 1\n",
    "        if self.has_encoder:\n",
    "            self.encoder_layers = CustomTransformerEncoder(\n",
    "                latent_dim=latent_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                n_heads=n_heads,\n",
    "                device=device,\n",
    "                attention_type=attention_type,\n",
    "                n_layers=enc_layers,\n",
    "            )\n",
    "        if self.has_decoder:\n",
    "            self.decoder_layers = CustomTransformerDecoder(\n",
    "                latent_dim=latent_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                n_heads=n_heads,\n",
    "                attention_type=attention_type,\n",
    "                n_layers=dec_layers,\n",
    "            )\n",
    "\n",
    "        # output head\n",
    "        self.head = torch.nn.Linear(latent_dim, 1).to(device)\n",
    "        self.final_result = torch.nn.Linear(NUM_REACTIVITIES, NUM_REACTIVITIES).to(\n",
    "            device\n",
    "        )\n",
    "\n",
    "        # activations\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.gelu = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mask = att_utils.maybe_merge_masks(\n",
    "            att_mask=None,\n",
    "            key_padding_mask=(x != 0).any(dim=-1),\n",
    "            batch_size=x.shape[0],\n",
    "            num_heads=self.n_heads,\n",
    "            src_len=x.shape[1],\n",
    "        )\n",
    "\n",
    "        # project to latent dimension\n",
    "        x = self.proj(x)\n",
    "\n",
    "        # embed and then perform attention\n",
    "        x = self.pos_embedding(x)\n",
    "        if self.has_decoder and self.has_encoder:\n",
    "            x = self.decoder_layers(\n",
    "                x, ctx=self.encoder_layers(x, attention_mask=mask), attention_mask=mask\n",
    "            )\n",
    "        elif self.has_encoder:\n",
    "            x = self.encoder_layers(x, attention_mask=mask)\n",
    "        elif self.has_decoder:\n",
    "            x = self.decoder_layers(x, ctx=x, attention_mask=mask)\n",
    "\n",
    "        # final result\n",
    "        x = self.relu(self.final_result(self.gelu(self.head(x).flatten(start_dim=1))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_baseline = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dms_kwargs = dict(\n",
    "    latent_dim=32,\n",
    "    n_heads=1,\n",
    "    enc_layers=1,\n",
    "    dec_layers=1,\n",
    "    ff_dim=2048,\n",
    ")\n",
    "model_2a3_kwargs = dict(\n",
    "    latent_dim=32,\n",
    "    n_heads=1,\n",
    "    enc_layers=1,\n",
    "    dec_layers=1,\n",
    "    ff_dim=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the model\n",
    "if use_baseline:\n",
    "    model = BaselineModel()\n",
    "elif desired_dataset == \"dms\":\n",
    "    model = AttentionModel(**model_dms_kwargs)\n",
    "elif desired_dataset == \"2a3\":\n",
    "    model = AttentionModel(**model_2a3_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not loading previous weights because Error(s) in loading state_dict for AttentionModel:\n",
      "\tMissing key(s) in state_dict: \"encoder_layers.0.attention.in_proj_container.q_proj.weight\", \"encoder_layers.0.attention.in_proj_container.q_proj.bias\", \"encoder_layers.0.attention.in_proj_container.k_proj.weight\", \"encoder_layers.0.attention.in_proj_container.k_proj.bias\", \"encoder_layers.0.attention.in_proj_container.v_proj.weight\", \"encoder_layers.0.attention.in_proj_container.v_proj.bias\", \"encoder_layers.0.attention.proj.weight\", \"encoder_layers.0.attention.proj.bias\", \"decoder_layers.0.crossattention.in_proj_container.q_proj.weight\", \"decoder_layers.0.crossattention.in_proj_container.q_proj.bias\", \"decoder_layers.0.crossattention.in_proj_container.k_proj.weight\", \"decoder_layers.0.crossattention.in_proj_container.k_proj.bias\", \"decoder_layers.0.crossattention.in_proj_container.v_proj.weight\", \"decoder_layers.0.crossattention.in_proj_container.v_proj.bias\", \"decoder_layers.0.crossattention.proj.weight\", \"decoder_layers.0.crossattention.proj.bias\", \"decoder_layers.0.selfattention.in_proj_container.q_proj.weight\", \"decoder_layers.0.selfattention.in_proj_container.q_proj.bias\", \"decoder_layers.0.selfattention.in_proj_container.k_proj.weight\", \"decoder_layers.0.selfattention.in_proj_container.k_proj.bias\", \"decoder_layers.0.selfattention.in_proj_container.v_proj.weight\", \"decoder_layers.0.selfattention.in_proj_container.v_proj.bias\", \"decoder_layers.0.selfattention.proj.weight\", \"decoder_layers.0.selfattention.proj.bias\", \"decoder_layers.0.layer_norm.weight\", \"decoder_layers.0.layer_norm.bias\", \"decoder_layers.0.ff1.weight\", \"decoder_layers.0.ff1.bias\", \"decoder_layers.0.ff2.weight\", \"decoder_layers.0.ff2.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"encoder_layers.1.attention.mha.in_proj_container.q_proj.weight\", \"encoder_layers.1.attention.mha.in_proj_container.q_proj.bias\", \"encoder_layers.1.attention.mha.in_proj_container.k_proj.weight\", \"encoder_layers.1.attention.mha.in_proj_container.k_proj.bias\", \"encoder_layers.1.attention.mha.in_proj_container.v_proj.weight\", \"encoder_layers.1.attention.mha.in_proj_container.v_proj.bias\", \"encoder_layers.1.attention.mha.proj.weight\", \"encoder_layers.1.attention.mha.proj.bias\", \"encoder_layers.1.layer_norm.weight\", \"encoder_layers.1.layer_norm.bias\", \"encoder_layers.1.ff1.weight\", \"encoder_layers.1.ff1.bias\", \"encoder_layers.1.ff2.weight\", \"encoder_layers.1.ff2.bias\", \"encoder_layers.2.attention.mha.in_proj_container.q_proj.weight\", \"encoder_layers.2.attention.mha.in_proj_container.q_proj.bias\", \"encoder_layers.2.attention.mha.in_proj_container.k_proj.weight\", \"encoder_layers.2.attention.mha.in_proj_container.k_proj.bias\", \"encoder_layers.2.attention.mha.in_proj_container.v_proj.weight\", \"encoder_layers.2.attention.mha.in_proj_container.v_proj.bias\", \"encoder_layers.2.attention.mha.proj.weight\", \"encoder_layers.2.attention.mha.proj.bias\", \"encoder_layers.2.layer_norm.weight\", \"encoder_layers.2.layer_norm.bias\", \"encoder_layers.2.ff1.weight\", \"encoder_layers.2.ff1.bias\", \"encoder_layers.2.ff2.weight\", \"encoder_layers.2.ff2.bias\", \"encoder_layers.3.attention.mha.in_proj_container.q_proj.weight\", \"encoder_layers.3.attention.mha.in_proj_container.q_proj.bias\", \"encoder_layers.3.attention.mha.in_proj_container.k_proj.weight\", \"encoder_layers.3.attention.mha.in_proj_container.k_proj.bias\", \"encoder_layers.3.attention.mha.in_proj_container.v_proj.weight\", \"encoder_layers.3.attention.mha.in_proj_container.v_proj.bias\", \"encoder_layers.3.attention.mha.proj.weight\", \"encoder_layers.3.attention.mha.proj.bias\", \"encoder_layers.3.layer_norm.weight\", \"encoder_layers.3.layer_norm.bias\", \"encoder_layers.3.ff1.weight\", \"encoder_layers.3.ff1.bias\", \"encoder_layers.3.ff2.weight\", \"encoder_layers.3.ff2.bias\", \"encoder_layers.4.attention.mha.in_proj_container.q_proj.weight\", \"encoder_layers.4.attention.mha.in_proj_container.q_proj.bias\", \"encoder_layers.4.attention.mha.in_proj_container.k_proj.weight\", \"encoder_layers.4.attention.mha.in_proj_container.k_proj.bias\", \"encoder_layers.4.attention.mha.in_proj_container.v_proj.weight\", \"encoder_layers.4.attention.mha.in_proj_container.v_proj.bias\", \"encoder_layers.4.attention.mha.proj.weight\", \"encoder_layers.4.attention.mha.proj.bias\", \"encoder_layers.4.layer_norm.weight\", \"encoder_layers.4.layer_norm.bias\", \"encoder_layers.4.ff1.weight\", \"encoder_layers.4.ff1.bias\", \"encoder_layers.4.ff2.weight\", \"encoder_layers.4.ff2.bias\", \"encoder_layers.5.attention.mha.in_proj_container.q_proj.weight\", \"encoder_layers.5.attention.mha.in_proj_container.q_proj.bias\", \"encoder_layers.5.attention.mha.in_proj_container.k_proj.weight\", \"encoder_layers.5.attention.mha.in_proj_container.k_proj.bias\", \"encoder_layers.5.attention.mha.in_proj_container.v_proj.weight\", \"encoder_layers.5.attention.mha.in_proj_container.v_proj.bias\", \"encoder_layers.5.attention.mha.proj.weight\", \"encoder_layers.5.attention.mha.proj.bias\", \"encoder_layers.5.layer_norm.weight\", \"encoder_layers.5.layer_norm.bias\", \"encoder_layers.5.ff1.weight\", \"encoder_layers.5.ff1.bias\", \"encoder_layers.5.ff2.weight\", \"encoder_layers.5.ff2.bias\", \"encoder_layers.0.attention.mha.in_proj_container.q_proj.weight\", \"encoder_layers.0.attention.mha.in_proj_container.q_proj.bias\", \"encoder_layers.0.attention.mha.in_proj_container.k_proj.weight\", \"encoder_layers.0.attention.mha.in_proj_container.k_proj.bias\", \"encoder_layers.0.attention.mha.in_proj_container.v_proj.weight\", \"encoder_layers.0.attention.mha.in_proj_container.v_proj.bias\", \"encoder_layers.0.attention.mha.proj.weight\", \"encoder_layers.0.attention.mha.proj.bias\". \n"
     ]
    }
   ],
   "source": [
    "# load old weights if possible\n",
    "if os.path.exists(f\"{desired_dataset}_model\"):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(f\"{desired_dataset}_model\"))\n",
    "        print(\"loaded previous weights\")\n",
    "    except Exception as e:\n",
    "        print(\"not loading previous weights because\", e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6330, 1.9121, 0.5980, 1.5088, 2.0176, 1.1394, 0.7977, 0.9069, 2.7514,\n",
       "         1.0843, 3.0220, 1.2027, 0.7950, 2.4139, 1.9953, 1.9061, 0.8193, 2.2110,\n",
       "         3.4277, 1.5045, 1.2788, 1.0302, 0.8751, 1.2034, 2.2282, 0.9672, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0695,\n",
       "         0.0986, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.4675, 0.0000, 0.0000,\n",
       "         1.4222, 0.0000, 0.0000, 0.0000, 0.0000, 3.6350, 0.0000, 0.8776, 0.1004,\n",
       "         0.9114, 1.8859, 0.4255, 0.0000, 0.0084, 1.2676, 1.9841, 0.0000, 1.5159,\n",
       "         2.1076, 1.4495, 4.3331, 4.3146, 4.5290, 3.6214, 2.7750, 1.3217, 2.2033,\n",
       "         3.3046, 2.9618, 1.9181, 0.0000, 1.5851, 1.7385, 1.0417, 1.3509, 1.6867,\n",
       "         1.8582, 1.6200, 0.6919, 1.8366, 0.4768, 1.4306, 0.5817, 0.6622, 1.6042,\n",
       "         1.8831, 1.0956, 1.5746, 0.9572, 1.5821, 1.4813, 2.6910, 1.0103, 3.1260,\n",
       "         2.6070, 0.8786, 2.6485, 1.7765, 1.4383, 0.3291, 1.4390, 1.0152, 0.9063,\n",
       "         1.9089, 0.3885, 1.5311, 1.0746, 1.3207, 2.2864, 1.3050, 0.6997, 0.9576,\n",
       "         1.4155, 1.5589, 0.3678, 1.5095, 1.4559, 0.7084, 2.4634, 2.5025, 1.3351,\n",
       "         1.6216, 1.8403, 0.8217, 0.5377, 2.1404, 0.6408, 1.7819, 1.7196, 2.1574,\n",
       "         2.5280, 0.9168, 1.7441, 1.0224, 1.9062, 1.3539, 2.2178, 3.1789, 0.7398,\n",
       "         1.5478, 1.7152, 1.2103, 2.5547, 2.5794, 0.7621, 2.0704, 0.9076, 1.1778,\n",
       "         1.5092, 0.7852, 1.5419, 2.0863, 1.2118, 2.5963, 1.8132, 2.5182, 0.4357,\n",
       "         1.2276, 1.6214, 1.2093, 2.4788, 0.8487, 0.8288, 2.6438, 1.2613, 1.2732,\n",
       "         1.4109, 1.4603, 1.1108, 1.7656, 1.2985, 1.2850, 1.9448, 1.4442, 1.8552,\n",
       "         1.5818, 0.9675, 2.2166, 1.7617, 1.0558, 1.1449, 1.0389, 1.2442, 1.5610,\n",
       "         1.6081, 0.7855, 1.6770, 1.9679, 0.9988, 1.3457, 1.2166, 1.1215, 1.1275,\n",
       "         1.5462, 2.6037, 0.8812, 0.9709, 1.5204, 1.8359, 1.5291, 2.3910, 1.1577,\n",
       "         1.4574, 0.8807, 1.5309, 0.7349, 2.5163, 3.2549, 2.0773, 2.2409, 1.8579,\n",
       "         2.4107, 1.8566, 1.5291, 1.5745, 1.5178, 1.6853, 1.6444, 0.2614, 0.0000,\n",
       "         1.5362, 1.8021, 2.8236, 0.8599, 2.1841, 0.7106, 1.8743, 1.7749, 2.3283,\n",
       "         0.5769, 1.0631, 0.6336, 0.2256, 1.8959, 0.3322, 0.7046, 0.0963, 0.9458,\n",
       "         2.6848, 2.1503, 0.8863, 1.0903, 1.2267, 1.3434, 1.9164, 1.4500, 0.9121,\n",
       "         0.9008, 1.3322, 0.5713, 1.6072, 1.1136, 0.5986, 1.6581, 2.1487, 0.5249,\n",
       "         1.7357, 1.8689, 1.2183, 0.6837, 1.9643, 1.2041, 0.8206, 2.3213, 1.2562,\n",
       "         2.1817, 0.5201, 1.5336, 0.0000, 0.8672, 2.0963, 2.8127, 1.1937, 0.6650,\n",
       "         2.9793, 1.0062, 1.6657, 1.6606, 0.4455, 1.7843, 2.0530, 0.6111, 1.0069,\n",
       "         1.7776, 0.6338, 2.3982, 0.2822, 2.1428, 2.4807, 1.4903, 1.4492, 0.9310,\n",
       "         1.1370, 2.1563, 0.2712, 2.3599, 2.0061, 0.7667, 1.2099, 0.9250, 0.2167,\n",
       "         1.9379, 2.0669, 0.9164, 1.1520, 1.9422, 1.1963, 1.7111, 0.8830, 0.8389,\n",
       "         1.5029, 2.1778, 1.6112, 1.7916, 1.6267, 1.0725, 1.6347, 0.8365, 0.2794,\n",
       "         2.3147, 2.6683, 0.2946, 1.5373, 1.0157, 0.5700, 2.7169, 1.2894, 1.2368,\n",
       "         1.1516, 1.8820, 2.1682, 1.2156, 0.7088, 1.4628, 0.4830, 1.3247, 1.4173,\n",
       "         1.1523, 1.2821, 1.2762, 1.7263, 2.2548, 1.1498, 2.4619, 1.9025, 2.0201,\n",
       "         3.2741, 1.8772, 1.6419, 1.9114, 1.7548, 2.6556, 0.8479, 1.0588, 1.9471,\n",
       "         0.9747, 0.6632, 1.8446, 1.5016, 1.5813, 1.1385, 1.4134, 1.4823, 0.9258,\n",
       "         2.3792, 0.6547, 2.1573, 1.4401, 1.8671, 1.7624, 1.1312, 0.9742, 0.7727,\n",
       "         1.9423, 0.8175, 1.2778, 1.1917, 2.3349, 1.2335, 1.1236, 1.7166, 0.8365,\n",
       "         0.5261, 1.9976, 1.2174, 0.5506, 1.5759, 2.0003, 1.4537, 2.2268, 1.3715,\n",
       "         0.6492, 1.8603, 1.5378, 1.5037, 2.1642, 2.1605, 0.6769],\n",
       "        [1.6702, 1.8871, 0.6110, 1.4946, 1.9968, 1.0785, 0.7790, 0.9224, 2.7585,\n",
       "         1.0730, 3.0218, 1.2274, 0.7466, 2.4444, 1.9843, 1.9124, 0.8367, 2.2152,\n",
       "         3.4389, 1.4379, 1.2238, 1.0001, 0.8906, 1.2100, 2.2248, 0.9210, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.4506,\n",
       "         0.0852, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5985, 0.0000, 0.0000,\n",
       "         1.8254, 0.0000, 0.0000, 0.0205, 0.0000, 3.4590, 0.0920, 0.9325, 0.2823,\n",
       "         1.0481, 2.0328, 0.3595, 0.0000, 0.0000, 1.5472, 2.2314, 0.0000, 1.8237,\n",
       "         2.2587, 1.4206, 4.3906, 4.5562, 4.6670, 3.4898, 3.1061, 1.7087, 2.2975,\n",
       "         3.3714, 3.0655, 1.9470, 0.0000, 1.5853, 1.6991, 1.0336, 1.3488, 1.6741,\n",
       "         1.8991, 1.6075, 0.7118, 1.8116, 0.4211, 1.3832, 0.6187, 0.6720, 1.6358,\n",
       "         1.8890, 1.1367, 1.5751, 0.9420, 1.5743, 1.4987, 2.6694, 1.0219, 3.1649,\n",
       "         2.5940, 0.8809, 2.6267, 1.7969, 1.4239, 0.3423, 1.3417, 1.0053, 0.8988,\n",
       "         1.9323, 0.3776, 1.5085, 1.0442, 1.3178, 2.2787, 1.2687, 0.6919, 0.9441,\n",
       "         1.4086, 1.5610, 0.3485, 1.4823, 1.4717, 0.7117, 2.4316, 2.5820, 1.3455,\n",
       "         1.6277, 1.8404, 0.8393, 0.5511, 2.1715, 0.6507, 1.7912, 1.6911, 2.1808,\n",
       "         2.5474, 0.9060, 1.7015, 1.0284, 1.9132, 1.3556, 2.2523, 3.1760, 0.7172,\n",
       "         1.5350, 1.7460, 1.2208, 2.5559, 2.6058, 0.8191, 2.0634, 0.9299, 1.1455,\n",
       "         1.5014, 0.7936, 1.5000, 2.0748, 1.2069, 2.5655, 1.7988, 2.5290, 0.4213,\n",
       "         1.2815, 1.6104, 1.1997, 2.4831, 0.8811, 0.8175, 2.6227, 1.2758, 1.2848,\n",
       "         1.4369, 1.4399, 1.1327, 1.7423, 1.2872, 1.2982, 1.9849, 1.4259, 1.8567,\n",
       "         1.5702, 0.9914, 2.2785, 1.7814, 1.0976, 1.1310, 1.0356, 1.2324, 1.5721,\n",
       "         1.6244, 0.8091, 1.6999, 2.0079, 1.0021, 1.2909, 1.2600, 1.1506, 1.1524,\n",
       "         1.5658, 2.6536, 0.8480, 0.9225, 1.5390, 1.7867, 1.5550, 2.4142, 1.2029,\n",
       "         1.4754, 0.8719, 1.5422, 0.7848, 2.5085, 3.2612, 2.0560, 2.2528, 1.9129,\n",
       "         2.3643, 1.8987, 1.4942, 1.5786, 1.5461, 1.6906, 1.6171, 0.2965, 0.0000,\n",
       "         1.4694, 1.7535, 2.8469, 0.8802, 2.1697, 0.6991, 1.9031, 1.7863, 2.3509,\n",
       "         0.5694, 1.0496, 0.6488, 0.1991, 1.9071, 0.3105, 0.6958, 0.1149, 0.9788,\n",
       "         2.6937, 2.1921, 0.8605, 1.0911, 1.2396, 1.3763, 1.8753, 1.5026, 0.9090,\n",
       "         0.9068, 1.3696, 0.5757, 1.6290, 1.0802, 0.6065, 1.6900, 2.1645, 0.5667,\n",
       "         1.7169, 1.8510, 1.1976, 0.7343, 1.9661, 1.2206, 0.8064, 2.3595, 1.2785,\n",
       "         2.1766, 0.5695, 1.5536, 0.0000, 0.8890, 2.0647, 2.7816, 1.1621, 0.6606,\n",
       "         2.9983, 1.0139, 1.6926, 1.6987, 0.4131, 1.8309, 2.1033, 0.5553, 0.9915,\n",
       "         1.7733, 0.6186, 2.3609, 0.2892, 2.1410, 2.4700, 1.5354, 1.4473, 0.9322,\n",
       "         1.1433, 2.1490, 0.2759, 2.3541, 1.9673, 0.7708, 1.2146, 0.9152, 0.1975,\n",
       "         1.9508, 2.0773, 0.9503, 1.1734, 1.9331, 1.1741, 1.6810, 0.8404, 0.8714,\n",
       "         1.5225, 2.1924, 1.6277, 1.8327, 1.6646, 1.0718, 1.6222, 0.8718, 0.3060,\n",
       "         2.3323, 2.6374, 0.3257, 1.5365, 1.0131, 0.5689, 2.6867, 1.2508, 1.2492,\n",
       "         1.2172, 1.8734, 2.2585, 1.1909, 0.6930, 1.4980, 0.4666, 1.3829, 1.4062,\n",
       "         1.1140, 1.3150, 1.2888, 1.7438, 2.3014, 1.1539, 2.4686, 1.8966, 1.9851,\n",
       "         3.2774, 1.9127, 1.6214, 1.9427, 1.7175, 2.7166, 0.8701, 1.0593, 1.9136,\n",
       "         0.9720, 0.6686, 1.8573, 1.5027, 1.6193, 1.1628, 1.3703, 1.4759, 0.9802,\n",
       "         2.3760, 0.6520, 2.1296, 1.5176, 1.8983, 1.7365, 1.1189, 1.0070, 0.7536,\n",
       "         1.9454, 0.7618, 1.3232, 1.1941, 2.3258, 1.1864, 1.1054, 1.7226, 0.7924,\n",
       "         0.5344, 1.9560, 1.2417, 0.5652, 1.5592, 1.9781, 1.4374, 2.2244, 1.4112,\n",
       "         0.6439, 1.8864, 1.5526, 1.5002, 2.1653, 2.1471, 0.6739]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure that calling the model works as expected\n",
    "inp = torch.zeros((2, NUM_REACTIVITIES, 2))\n",
    "inp[:, 0, :] = 1\n",
    "\n",
    "model(inp.to(DEVICE)).cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionModel(\n",
      "  (proj): Linear(in_features=2, out_features=32, bias=True)\n",
      "  (pos_embedding): SinePositionalEmbedding()\n",
      "  (encoder_layers): CustomTransformerEncoder(\n",
      "    (0): CustomTransformerEncoderLayer(\n",
      "      (attention): MultiHeadDispatch(\n",
      "        (attention): ScaledDotProduct(\n",
      "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (in_proj_container): InputProjection(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=2048, bias=True)\n",
      "      (ff2): Linear(in_features=2048, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (decoder_layers): CustomTransformerDecoder(\n",
      "    (0): CustomTransformerDecoderLayer(\n",
      "      (crossattention): MultiHeadDispatch(\n",
      "        (attention): ScaledDotProduct(\n",
      "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (in_proj_container): InputProjection(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (selfattention): MultiHeadDispatch(\n",
      "        (attention): ScaledDotProduct(\n",
      "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (in_proj_container): InputProjection(\n",
      "          (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=2048, bias=True)\n",
      "      (ff2): Linear(in_features=2048, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (head): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (final_result): Linear(in_features=457, out_features=457, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 488539\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"Total params:\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 3e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"dms_1enc_1dec_2048ff_32lat_1hd\"\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "SHUFFLE = True\n",
    "\n",
    "# create dataloaders\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE\n",
    ")\n",
    "# no point in shuffling validation set\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedL1(\n",
    "    y_pred: torch.Tensor,\n",
    "    y_true: torch.Tensor,\n",
    "    weights: torch.Tensor,\n",
    "    l1=torch.nn.L1Loss(),\n",
    "):\n",
    "    \"\"\"\n",
    "    This is our custom loss function that takes into account sample weights\n",
    "    \"\"\"\n",
    "    return (l1(y_pred, y_true) * weights).sum(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:43.992618Z",
     "start_time": "2023-10-05T17:09:43.053099Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_batch(\n",
    "    m: torch.nn.Module, inps: torch.Tensor, outs: torch.Tensor, masks: torch.Tensor\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the loss on a batch and perform the corresponding weight updates.\n",
    "    Used for training purposes\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    loss = weightedL1(m(inps), outs, masks)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # calculate gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # return mae loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def noupdate_batch(\n",
    "    m: torch.nn.Module, inps: torch.Tensor, outs: torch.Tensor, masks: torch.Tensor\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the loss on a batch without performing any updates.\n",
    "    Used for validation purposes\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        loss = weightedL1(m(inps), outs, masks)\n",
    "\n",
    "    # return mae loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_train(\n",
    "    m: torch.nn.Module,\n",
    "    train_dataloader: data.DataLoader,\n",
    "    val_dataloader: data.DataLoader,\n",
    "    epochs: int = 1,\n",
    "    device: str = DEVICE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the given model.\n",
    "\n",
    "    Arguments:\n",
    "        - m: torch.nn.Module - the model to train.\n",
    "        - train_dataloader: data.Dataloader - the dataloader that provides the batched training data\n",
    "        - val_dataloader: data.Dataloader - the dataloader that provides the batched validation data\n",
    "        - epochs: int - how many epochs to train for. Defaults to `1`.\n",
    "        - device: str - the device to train on, defaults to `DEVICE`\n",
    "    \"\"\"\n",
    "    m = m.to(device)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        epoch_mae = 0.0\n",
    "\n",
    "        m = m.train()\n",
    "        for batch, tdata in (\n",
    "            prog := tqdm(\n",
    "                enumerate(train_dataloader), total=len(train_dataloader), desc=\"batch\"\n",
    "            )\n",
    "        ):\n",
    "            inps = torch.stack([tdata[\"inputs\"], tdata[\"bpp\"]], dim=-1)\n",
    "            outs = tdata[\"outputs\"]\n",
    "            masks = tdata[\"output_masks\"]\n",
    "\n",
    "            inps = inps.to(device)\n",
    "            outs = outs.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            mae_loss = train_batch(m, inps, outs, masks).detach().cpu()\n",
    "\n",
    "            epoch_mae += mae_loss\n",
    "\n",
    "            # log\n",
    "            prog.set_postfix_str(f\"mae_loss: {mae_loss:.5f}\")\n",
    "\n",
    "            # break  # used for sanity check\n",
    "        epoch_mae /= batch + 1\n",
    "\n",
    "        # do validation\n",
    "        val_mae = 0.0\n",
    "        m = m.eval()\n",
    "        for batch, vdata in enumerate(val_dataloader):\n",
    "            inps = torch.stack([vdata[\"inputs\"], vdata[\"bpp\"]], dim=-1)\n",
    "            outs = vdata[\"outputs\"]\n",
    "            masks = vdata[\"output_masks\"]\n",
    "\n",
    "            inps = inps.to(device)\n",
    "            outs = outs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            mae_loss = noupdate_batch(m, inps, outs, masks)\n",
    "\n",
    "            val_mae += mae_loss\n",
    "        val_mae /= len(val_dataloader)\n",
    "\n",
    "        print(f\"Epoch MAE: {epoch_mae:.5f}\\tVal MAE: {val_mae:.5f}\")\n",
    "\n",
    "        writer.add_scalar(\"epoch_mae\", epoch_mae, global_step=epoch)\n",
    "        writer.add_scalar(\"val_mae\", val_mae, global_step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.128074Z",
     "start_time": "2023-10-05T17:09:43.998369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|| 798/798 [02:27<00:00,  5.41it/s, mae_loss: 3.44902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch MAE: 3.82544\tVal MAE: 3.46834\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|| 798/798 [02:25<00:00,  5.47it/s, mae_loss: 3.29652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch MAE: 3.41755\tVal MAE: 3.38124\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|| 798/798 [02:27<00:00,  5.43it/s, mae_loss: 3.06638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch MAE: 3.12350\tVal MAE: 2.90814\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|| 798/798 [02:25<00:00,  5.49it/s, mae_loss: 2.85471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch MAE: 2.87327\tVal MAE: 2.83744\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|| 798/798 [02:26<00:00,  5.46it/s, mae_loss: 2.82655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch MAE: 2.81041\tVal MAE: 2.78267\n"
     ]
    }
   ],
   "source": [
    "# baseline gets ~ 3.02 on dms w/ 10 epochs, ~ 3.87 on 2a3 w/ 20 epochs\n",
    "masked_train(\n",
    "    model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained our model, we can save its weights and biases (\"state dict\") so that we can load them for later inferencing or further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.616528Z",
     "start_time": "2023-10-05T17:14:00.119617Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"{desired_dataset}_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have both models, it's time to create a submission file. \n",
    "This section creates a zipped csv submission file that can\n",
    "be submitted on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.624146Z",
     "start_time": "2023-10-05T17:14:00.601389Z"
    }
   },
   "outputs": [],
   "source": [
    "make_submissions = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:01.233216Z",
     "start_time": "2023-10-05T17:14:00.608006Z"
    }
   },
   "outputs": [],
   "source": [
    "valid = False\n",
    "\n",
    "if (\n",
    "    os.path.exists(\"2a3_model\")\n",
    "    and os.path.exists(\"dms_model\")\n",
    "    and os.path.exists(\"test_sequences.csv\")\n",
    "    and make_submissions\n",
    "):\n",
    "    if use_baseline:\n",
    "        model_dms = BaselineModel()\n",
    "        model_2a3 = BaselineModel()\n",
    "    else:\n",
    "        model_2a3 = AttentionModel(**model_2a3_kwargs)\n",
    "        model_dms = AttentionModel(**model_dms_kwargs)\n",
    "\n",
    "    model_2a3.load_state_dict(torch.load(\"2a3_model\"))\n",
    "    model_dms.load_state_dict(torch.load(\"dms_model\"))\n",
    "\n",
    "    model_2a3.eval().to(DEVICE)\n",
    "    model_dms.eval().to(DEVICE)\n",
    "\n",
    "    valid = True\n",
    "else:\n",
    "    print(\"Not going to create submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "    model_2a3: torch.nn.Module,\n",
    "    model_dms: torch.nn.Module,\n",
    "    input_ds: str,\n",
    "    out: str,\n",
    "    batch_size: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Make predictions on the test dataset and write them to a csv file\n",
    "\n",
    "    Parameters:\n",
    "        - model_2a3: torch.nn.Module - the model trained on the 2a3 distribution\n",
    "        - model_dms: torch.nn.Module - the model trained on the dms distribution\n",
    "        - input_ds: str - name of the dataset to load\n",
    "        - out: str - name of the file to write to\n",
    "        - batch_size: int - size of the batches to use to process the data.\n",
    "            In general, larger batch sizes mean faster runtime\n",
    "    \"\"\"\n",
    "    ds = Dataset.load_from_disk(input_ds).with_format(\"torch\")\n",
    "    loader = data.DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    iterable = iter(loader)\n",
    "\n",
    "    with open(out, \"w\") as outfile:\n",
    "        # write the header\n",
    "        outfile.write(\"id,reactivity_DMS_MaP,reactivity_2A3_MaP\\n\")\n",
    "\n",
    "        for _ in tqdm(range(len(loader))):\n",
    "            # get the next group of data\n",
    "            tdata = next(iterable)\n",
    "            inputs = torch.stack([tdata[\"inputs\"], tdata[\"bpp\"]], dim=-1).to(DEVICE)\n",
    "            min_ids = tdata[\"id_min\"].numpy()\n",
    "            max_ids = tdata[\"id_max\"].numpy()\n",
    "\n",
    "            # make predictions w/o gradients\n",
    "            with torch.no_grad():\n",
    "                preds_2a3 = model_2a3(inputs).cpu().numpy()\n",
    "                preds_dms = model_dms(inputs).cpu().numpy()\n",
    "\n",
    "            # write preds\n",
    "            for i in range(inputs.shape[0]):\n",
    "                outfile.writelines(\n",
    "                    map(\n",
    "                        lambda seq_idx: f\"{seq_idx},{preds_dms[i, seq_idx-min_ids[i]]:.3f},{preds_2a3[i, seq_idx-min_ids[i]]:.3f}\\n\",\n",
    "                        # +1 since the id_max is inclusive\n",
    "                        range(min_ids[i], max_ids[i] + 1),\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-05T17:14:01.248308Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10499/10499 [39:34<00:00,  4.42it/s]\n"
     ]
    }
   ],
   "source": [
    "if valid:\n",
    "    pipeline(\n",
    "        model_2a3,\n",
    "        model_dms,\n",
    "        \"test_data_preprocessed\",\n",
    "        \"submission.csv\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "else:\n",
    "    print(\"Not going to create submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipping submissions. This may take a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: submission.csv (deflated 80%)\n",
      "Done zipping submissions!\n"
     ]
    }
   ],
   "source": [
    "if valid:\n",
    "    # zip our submission into an easily-uploadable zip file\n",
    "    print(\"zipping submissions. This may take a while...\")\n",
    "    os.system(\"zip submission.csv.zip submission.csv\")\n",
    "    print(\"Done zipping submissions!\")\n",
    "else:\n",
    "    print(\"Not going to zip submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
