{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ribonanza - Attempt 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second approach to the [Stanford Ribonanza problem](https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/) that builds off the first approach.\n",
    "\n",
    "Major differences:\n",
    "- use of attention model architecture\n",
    "- use of only filtered data (data in which SN_filter == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the attention architecture scores 0.20859"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- improve model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filesystem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your project directory should look like this:\n",
    "\n",
    "- `(project directory)`\n",
    "    - `ribonanza2.ipynb`\n",
    "    - `train_data.csv`\n",
    "    - `test_data.csv` (optional)\n",
    "\n",
    "`train_data.csv` is the only file necessary for training, and it can be downloaded from the kaggle competition linked in the description.\n",
    "\n",
    "`test_data.csv` is only necessary if you intend to make and submit predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import pandas\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "# according to kaggle, this is the maximum # of reactivites to be used\n",
    "NUM_REACTIVITIES = 457\n",
    "\n",
    "# there are 4 different bases (AUCG)\n",
    "NUM_BASES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(out: str, key: str, value: str, file_name: str, force: bool):\n",
    "    \"\"\"\n",
    "    Filters a file to only take datapoints\n",
    "    whose values of `key` are `value`.\n",
    "\n",
    "    Parameters:\n",
    "        - out: str - the name of the file that will store the filtered datapoints\n",
    "        - key: str - the name of the key to look at\n",
    "        - value: str - the value that the key should have\n",
    "        - file_name: str - the name of the file that contains all the datapoints.\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    if os.path.exists(out) and not force:\n",
    "        print(\"File already exists, not doing any work\")\n",
    "        return\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    # count how many lines we have in total\n",
    "    with open(file_name) as file:\n",
    "        line = file.readline()  # ignore the header\n",
    "        line = (\n",
    "            file.readline()\n",
    "        )  # take the first line since we increment count in the loop\n",
    "        while line != \"\":\n",
    "            count += 1\n",
    "            line = file.readline()\n",
    "\n",
    "    # use that knowledge for a progress bar\n",
    "    with open(file_name, \"r\") as file, open(out, \"w\") as outfile:\n",
    "        # write the header\n",
    "        header = file.readline()\n",
    "        outfile.write(header)\n",
    "\n",
    "        # get what index the SN_filter is\n",
    "        SN_idx = header.split(\",\").index(key)\n",
    "\n",
    "        # only take the approved filtered lines\n",
    "        for _ in tqdm(range(count)):\n",
    "            line = file.readline()\n",
    "            temp = line.split(\",\")\n",
    "            if temp[SN_idx] == value:\n",
    "                outfile.write(line)\n",
    "\n",
    "\n",
    "def filter_train_data(force: bool = False):\n",
    "    \"\"\"\n",
    "    Filters the immense train_data.csv to only take datapoints\n",
    "    whose SN_filter (Signal to Noise filter) is 1. In other words,\n",
    "    we only take good reads. These filtered datapoints are then\n",
    "    written to the file provided\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\"train_data_filtered.csv\", \"SN_filter\", \"1\", \"train_data.csv\", force)\n",
    "\n",
    "\n",
    "def filter_2A3(force: bool = False):\n",
    "    \"\"\"\n",
    "    Only take the 2A3 points\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\n",
    "        \"train_data_2a3.csv\",\n",
    "        \"experiment_type\",\n",
    "        \"2A3_MaP\",\n",
    "        \"train_data_filtered.csv\",\n",
    "        force,\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_DMS(force: bool = False):\n",
    "    \"\"\"\n",
    "    Only take the DMS points\n",
    "\n",
    "    Parameters:\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done)\n",
    "    \"\"\"\n",
    "    filter_data(\n",
    "        \"train_data_dms.csv\",\n",
    "        \"experiment_type\",\n",
    "        \"DMS_MaP\",\n",
    "        \"train_data_filtered.csv\",\n",
    "        force,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# filter our data\n",
    "filter_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# take the 2a3 points\n",
    "filter_2A3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "# take the dms points\n",
    "filter_DMS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode inputs as\n",
    "# A : 1\n",
    "# U : 2\n",
    "# C : 3\n",
    "# G : 4\n",
    "base_map = {\n",
    "    \"A\": 1,\n",
    "    \"U\": 2,\n",
    "    \"C\": 3,\n",
    "    \"G\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_csv(out: str, file_name: str, force: bool = False):\n",
    "    \"\"\"\n",
    "    Preprocess the csv and save the preprocessed data as a .npz file\n",
    "\n",
    "    Parameters:\n",
    "        - out: str - the name of the file to save the arrays to\n",
    "        - file_name: str - the name of the input csv file\n",
    "        - force: bool - whether or not to force re-processing of the data (if False and `out` already exists, no work will be done).\n",
    "                Defaults to `False`\n",
    "    \"\"\"\n",
    "    if os.path.exists(out) and not force:\n",
    "        print(\"File already exists, not doing any work\")\n",
    "        return\n",
    "\n",
    "    df = pandas.read_csv(file_name)\n",
    "\n",
    "    inputs = np.zeros((len(df), NUM_REACTIVITIES))\n",
    "    outputs = np.zeros((len(df), NUM_REACTIVITIES))\n",
    "    output_masks = np.ones((len(df), NUM_REACTIVITIES), dtype=np.bool_)\n",
    "    errors = np.zeros((len(df), NUM_REACTIVITIES))\n",
    "\n",
    "    for index in tqdm(range(len(df))):\n",
    "        row = df.iloc[index]\n",
    "\n",
    "        # get the sequence\n",
    "        seq_len = len(row[\"sequence\"])\n",
    "\n",
    "        # map the base to its one-hot encoding\n",
    "        inputs[index, :seq_len] = np.array(\n",
    "            list(map(lambda letter: base_map[letter], row[\"sequence\"]))\n",
    "        )\n",
    "\n",
    "        # get all the reactivities and reactivity errors\n",
    "        reactivities = np.array(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda seq_idx: row[\"reactivity_\" + str(seq_idx + 1).rjust(4, \"0\")],\n",
    "                    range(seq_len),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        reactivity_errors = np.array(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda seq_idx: row[\n",
    "                        \"reactivity_error_\" + str(seq_idx + 1).rjust(4, \"0\")\n",
    "                    ],\n",
    "                    range(seq_len),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # replace reactivity error nans with 0s (assume no error)\n",
    "        reactivity_errors = np.where(\n",
    "            np.isnan(reactivity_errors), 0.0, reactivity_errors\n",
    "        )\n",
    "\n",
    "        # get where all the reactivities are nan\n",
    "        nan_locats = np.isnan(reactivities)\n",
    "\n",
    "        # where it is nan, store True, else false\n",
    "        output_masks[index, :seq_len] = nan_locats\n",
    "\n",
    "        # where it is not nan, store the reactivity and error, else 0\n",
    "        outputs[index, :seq_len] = np.where(nan_locats == False, reactivities, 0.0)\n",
    "        errors[index, :seq_len] = np.where(nan_locats == False, reactivity_errors, 0.0)\n",
    "\n",
    "    # save the outputs\n",
    "    np.savez_compressed(\n",
    "        out, inputs=inputs, outputs=outputs, output_masks=output_masks, errors=errors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\"train_data_2a3_preprocessed.npz\", \"train_data_2a3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, not doing any work\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv(\"train_data_dms_preprocessed.npz\", \"train_data_dms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the desired dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:33.551791Z",
     "start_time": "2023-10-05T17:09:33.535197Z"
    }
   },
   "outputs": [],
   "source": [
    "desired_dataset = \"2a3\"  # either \"2a3\" or \"dms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.265620Z",
     "start_time": "2023-10-05T17:09:33.728394Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "# load the npz file\n",
    "npz_file = np.load(f\"train_data_{desired_dataset}_preprocessed.npz\")\n",
    "\n",
    "# stored inputs, outputs, and output_masks\n",
    "full_dataset = data.TensorDataset(\n",
    "    torch.tensor(npz_file[\"inputs\"], dtype=torch.float32),\n",
    "    torch.tensor(np.clip(npz_file[\"outputs\"], 0, 1), dtype=torch.float32),\n",
    "    torch.tensor(\n",
    "        np.clip(\n",
    "            np.where(npz_file[\"output_masks\"], 0.0, 1.0) - np.abs(npz_file[\"errors\"]),\n",
    "            0,\n",
    "            1,\n",
    "        ),\n",
    "        dtype=torch.float32,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# split into train, val\n",
    "train_dataset, val_dataset = data.random_split(full_dataset, lengths=[0.9, 0.1])\n",
    "\n",
    "# close the npz file\n",
    "npz_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set is len 189893 and val dataset is len 21099\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"train set is len\", len(train_dataset), \"and val dataset is len\", len(val_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the notebook allows for visualizing the reactivities of the\n",
    "current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.844615Z",
     "start_time": "2023-10-05T17:09:42.843236Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.851158Z",
     "start_time": "2023-10-05T17:09:42.846563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not visualizing. Set `visualize` to `True` to visualize data\n"
     ]
    }
   ],
   "source": [
    "if visualize:\n",
    "    npz_file = np.load(f\"train_data_{desired_dataset}_preprocessed.npz\")\n",
    "    outputs, bool_output_masks = npz_file[\"outputs\"], npz_file[\"output_masks\"]\n",
    "    npz_file.close()\n",
    "    visualized_items = []\n",
    "    for i in tqdm(range(len(outputs))):\n",
    "        for x in range(NUM_REACTIVITIES):\n",
    "            if not bool_output_masks[i, x]:\n",
    "                visualized_items.append(outputs[i, x])\n",
    "    visualized_items = np.array(visualized_items)\n",
    "    print(f\"took {len(visualized_items)}/{len(outputs)*NUM_REACTIVITIES} reactivities\")\n",
    "else:\n",
    "    print(\"Not visualizing. Set `visualize` to `True` to visualize data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:42.851272Z",
     "start_time": "2023-10-05T17:09:42.849526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not visualizing. Set `visualize` to `True` to visualize data\n"
     ]
    }
   ],
   "source": [
    "if visualize:\n",
    "    seaborn.histplot(visualized_items, binwidth=0.1)\n",
    "else:\n",
    "    print(\"Not visualizing. Set `visualize` to `True` to visualize data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xformers.components.positional_embedding as embeddings\n",
    "import xformers.ops as xops\n",
    "import xformers.components.attention as attentions\n",
    "import xformers.components.attention.utils as att_utils\n",
    "import xformers.components as components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEfficientSelfAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, latent_dim: int, n_heads: int, dropout: float, *args, **kwargs\n",
    "    ) -> None:\n",
    "        super(MemoryEfficientSelfAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        assert latent_dim % n_heads == 0\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        # B, Seq Len, embedding -> B, Seq Len, Heads, Embedding per head\n",
    "        x = x.reshape(x.shape[0], x.shape[1], self.n_heads, x.shape[2] // self.n_heads)\n",
    "        x = xops.memory_efficient_attention(x, x, x, p=self.dropout)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SDPAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int,\n",
    "        n_heads: int,\n",
    "        dropout: float,\n",
    "        device: str,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(SDPAttention, self).__init__()\n",
    "        self.mha = components.MultiHeadDispatch(\n",
    "            latent_dim,\n",
    "            num_heads=n_heads,\n",
    "            attention=attentions.ScaledDotProduct(dropout=dropout),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        return self.mha(x, att_mask=attention_mask)\n",
    "\n",
    "\n",
    "class SlidingWindowAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int,\n",
    "        n_heads: int,\n",
    "        dropout: float,\n",
    "        device: str,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(SlidingWindowAttention, self).__init__()\n",
    "        self.mha = components.MultiHeadDispatch(\n",
    "            latent_dim,\n",
    "            n_heads,\n",
    "            attentions.LocalAttention(\n",
    "                dropout=dropout, window_size=kwargs[\"context_window\"]\n",
    "            ),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        return self.mha(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerEncoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_type: torch.nn.Module,\n",
    "        latent_dim: int,\n",
    "        ff_dim: int,\n",
    "        n_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        device: str = \"cuda\",\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(CustomTransformerEncoderLayer, self).__init__()\n",
    "        self.attention = attention_type(\n",
    "            latent_dim=latent_dim,\n",
    "            n_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            device=device,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.layer_norm = torch.nn.LayerNorm(latent_dim).to(device)\n",
    "\n",
    "        self.ff1 = torch.nn.Linear(latent_dim, ff_dim).to(device)\n",
    "        self.ff2 = torch.nn.Linear(ff_dim, latent_dim).to(device)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        # MHA, add, norm\n",
    "        x = self.layer_norm(self.attention(x, attention_mask=attention_mask) + x)\n",
    "\n",
    "        # ff, add, norm\n",
    "        x = self.layer_norm(self.gelu(self.ff2(self.gelu(self.ff1(x)))) + x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomTransformerEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_type: torch.nn.Module,\n",
    "        n_layers: int,\n",
    "        latent_dim: int,\n",
    "        ff_dim: int,\n",
    "        n_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        device: str = \"cuda\",\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(CustomTransformerEncoder, self).__init__()\n",
    "        for i in range(n_layers):\n",
    "            self.add_module(\n",
    "                str(i),\n",
    "                CustomTransformerEncoderLayer(\n",
    "                    attention_type=attention_type,\n",
    "                    latent_dim=latent_dim,\n",
    "                    ff_dim=ff_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    dropout=dropout,\n",
    "                    device=device,\n",
    "                    **kwargs\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        for module in self._modules.values():\n",
    "            x = module(x, attention_mask=attention_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_type: torch.nn.Module,\n",
    "        context_window: int = 31,\n",
    "        latent_dim: int = 128,\n",
    "        ff_dim: int = 1024,\n",
    "        n_heads: int = 2,\n",
    "        enc_layers: int = 1,\n",
    "        device: str = \"cuda\",\n",
    "    ) -> None:\n",
    "        super(AttentionModel, self).__init__()\n",
    "\n",
    "        # data\n",
    "        self.n_heads = n_heads\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # prepatory conv layers\n",
    "        self.conv_layer = torch.nn.Conv1d(\n",
    "            1, out_channels=latent_dim, kernel_size=context_window, padding=\"same\"\n",
    "        ).to(device)\n",
    "        self.conv_layer_b = torch.nn.Conv1d(\n",
    "            1, out_channels=latent_dim, kernel_size=context_window, padding=\"same\"\n",
    "        ).to(device)\n",
    "\n",
    "        # positional embedding and encoder layers\n",
    "        self.pos_embedding = embeddings.SinePositionalEmbedding(latent_dim).to(device)\n",
    "        self.encoder_layers = CustomTransformerEncoder(\n",
    "            latent_dim=latent_dim,\n",
    "            ff_dim=ff_dim,\n",
    "            n_heads=n_heads,\n",
    "            device=device,\n",
    "            attention_type=attention_type,\n",
    "            n_layers=enc_layers,\n",
    "            context_window=context_window,\n",
    "        )\n",
    "\n",
    "        # output head\n",
    "        self.head = torch.nn.Linear(latent_dim, 1).to(device)\n",
    "        self.final_result = torch.nn.Linear(NUM_REACTIVITIES, NUM_REACTIVITIES).to(\n",
    "            device\n",
    "        )\n",
    "\n",
    "        # activations\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.gelu = torch.nn.GELU()\n",
    "\n",
    "    def _forward(self, x: torch.Tensor):\n",
    "        mask = att_utils.maybe_merge_masks(\n",
    "            att_mask=None,\n",
    "            key_padding_mask=x != 0,\n",
    "            batch_size=x.shape[0],\n",
    "            num_heads=self.n_heads,\n",
    "            src_len=x.shape[1],\n",
    "        )\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = self.gelu(\n",
    "            self.conv_layer(x)\n",
    "            + torch.flip(self.conv_layer_b(torch.flip(x, dims=[2])), dims=[2])\n",
    "        )\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = x.permute((0, 2, 1)).contiguous()\n",
    "\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.encoder_layers(x, attention_mask=mask)\n",
    "\n",
    "        x = self.relu(self.final_result(self.gelu(self.head(x).flatten(start_dim=1))))\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "\n",
    "    # used for calculating lr to use\n",
    "    def weightedForward(self, x: torch.Tensor, weights: torch.Tensor):\n",
    "        self.weights = weights\n",
    "        return self._forward(x)\n",
    "\n",
    "    def weightedLoss(self, y_pred: torch.Tensor, y_true: torch.Tensor):\n",
    "        return (\n",
    "            (torch.nn.L1Loss()(y_pred.cuda(), y_true.cuda()) * self.weights.cuda())\n",
    "            .sum(dim=-1)\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "    def configure_for_lr_testing(self):\n",
    "        self.forward = self.weightedForward\n",
    "\n",
    "    def configure_for_training(self):\n",
    "        self.forward = self._forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(torch.nn.Module):\n",
    "    def __init__(self, context_window: int = 31, device: str = \"cuda\"):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.conv_layer = torch.nn.Conv1d(1, 1, context_window, padding=\"same\").to(\n",
    "            device\n",
    "        )\n",
    "        self.conv_layer_b = torch.nn.Conv1d(1, 1, context_window, padding=\"same\").to(\n",
    "            device\n",
    "        )\n",
    "        self.ff = torch.nn.Linear(NUM_REACTIVITIES, NUM_REACTIVITIES).to(device)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def _forward(self, x: torch.Tensor):\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "\n",
    "        x = self.gelu(\n",
    "            self.conv_layer(x)\n",
    "            + torch.flip(self.conv_layer_b(torch.flip(x, dims=[2])), dims=[2])\n",
    "        )\n",
    "\n",
    "        return self.relu(self.ff(x.flatten(start_dim=1)))\n",
    "\n",
    "    def weightedForward(self, x: torch.Tensor, weights: torch.Tensor):\n",
    "        self.weights = weights\n",
    "        return self._forward(x)\n",
    "\n",
    "    def weightedLoss(self, y_pred: torch.Tensor, y_true: torch.Tensor):\n",
    "        return (\n",
    "            (torch.nn.L1Loss()(y_pred.cuda(), y_true.cuda()) * self.weights.cuda())\n",
    "            .sum(dim=-1)\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "    def configure_for_lr_testing(self):\n",
    "        self.forward = self.weightedForward\n",
    "\n",
    "    def configure_for_training(self):\n",
    "        self.forward = self._forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dms_kwargs = dict(\n",
    "    latent_dim=32, n_heads=4, enc_layers=2, ff_dim=256, attention_type=SDPAttention\n",
    ")\n",
    "model_2a3_kwargs = dict(\n",
    "    latent_dim=32, n_heads=4, enc_layers=2, ff_dim=256, attention_type=SDPAttention\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if desired_dataset == \"dms\":\n",
    "    model = AttentionModel(**model_dms_kwargs)\n",
    "elif desired_dataset == \"2a3\":\n",
    "    model = AttentionModel(**model_2a3_kwargs)\n",
    "# model= BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.zeros((2, NUM_REACTIVITIES))\n",
    "inp[:, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 5.3057e-02, 4.4558e-01, 6.5869e-03, 0.0000e+00, 1.1272e-01,\n",
       "         2.6484e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.1126e-02, 8.8696e-04, 1.7578e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 5.3540e-02, 7.5706e-03, 1.1682e-02, 3.5635e-01, 0.0000e+00,\n",
       "         1.4374e-01, 5.5607e-01, 4.3531e-01, 0.0000e+00, 2.0895e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 3.3457e-01, 4.7717e-01, 0.0000e+00, 3.2619e-01,\n",
       "         4.5629e-01, 0.0000e+00, 2.0239e-01, 3.1963e-01, 0.0000e+00, 1.5677e-01,\n",
       "         7.6257e-02, 3.8343e-01, 2.6197e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.8042e-01, 0.0000e+00,\n",
       "         2.4630e-01, 4.7028e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4386e-01,\n",
       "         5.1121e-01, 1.7544e-01, 0.0000e+00, 3.0134e-01, 0.0000e+00, 0.0000e+00,\n",
       "         1.3123e-01, 7.9583e-02, 1.3463e-01, 2.9989e-02, 4.7034e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 5.5664e-01, 0.0000e+00, 0.0000e+00, 2.8434e-01,\n",
       "         0.0000e+00, 1.3795e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.2645e-01, 0.0000e+00, 0.0000e+00, 2.9080e-01, 0.0000e+00, 2.7522e-01,\n",
       "         0.0000e+00, 0.0000e+00, 2.2825e-01, 0.0000e+00, 0.0000e+00, 3.8688e-01,\n",
       "         2.2704e-01, 2.1826e-01, 2.7007e-03, 0.0000e+00, 8.5590e-02, 0.0000e+00,\n",
       "         2.7963e-01, 1.5206e-01, 9.8572e-02, 7.5016e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.2122e-01, 0.0000e+00, 0.0000e+00, 3.7221e-01, 1.7125e-01,\n",
       "         0.0000e+00, 0.0000e+00, 1.1743e-02, 2.1568e-01, 0.0000e+00, 0.0000e+00,\n",
       "         7.0555e-03, 2.4954e-01, 0.0000e+00, 7.0299e-02, 0.0000e+00, 0.0000e+00,\n",
       "         1.4959e-01, 7.8308e-02, 2.3251e-01, 0.0000e+00, 2.2136e-01, 0.0000e+00,\n",
       "         1.5892e-01, 4.0417e-01, 0.0000e+00, 0.0000e+00, 2.6761e-01, 0.0000e+00,\n",
       "         0.0000e+00, 4.6774e-02, 3.9982e-02, 0.0000e+00, 0.0000e+00, 1.9301e-01,\n",
       "         8.8925e-02, 0.0000e+00, 1.2940e-01, 0.0000e+00, 3.3135e-01, 0.0000e+00,\n",
       "         0.0000e+00, 9.1904e-02, 0.0000e+00, 0.0000e+00, 5.7206e-01, 7.7395e-02,\n",
       "         0.0000e+00, 1.4854e-01, 4.7281e-01, 7.7356e-02, 4.8468e-01, 1.0971e-01,\n",
       "         2.7081e-01, 5.4042e-02, 1.8791e-01, 0.0000e+00, 2.0859e-01, 0.0000e+00,\n",
       "         0.0000e+00, 2.0057e-01, 2.8679e-01, 1.7355e-01, 0.0000e+00, 0.0000e+00,\n",
       "         1.7793e-01, 1.7288e-01, 3.0482e-01, 0.0000e+00, 0.0000e+00, 3.0920e-02,\n",
       "         3.5036e-01, 0.0000e+00, 5.3291e-02, 3.4337e-01, 1.5071e-01, 4.4726e-01,\n",
       "         0.0000e+00, 5.1697e-01, 0.0000e+00, 2.1807e-01, 0.0000e+00, 4.6908e-02,\n",
       "         1.8120e-02, 0.0000e+00, 6.2312e-03, 2.5588e-01, 0.0000e+00, 0.0000e+00,\n",
       "         4.8645e-01, 0.0000e+00, 0.0000e+00, 2.4988e-01, 7.6131e-02, 1.2466e-01,\n",
       "         0.0000e+00, 3.0778e-01, 2.6733e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.1271e-01, 0.0000e+00, 2.7319e-01, 3.4608e-02,\n",
       "         2.1483e-03, 0.0000e+00, 1.0397e-01, 0.0000e+00, 1.4555e-01, 0.0000e+00,\n",
       "         0.0000e+00, 1.7211e-01, 2.3301e-01, 3.6724e-01, 1.0970e-01, 7.4551e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1084e-03, 1.4090e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.1612e-01, 0.0000e+00, 1.7427e-01, 2.7435e-01,\n",
       "         3.4466e-02, 1.4388e-02, 3.8223e-02, 0.0000e+00, 0.0000e+00, 3.9785e-01,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2094e-01, 1.4852e-01,\n",
       "         1.3808e-01, 2.0033e-02, 2.8609e-01, 4.0164e-01, 0.0000e+00, 1.1213e-02,\n",
       "         0.0000e+00, 0.0000e+00, 4.2548e-01, 2.4690e-01, 2.7587e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4614e-01,\n",
       "         2.8421e-01, 0.0000e+00, 1.1528e-01, 4.5513e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.5724e-01, 3.2122e-01, 0.0000e+00, 1.6460e-01, 0.0000e+00,\n",
       "         3.1444e-01, 4.0463e-01, 5.7339e-02, 1.2035e-01, 1.1289e-01, 1.0212e+00,\n",
       "         5.0424e-01, 1.1228e-01, 4.3681e-01, 1.4188e-02, 0.0000e+00, 1.4691e-01,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3199e-01, 1.2897e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.2076e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.5100e-01, 0.0000e+00, 1.2077e-01, 0.0000e+00,\n",
       "         0.0000e+00, 1.9205e-01, 3.4385e-01, 0.0000e+00, 4.6406e-01, 1.3598e-01,\n",
       "         0.0000e+00, 2.5788e-01, 0.0000e+00, 0.0000e+00, 3.5145e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8699e-01, 9.1700e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 8.1959e-02, 1.0459e-01, 0.0000e+00,\n",
       "         3.6264e-02, 0.0000e+00, 2.1713e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.3626e-01, 2.2731e-02, 0.0000e+00, 4.7606e-02, 2.6802e-01,\n",
       "         6.9841e-02, 2.5705e-01, 1.0005e-02, 2.4628e-01, 0.0000e+00, 3.7143e-01,\n",
       "         3.1822e-01, 2.8086e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3553e-02,\n",
       "         2.1570e-01, 0.0000e+00, 1.7795e-01, 2.5883e-01, 3.8704e-02, 4.2246e-01,\n",
       "         1.0664e-01, 0.0000e+00, 2.2757e-03, 0.0000e+00, 3.8180e-01, 7.2796e-02,\n",
       "         0.0000e+00, 4.1492e-01, 1.8452e-01, 0.0000e+00, 0.0000e+00, 7.7852e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4962e-01, 1.6494e-01, 5.0762e-02,\n",
       "         0.0000e+00, 4.2238e-01, 7.3310e-02, 1.0615e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2319e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.0739e-01, 0.0000e+00, 0.0000e+00, 6.1242e-01, 4.4199e-02,\n",
       "         0.0000e+00, 0.0000e+00, 2.7528e-01, 0.0000e+00, 1.0211e-01, 0.0000e+00,\n",
       "         4.8985e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3821e-01,\n",
       "         0.0000e+00, 1.2997e-01, 4.8383e-02, 1.8537e-03, 1.3075e-01, 0.0000e+00,\n",
       "         0.0000e+00, 1.6430e-01, 0.0000e+00, 0.0000e+00, 5.4406e-01, 0.0000e+00,\n",
       "         2.0570e-02, 6.3786e-02, 2.7401e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.3007e-02, 0.0000e+00,\n",
       "         8.8457e-02, 0.0000e+00, 2.9225e-01, 0.0000e+00, 2.3348e-01, 9.5859e-02,\n",
       "         0.0000e+00, 5.6770e-02, 0.0000e+00, 2.0612e-01, 0.0000e+00, 2.7697e-01,\n",
       "         1.3623e-01],\n",
       "        [0.0000e+00, 3.6683e-02, 4.1633e-01, 0.0000e+00, 0.0000e+00, 7.7265e-02,\n",
       "         2.5058e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         4.2348e-02, 2.7766e-02, 2.0363e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.5194e-02, 0.0000e+00, 4.5127e-02, 3.1059e-01, 0.0000e+00,\n",
       "         2.0233e-01, 6.5449e-01, 3.4233e-01, 0.0000e+00, 2.7416e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 3.5023e-01, 4.0099e-01, 0.0000e+00, 3.6021e-01,\n",
       "         4.5550e-01, 0.0000e+00, 2.6745e-01, 3.3150e-01, 0.0000e+00, 1.7528e-01,\n",
       "         5.5970e-02, 4.0338e-01, 2.6436e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         5.6807e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.0711e-01, 0.0000e+00,\n",
       "         1.9117e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.5890e-02,\n",
       "         5.4933e-01, 2.3122e-01, 0.0000e+00, 3.2000e-01, 0.0000e+00, 0.0000e+00,\n",
       "         1.5832e-01, 4.5195e-02, 1.5799e-01, 0.0000e+00, 3.7203e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 5.8359e-01, 0.0000e+00, 0.0000e+00, 3.3616e-01,\n",
       "         0.0000e+00, 8.8045e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.9977e-01, 0.0000e+00, 0.0000e+00, 3.2630e-01, 0.0000e+00, 2.8269e-01,\n",
       "         0.0000e+00, 0.0000e+00, 1.7638e-01, 0.0000e+00, 0.0000e+00, 3.8236e-01,\n",
       "         1.5769e-01, 2.0465e-01, 0.0000e+00, 0.0000e+00, 8.0567e-02, 0.0000e+00,\n",
       "         2.1247e-01, 1.8458e-01, 7.6624e-02, 1.0803e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 8.8730e-02, 0.0000e+00, 0.0000e+00, 3.5003e-01, 1.5392e-01,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0436e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.3536e-01, 0.0000e+00, 1.6037e-01, 0.0000e+00, 0.0000e+00,\n",
       "         9.6469e-02, 2.4492e-02, 2.8737e-01, 0.0000e+00, 1.9350e-01, 0.0000e+00,\n",
       "         2.0883e-01, 4.2373e-01, 0.0000e+00, 0.0000e+00, 2.8029e-01, 0.0000e+00,\n",
       "         0.0000e+00, 6.2041e-02, 9.7313e-02, 2.8347e-02, 0.0000e+00, 2.6132e-01,\n",
       "         2.8591e-02, 0.0000e+00, 1.2623e-01, 0.0000e+00, 2.6008e-01, 0.0000e+00,\n",
       "         0.0000e+00, 1.5059e-01, 0.0000e+00, 0.0000e+00, 5.8285e-01, 5.0383e-02,\n",
       "         0.0000e+00, 1.4768e-01, 4.8284e-01, 1.4281e-01, 3.9042e-01, 1.7204e-01,\n",
       "         2.7531e-01, 0.0000e+00, 1.0834e-01, 0.0000e+00, 1.9224e-01, 1.7901e-02,\n",
       "         0.0000e+00, 2.2625e-01, 2.9518e-01, 1.8806e-01, 0.0000e+00, 0.0000e+00,\n",
       "         1.1544e-01, 1.8055e-01, 3.9180e-01, 0.0000e+00, 0.0000e+00, 3.5076e-02,\n",
       "         3.5795e-01, 0.0000e+00, 1.1652e-01, 3.1631e-01, 1.3372e-01, 4.5088e-01,\n",
       "         0.0000e+00, 5.2554e-01, 0.0000e+00, 2.1110e-01, 0.0000e+00, 7.1607e-02,\n",
       "         2.4020e-02, 0.0000e+00, 0.0000e+00, 2.3428e-01, 3.1570e-03, 0.0000e+00,\n",
       "         4.9820e-01, 0.0000e+00, 0.0000e+00, 3.5649e-01, 2.1052e-02, 1.2419e-01,\n",
       "         0.0000e+00, 3.3115e-01, 2.9502e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 3.7936e-02, 2.2915e-02, 3.1390e-01, 5.5777e-03,\n",
       "         0.0000e+00, 0.0000e+00, 9.8711e-02, 0.0000e+00, 1.1871e-01, 0.0000e+00,\n",
       "         6.6075e-02, 2.3795e-01, 2.1298e-01, 2.8800e-01, 5.7797e-02, 2.2228e-01,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5920e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.9655e-01, 0.0000e+00, 2.5545e-01, 2.4197e-01,\n",
       "         0.0000e+00, 3.9677e-02, 4.4878e-02, 0.0000e+00, 0.0000e+00, 4.1979e-01,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3023e-01, 1.0288e-01,\n",
       "         6.9568e-02, 1.7151e-02, 2.9033e-01, 3.7840e-01, 0.0000e+00, 4.0653e-02,\n",
       "         0.0000e+00, 0.0000e+00, 4.9375e-01, 1.6041e-01, 2.6258e-01, 0.0000e+00,\n",
       "         2.7708e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5585e-01,\n",
       "         3.5824e-01, 1.6648e-02, 1.7685e-01, 4.4555e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.6771e-01, 2.9361e-01, 0.0000e+00, 2.0001e-01, 0.0000e+00,\n",
       "         3.4117e-01, 4.1704e-01, 3.7013e-02, 1.3553e-01, 1.8402e-01, 1.0744e+00,\n",
       "         4.5609e-01, 2.3170e-01, 4.1228e-01, 0.0000e+00, 0.0000e+00, 1.3231e-01,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5777e-01, 5.5372e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0925e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.2145e-01, 0.0000e+00, 1.4767e-01, 0.0000e+00,\n",
       "         0.0000e+00, 1.3326e-01, 3.4575e-01, 0.0000e+00, 4.7622e-01, 1.8282e-01,\n",
       "         0.0000e+00, 2.9610e-01, 0.0000e+00, 0.0000e+00, 2.7459e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3131e-02, 4.3881e-01, 8.3227e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2933e-01, 1.3517e-01, 0.0000e+00,\n",
       "         3.1447e-02, 0.0000e+00, 2.1157e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.0062e-01, 0.0000e+00, 0.0000e+00, 6.5783e-02, 2.0237e-01,\n",
       "         1.2149e-01, 2.2486e-01, 0.0000e+00, 1.3542e-01, 0.0000e+00, 4.3681e-01,\n",
       "         2.5582e-01, 2.6478e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.9956e-01, 0.0000e+00, 1.9874e-01, 3.0949e-01, 4.8608e-02, 4.9201e-01,\n",
       "         4.8848e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1170e-01, 1.2610e-01,\n",
       "         0.0000e+00, 4.1273e-01, 2.3622e-01, 0.0000e+00, 0.0000e+00, 5.0792e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8066e-01, 2.0565e-01, 6.2146e-02,\n",
       "         0.0000e+00, 4.0109e-01, 6.6194e-02, 4.1908e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0848e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.4732e-01, 0.0000e+00, 0.0000e+00, 6.4009e-01, 7.7796e-02,\n",
       "         0.0000e+00, 4.5782e-02, 2.2962e-01, 2.2975e-02, 1.4890e-01, 0.0000e+00,\n",
       "         2.7247e-02, 0.0000e+00, 0.0000e+00, 1.4814e-02, 0.0000e+00, 1.4226e-01,\n",
       "         0.0000e+00, 1.9342e-01, 7.1430e-02, 4.6938e-02, 1.1565e-01, 0.0000e+00,\n",
       "         0.0000e+00, 1.5611e-01, 0.0000e+00, 0.0000e+00, 5.0026e-01, 0.0000e+00,\n",
       "         0.0000e+00, 7.7878e-02, 3.5137e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.2173e-02, 0.0000e+00,\n",
       "         5.3788e-02, 0.0000e+00, 3.0403e-01, 0.0000e+00, 2.1721e-01, 1.1882e-01,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1111e-01, 0.0000e+00, 2.3996e-01,\n",
       "         1.8854e-01]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.configure_for_training()\n",
    "model(inp.cuda()).cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionModel(\n",
      "  (conv_layer): Conv1d(1, 32, kernel_size=(31,), stride=(1,), padding=same)\n",
      "  (conv_layer_b): Conv1d(1, 32, kernel_size=(31,), stride=(1,), padding=same)\n",
      "  (pos_embedding): SinePositionalEmbedding()\n",
      "  (encoder_layers): CustomTransformerEncoder(\n",
      "    (0): CustomTransformerEncoderLayer(\n",
      "      (attention): SDPAttention(\n",
      "        (mha): MultiHeadDispatch(\n",
      "          (attention): ScaledDotProduct(\n",
      "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (in_proj_container): InputProjection(\n",
      "            (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (ff2): Linear(in_features=256, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "    (1): CustomTransformerEncoderLayer(\n",
      "      (attention): SDPAttention(\n",
      "        (mha): MultiHeadDispatch(\n",
      "          (attention): ScaledDotProduct(\n",
      "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (in_proj_container): InputProjection(\n",
      "            (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff1): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (ff2): Linear(in_features=256, out_features=32, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (head): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (final_result): Linear(in_features=457, out_features=457, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 253307\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"Total params:\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.010964781977236271)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import fastai.learner as learner\n",
    "import fastai.callback.schedule\n",
    "import fastai.data.core as fdata\n",
    "\n",
    "model.configure_for_lr_testing()\n",
    "learner.Learner(\n",
    "    fdata.DataLoaders(\n",
    "        data.DataLoader(train_dataset, batch_size=32, shuffle=True),\n",
    "        data.DataLoader(val_dataset, batch_size=32, shuffle=True),\n",
    "    ),\n",
    "    model,\n",
    "    loss_func=model.weightedLoss,\n",
    ").lr_find(stop_div=False, num_it=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.configure_for_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), 2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "SHUFFLE = True\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE\n",
    ")\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False\n",
    ")  # no point in shuffling val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedL1(\n",
    "    y_pred: torch.Tensor,\n",
    "    y_true: torch.Tensor,\n",
    "    weights: torch.Tensor,\n",
    "    l1=torch.nn.L1Loss(),\n",
    "):\n",
    "    return (l1(y_pred, y_true) * weights).sum(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:09:43.992618Z",
     "start_time": "2023-10-05T17:09:43.053099Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_batch(\n",
    "    m: torch.nn.Module, inps: torch.Tensor, outs: torch.Tensor, masks: torch.Tensor\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the loss on a batch and perform the corresponding weight updates.\n",
    "    Used for training purposes\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    loss = weightedL1(m(inps), outs, masks)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # calculate gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # return mae loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def noupdate_batch(\n",
    "    m: torch.nn.Module, inps: torch.Tensor, outs: torch.Tensor, masks: torch.Tensor\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the loss on a batch without performing any updates.\n",
    "    Used for validation purposes\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        loss = weightedL1(m(inps), outs, masks)\n",
    "\n",
    "    # return mae loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_train(\n",
    "    m: torch.nn.Module,\n",
    "    train_dataloader: data.DataLoader,\n",
    "    val_dataloader: data.DataLoader,\n",
    "    epochs: int = 1,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the given model.\n",
    "\n",
    "    Arguments:\n",
    "        - m: keras.Model - the model to train.\n",
    "        - x: np.ndarray - the numpy array of inputs.\n",
    "        - y: np.ndarray - the numpy array of outputs.\n",
    "        - masks: np.ndarray - the sample weights (1s and 0s).\n",
    "        - batch_size: int - how large the batches should be. Defaults to `32`.\n",
    "        - epochs: int - how many epochs to train for. Defaults to `1`.\n",
    "        - validation_split: float - how large the validation subset should be, in the range (0, 1]. Defaults to `0.1`.\n",
    "\n",
    "    Note - The choice of np.ndarray is purely arbitrary, and this function can be modified to use tf.Tensors\n",
    "\n",
    "    Note - shuffle code is provided in numpy, but commented out because of memory limitations that less powerful computers\n",
    "    may encounter.\n",
    "    \"\"\"\n",
    "    # shuffle\n",
    "    # shuffled_idxs = np.arange(x.shape[0])\n",
    "    # np.random.shuffle(shuffled_idxs)\n",
    "    # x = x[shuffled_idxs]\n",
    "    # y = y[shuffled_idxs]\n",
    "    # masks = masks[shuffled_idxs]\n",
    "    m = m.to(device)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        epoch_mae = 0.0\n",
    "\n",
    "        m = m.train()\n",
    "        for batch, tdata in enumerate(train_dataloader):\n",
    "            inps, outs, masks = tdata\n",
    "\n",
    "            inps = inps.to(device)\n",
    "            outs = outs.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            mae_loss = train_batch(m, inps, outs, masks).detach().cpu()\n",
    "\n",
    "            epoch_mae += mae_loss\n",
    "\n",
    "            # log\n",
    "            print(\n",
    "                f\"Batch {batch+1}/{len(train_dataloader)}\\t- mae loss: {mae_loss:.5f}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "            # break  # used for sanity check\n",
    "        epoch_mae /= len(train_dataloader)\n",
    "\n",
    "        # do validation\n",
    "        val_mae = 0.0\n",
    "        m = m.eval()\n",
    "        for batch, vdata in enumerate(val_dataloader):\n",
    "            inps, outs, masks = vdata\n",
    "            inps = inps.to(device)\n",
    "            outs = outs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            mae_loss = noupdate_batch(m, inps, outs, masks)\n",
    "\n",
    "            val_mae += mae_loss\n",
    "        val_mae /= len(val_dataloader)\n",
    "\n",
    "        # shuffle\n",
    "        # shuffled_idxs = np.arange(x.shape[0])\n",
    "        # np.random.shuffle(shuffled_idxs)\n",
    "        # x = x[shuffled_idxs]\n",
    "        # y = y[shuffled_idxs]\n",
    "        # masks = masks[shuffled_idxs]\n",
    "\n",
    "        print()\n",
    "        print(f\"Epoch MAE: {epoch_mae:.5f}\\tVal MAE: {val_mae:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.128074Z",
     "start_time": "2023-10-05T17:09:43.998369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Batch 1484/1484\t- mae loss: 4.56457\n",
      "Epoch MAE: 4.73672\tVal MAE: 4.59158\n",
      "Epoch 2\n",
      "Batch 1484/1484\t- mae loss: 4.22912\n",
      "Epoch MAE: 4.47730\tVal MAE: 4.35422\n",
      "Epoch 3\n",
      "Batch 1484/1484\t- mae loss: 3.93581\n",
      "Epoch MAE: 4.25839\tVal MAE: 4.18861\n",
      "Epoch 4\n",
      "Batch 1484/1484\t- mae loss: 3.91748\n",
      "Epoch MAE: 4.12989\tVal MAE: 4.08329\n",
      "Epoch 5\n",
      "Batch 1484/1484\t- mae loss: 4.16344\n",
      "Epoch MAE: 4.04672\tVal MAE: 4.01285\n"
     ]
    }
   ],
   "source": [
    "# baseline gets ~ 3.64968 on dms w/ 10 epochs, ~ 4.64478 on 2a3 w/ 10 epochs\n",
    "masked_train(\n",
    "    model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section saves the current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.616528Z",
     "start_time": "2023-10-05T17:14:00.119617Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"{desired_dataset}_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the noteboook creates a zipped csv submission file that can\n",
    "be submitted on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:00.624146Z",
     "start_time": "2023-10-05T17:14:00.601389Z"
    }
   },
   "outputs": [],
   "source": [
    "make_submissions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:01.233216Z",
     "start_time": "2023-10-05T17:14:00.608006Z"
    }
   },
   "outputs": [],
   "source": [
    "valid = False\n",
    "\n",
    "if (\n",
    "    os.path.exists(\"2a3_model\")\n",
    "    and os.path.exists(\"dms_model\")\n",
    "    and os.path.exists(\"test_sequences.csv\")\n",
    "    and make_submissions\n",
    "):\n",
    "    valid = True\n",
    "    model_2a3 = AttentionModel(**model_2a3_kwargs)\n",
    "    model_2a3.load_state_dict(torch.load(\"2a3_model\"))\n",
    "    model_dms = AttentionModel(**model_dms_kwargs)\n",
    "    model_dms.load_state_dict(torch.load(\"dms_model\"))\n",
    "\n",
    "    model_2a3.eval().cuda().configure_for_training()\n",
    "    model_dms.eval().cuda().configure_for_training()\n",
    "else:\n",
    "    print(\"Not going to create submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T17:14:01.247263Z",
     "start_time": "2023-10-05T17:14:01.233464Z"
    }
   },
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "    model_2a3: torch.nn.Module,\n",
    "    model_dms: torch.nn.Module,\n",
    "    input_csv: str,\n",
    "    out: str,\n",
    "    batch_size: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Process test data and write submissions to a csv file\n",
    "\n",
    "    Arguments:\n",
    "        - model_2a3: keras.Model - the model trained on the 2a3 distribution\n",
    "        - model_dms: keras.Model - the model trained on the dms distribution\n",
    "        - input_csv: str - the name of the file that contains the test data\n",
    "        - out: str - the name of the file to write predictions to\n",
    "        - batch_size: int - how many predictions to make at a time\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "\n",
    "    # count how many lines we have in total\n",
    "    with open(input_csv) as file:\n",
    "        line = file.readline()  # ignore the header\n",
    "        # take the first line since we increment count in the loop\n",
    "        line = file.readline()\n",
    "        while line != \"\":\n",
    "            count += 1\n",
    "            line = file.readline()\n",
    "\n",
    "    # use that knowledge for a progress bar\n",
    "    with open(input_csv, \"r\") as file, open(out, \"w\") as outfile:\n",
    "        # write the header\n",
    "        outfile.write(\"id,reactivity_DMS_MaP,reactivity_2A3_MaP\\n\")\n",
    "\n",
    "        # get what index the things we need are\n",
    "        header = file.readline()\n",
    "        split_header = header.split(\",\")\n",
    "        min_idx = split_header.index(\"id_min\")\n",
    "        max_idx = split_header.index(\"id_max\")\n",
    "        sequence_idx = split_header.index(\"sequence\")\n",
    "\n",
    "        # only take the approved filtered lines\n",
    "        num_batches = count // batch_size\n",
    "        if count % batch_size != 0:\n",
    "            num_batches += 1\n",
    "        for batch in tqdm(range(num_batches)):\n",
    "            num_items = min(batch_size, count - batch * batch_size)\n",
    "\n",
    "            # initialize variables\n",
    "            inputs = np.zeros((num_items, NUM_REACTIVITIES))\n",
    "            min_seq_idxs = []\n",
    "            sequence_lengths = []\n",
    "\n",
    "            # collect the inputs\n",
    "            for i in range(num_items):\n",
    "                line = file.readline()\n",
    "                temp = line.split(\",\")\n",
    "                sequence = temp[sequence_idx]\n",
    "                max_seq_idx = int(temp[max_idx])\n",
    "                min_seq_idx = int(temp[min_idx])\n",
    "\n",
    "                # verify that everything is correct\n",
    "                assert len(sequence) + min_seq_idx - 1 == max_seq_idx\n",
    "\n",
    "                # store the data\n",
    "                inputs[i, : len(sequence)] = np.array(\n",
    "                    list(map(lambda letter: base_map[letter], sequence))\n",
    "                )\n",
    "                min_seq_idxs.append(min_seq_idx)\n",
    "                sequence_lengths.append(len(sequence))\n",
    "\n",
    "            # run inputs through the associated model\n",
    "            inputs = torch.tensor(inputs, dtype=torch.float32).cuda()\n",
    "            with torch.no_grad():\n",
    "                probs_2a3, probs_dms = model_2a3(inputs), model_dms(inputs)\n",
    "            probs_dms = probs_dms.cpu()\n",
    "            probs_2a3 = probs_2a3.cpu()\n",
    "\n",
    "            # write predictions\n",
    "            for i in range(num_items):\n",
    "                for seq_idx in range(\n",
    "                    min_seq_idxs[i], min_seq_idxs[i] + sequence_lengths[i]\n",
    "                ):\n",
    "                    outfile.write(\n",
    "                        f\"{seq_idx},{probs_dms[i, seq_idx - min_seq_idxs[i]]:.3f},{probs_2a3[i, seq_idx - min_seq_idxs[i]]:.3f}\\n\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-05T17:14:01.248308Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5250/5250 [1:06:03<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "if valid:\n",
    "    pipeline(\n",
    "        model_2a3, model_dms, \"test_sequences.csv\", \"submission.csv\", batch_size=256\n",
    "    )\n",
    "else:\n",
    "    print(\"Not going to create submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipping submissions. This may take a while...\n",
      "updating: submission.csv (deflated 81%)\n",
      "Done zipping submissions!\n"
     ]
    }
   ],
   "source": [
    "if valid:\n",
    "    # zip our submission into an easily-uploadable zip file\n",
    "    print(\"zipping submissions. This may take a while...\")\n",
    "    os.system(\"zip submission.csv.zip submission.csv\")\n",
    "    print(\"Done zipping submissions!\")\n",
    "else:\n",
    "    print(\"Not going to zip submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
